\documentclass[a4paper,11pt]{report}

\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}

\usepackage{tikz}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsfonts}







\begin{document}

%%%%  Number the initial front matter with roman numerals
\pagenumbering{roman}


%%%%  TITLE PAGE
%%  Here are the elements that make up the title page of the document.  

\thispagestyle{empty}  %%  Make the title page have no number.  


%%% Comment this part until the next %%% for your dissertation.
\vspace{2cm}
\begin{center}
This page is to be omitted in your dissertation!
\end{center}
\vspace{10cm}
\noindent
{\Large {\bf Note:} This template is only an example. 
It shows what needs to be included for a typical project that includes implementation and system building.
Different projects may require different format.
Please consult your supervisor for the most suitable format.}
\pagebreak
%%% 





\thispagestyle{empty}  %%  this page has no number. 

~\vspace{1.5cm}
\begin{center}
{\LARGE\bf
Stereo Vision-Based Sensing Area Detection for Laparoscopic Gamma Probes in Minimally Invasive Cancer Surgery}

\bigskip\bigskip
{\large
Dissertation}

\smallskip
COMP702 -- M.Sc. project (2024/25)

\vfill
{\large
Submitted by}

{\large
\emph{GEO RAJU}}

{\large
(201855055)}

\bigskip\bigskip
{\large
under the supervision of}

{\large
\emph{DR. BAORU HUANG}}

\vspace{4cm}

{\large\sc
Department of Computer Science}

\smallskip
{\large\sc
University of Liverpool}
\end{center}






\pagebreak


%%%%  Turn page numbering back to arabic.  This also resets the numbering
%%%%  to begin again at page 1.  

\pagenumbering{arabic}

%%%%  






%\thispagestyle{empty}  %%  this page has no number.  


%%%%  STUDENT DECLARATION ON PLAGIARISM
\chapter*{\center Student Declaration} 

\thispagestyle{empty}  

%%  this page has no number. 

I confirm that I have read and understood the University's Academic Integrity Policy.

I confirm that I have acted honestly, ethically and professionally in conduct leading
to assessment for the programme of study.  

I confirm that I have not copied material from another source nor committed plagiarism
nor fabricated data when completing the attached piece of work.  I confirm that I have 
not previously presented the work or part thereof for assessment for another University
of Liverpool module.  I confirm that I have not copied material from another source, nor
colluded with any other student in the preparation and production of this work.  

I confirm that I have not incorporated into this assignment material that has been 
submitted by me or any other person in support of a successful application for a 
degree of this or any other university or degree-awarding body.  

\vspace*{1in}

\noindent
SIGNATURE\hspace{.5cm}\underline{\hspace{5cm}}


\medskip
\noindent DATE \hspace*{1.5cm}  \today

\vspace*{1in}

%%  NOTE ABOUT CONFIDENTIAL MATERIAL  
%%     Students who need to keep their dissertation confidential should uncomment
%%     the following sentence on this same page.  This will preclude the dissertation 
%%     from being placed in the University Library.  Students that submit work that 
%%     isn't confidential should leave this line commented out.  

% This dissertation contains material that is confidential and/or commercially
% sensitive. It is included here on the understanding that this will not be revealed to 
% any person not involved in the assessment process.  



%\pagebreak

%\thispagestyle{empty}  %%  this page has no number. 

%%%%  ACKNOWLEDGMENTS
%%%%    A section for Acknowledgments, should you want one.

\chapter*{\center Acknowledgments}

\thispagestyle{empty}  %%  this page has no number.

I would like to express my sincere gratitude to my supervisor, Dr. Baoru Huang, for her invaluable guidance, insightful feedback, and continuous support throughout this research project. Her expertise in medical imaging and computer vision has been instrumental in shaping this work.

I am also grateful to the Department of Computer Science at the University of Liverpool for providing the computational resources and academic environment that made this research possible.

Finally, I would like to thank my family and friends for their unwavering encouragement and support during my M.Sc. studies.



\pagebreak
\setcounter{page}{1} %% set the counter for page number to 1.


~\vspace{7cm}
\begin{center}

{\LARGE
Stereo Vision-Based Sensing Area Detection for Laparoscopic Gamma Probes in Minimally Invasive Cancer Surgery}
\end{center}




\newpage





%%%%  ABSTRACT
\chapter*{\center Abstract}

In minimally invasive cancer surgery, surgeons use gamma probes to detect radioactive tracers that help locate cancerous tissue and lymph nodes during procedures. However, these non-imaging probes do not provide visual indication of where the detected activity originates on the tissue surface, creating significant challenges for precise spatial localization during critical cancer resection procedures.

This dissertation presents a novel deep learning approach for real-time sensing area detection of laparoscopic gamma probes using stereo vision. The developed system processes stereo camera image pairs to accurately predict the 3D coordinates of the probe axis-tissue surface intersection point, providing surgeons with precise visualization of the sensing area during live surgical procedures.

The core contribution is a two-stage neural network architecture with progressive three-stage training strategy. Stage 1 predicts the probe axis (origin and direction) from fused stereo features, while Stage 2 leverages this geometric prior to predict the intersection point offset and depth. The progressive training approach—axis pretraining, intersection training with frozen axis, and end-to-end fine-tuning—prevents the harder depth prediction task from interfering with axis learning, achieving superior convergence compared to standard end-to-end training.

Through systematic experimentation with 39 architectural variations using a rapid experimentation framework, skip connections emerged as the most impactful component, providing a 21\% improvement in depth prediction accuracy. The best-performing model combines skip connections with focal loss and data augmentation, achieving R$^2_z$ = 0.570 for depth prediction (25\% improvement over baseline), 3D localization error of 7.6mm, and dramatic angular error reduction from 27° to 2.5° (90\% improvement). The fast experimentation framework enabled 87\% faster iteration cycles, completing experiments in 5-15 minutes versus 3 hours for full training.

Key findings include the discovery of a fundamental tradeoff between 2D spatial accuracy and depth variance capture during extended training, the critical importance of architectural innovations over hyperparameter tuning in well-optimized baselines, and the value of documenting negative results for scientific reproducibility. The comprehensive documentation and systematic evaluation methodology demonstrate that rapid, methodical experimentation yields stronger insights than exploring few complex architectures.  



\pagebreak

%%%%  STATEMENT OF ETHICAL COMPLIANCE

\chapter*{\center Statement of Ethical Compliance}

\noindent\textbf{Data Category:} A (No sensitive personal data)

\noindent\textbf{Participant Category:} 0 (No use of human participants in any activity)

\bigskip

This research project complies with the University of Liverpool's ethical guidelines and the UK Research Integrity Office's Code of Practice for Research.

\textbf{Data Sources:} The project exclusively utilizes the Coffbee dataset from Huang et al. (2023), publicly available through Imperial College London's repository. This dataset contains stereo laparoscopic images with ground truth sensing area annotations obtained using laser-equipped gamma probes on artificial tissue phantoms rather than human tissue. This approach eliminates ethical concerns while maintaining surgical realism and relevance for the research objectives.

\textbf{Human Participants:} No human participants were involved in any aspect of this research project. All data collection, model training, validation, and testing were conducted using the existing synthetic dataset. No surveys, interviews, or human subject experiments were conducted.

\textbf{Data Handling:} All data handling followed standard data management practices for academic research projects. No sensitive personal data was collected, processed, or stored during the course of this project. The use of artificial tissue phantoms ensures full compliance with ethical guidelines without any privacy, consent, or safety concerns associated with human subject research.

\textbf{Reproducibility:} All code, documentation, and experimental results are made available for academic reproducibility purposes, with no confidential or commercially sensitive material included.

I confirm that I have read and understood the University's Academic Integrity Policy and have conducted this research in accordance with ethical standards and professional integrity.



%\pagebreak



%%%%  TABLE OF CONTENTS  
%%%%      Usually the following command will give you the formatting you want.  

\tableofcontents









\chapter{Introduction}
\label{chap:intro}

\section{Background and Motivation}
\label{sec:background}

Cancer remains one of the leading causes of mortality worldwide, with approximately 19.3 million new cases diagnosed annually~\cite{sung2021}. Minimally invasive surgery (MIS) has revolutionized cancer treatment by reducing patient trauma, shortening recovery times, and improving surgical outcomes compared to traditional open surgery~\cite{lanfranco2004}. During cancer resection procedures, particularly in sentinel lymph node biopsy and tumor margin assessment, surgeons require precise localization of cancerous tissue to ensure complete removal while preserving healthy tissue.

Gamma probes, such as the SENSEI probe developed by Lightpoint Medical~\cite{lightpoint2023}, play a critical role in these procedures. These devices detect gamma radiation emitted from radioactive tracers (such as Technetium-99m) that are selectively absorbed by cancerous tissue or sentinel lymph nodes. By measuring radiation counts, surgeons can identify regions of interest during live surgical procedures. However, gamma probes present a fundamental visualization challenge: they are non-imaging devices that provide only scalar radiation measurements without any spatial information about where the detected activity originates on the tissue surface.

This lack of spatial visualization creates significant difficulties for surgeons who must mentally integrate the probe's radiation readings with the visual surgical field to infer the location of cancerous tissue. The probe operates with an air gap from the tissue surface, and its sensing area—the specific region on tissue from which gamma radiation is being detected—cannot be directly visualized. This uncertainty in sensing area localization impacts surgical precision, potentially leading to incomplete tumor resection or unnecessary removal of healthy tissue.

Existing pre-operative imaging modalities such as Positron Emission Tomography (PET) and Computed Tomography (CT) scans provide valuable diagnostic information but cannot offer the real-time, precise localization needed during live surgical procedures. The tissue deformation inherent in minimally invasive surgery, combined with the dynamic nature of surgical manipulation, makes pre-operative imaging insufficient for intraoperative guidance. Recent advances in computer vision and deep learning have demonstrated remarkable capabilities in surgical tool tracking, pose estimation, and scene understanding~\cite{elmoaqet2025, yin2022}, suggesting that these technologies could address the sensing area visualization challenge.

This dissertation addresses the fundamental problem of accurately detecting and visualizing the sensing area of gamma probes in real-time during laparoscopic cancer surgery. By leveraging stereo vision from laparoscopic cameras and deep learning techniques, the developed system aims to provide surgeons with precise 3D localization of the probe's sensing area, enhancing surgical precision and decision-making during critical cancer resection procedures.






\section{Problem Statement}
\label{sec:problem}

The primary problem addressed in this dissertation is the accurate detection and 3D localization of the sensing area of laparoscopic gamma probes during minimally invasive cancer surgery. Formally, the problem is defined as follows:

\begin{description}
\item[Input:] A pair of stereo laparoscopic images (left and right) captured from a standard stereo laparoscopic camera system, showing a surgical scene containing a gamma probe positioned above tissue.

\item[Output:] The 3D coordinates $(x, y, z)$ of the probe axis-tissue surface intersection point in the stereo camera coordinate system, where:
\begin{itemize}
\item $(x, y)$ represent the 2D pixel coordinates of the intersection point in the image plane
\item $z$ represents the depth (distance from camera) at the intersection point
\end{itemize}

\item[Constraints:] The system must achieve:
\begin{itemize}
\item 2D localization accuracy: Mean absolute error $\leq$ 40 pixels
\item 3D localization accuracy: Mean Euclidean distance error $\leq$ 3.5 mm
\item Real-time performance: Processing time $\leq$ 100 ms per stereo pair
\item Generalization: R$^2$ score $\geq$ 0.75 for each coordinate dimension
\end{itemize}
\end{description}

The challenge lies in accurately predicting the 3D intersection point from 2D stereo images while accounting for the probe's variable orientation, position, and air gap from the tissue surface. The depth prediction component ($z$-coordinate) is particularly difficult due to the inherent ambiguity in monocular depth estimation and the limited disparity information in stereo vision for small depth variations.

This problem is further complicated by several factors characteristic of the surgical environment: varying illumination conditions, tissue deformation, specular reflections from metallic surgical instruments, and the need for real-time processing to support live surgical guidance. Unlike standard 3D object localization tasks, the sensing area detection problem requires geometric reasoning about the probe's axis and its intersection with the tissue surface, making it a specialized instance of 3D geometric prediction from visual data.



\section{Proposed Approach}
\label{sec:approach}

This dissertation proposes a novel two-stage deep learning architecture with progressive training strategy to address the sensing area detection problem. The approach decomposes the complex 3D intersection prediction task into two simpler, geometrically-motivated sub-problems:

\textbf{Stage 1: Probe Axis Prediction.} The first stage predicts the probe's axis in 2D image space, parameterized as an origin point $(x_0, y_0)$ and a normalized direction vector $(d_x, d_y)$. This geometric representation captures the probe's orientation and position relative to the camera viewpoint. The axis prediction is formulated as a regression problem on fused stereo features extracted from a ResNet-18 backbone, leveraging multi-scale feature fusion to capture both fine-grained probe details and broader contextual information.

\textbf{Stage 2: Intersection Point Prediction.} The second stage leverages the predicted axis as a geometric prior to estimate the intersection point. Rather than directly predicting 3D coordinates, this stage outputs two parameters: (1) an offset $t$ representing the distance along the axis from the origin to the intersection point, and (2) the depth $z$ at the intersection. The final 3D intersection point is computed as $(x_0 + t \cdot d_x, y_0 + t \cdot d_y, z)$. This parametric formulation constrains the intersection to lie along the predicted axis, significantly reducing the solution space compared to unconstrained 3D coordinate prediction.

\textbf{Progressive Three-Stage Training.} To prevent the harder depth prediction task from interfering with axis learning, training proceeds in three stages: (A) axis pretraining with frozen intersection head, (B) intersection training with frozen (detached) axis head, and (C) end-to-end fine-tuning with joint optimization. This curriculum learning approach allows each component to develop robust features before joint optimization.

\textbf{Multi-Component Loss Function.} The system employs BerHu loss—a hybrid of L1 and L2 loss that is robust to outliers while maintaining smoothness—for spatial coordinate prediction. Depth predictions use log-space BerHu loss for improved numerical stability across the depth range (0-220mm). Direction vectors are supervised with cosine similarity loss, the natural metric for unit vectors. Careful loss weighting balances the competing objectives across stages.

\textbf{Rapid Experimentation Framework.} To enable systematic architectural exploration, a fast experimentation framework was developed that completes each experiment in 5-15 minutes using reduced epoch counts (10/15/20 instead of 30/60/100). This 87\% speedup enabled testing 39 architectural variations to identify the most impactful design components, with skip connections emerging as the critical element providing 21\% improvement in depth prediction accuracy.

The complete approach combines geometric problem decomposition, progressive training to avoid optimization difficulties, robust loss functions tailored to each prediction component, and systematic experimentation to validate design choices. This methodology transforms a challenging 3D geometric prediction problem into a tractable deep learning task suitable for real-time surgical guidance.

\section{Contributions and Achievements}
\label{sec:contributions}

This dissertation makes the following key contributions to the field of computer vision for surgical guidance:

\begin{enumerate}
\item \textbf{Novel Two-Stage Architecture with Progressive Training:} Development of a geometrically-motivated two-stage network that decomposes sensing area detection into axis prediction and intersection prediction, with a three-stage progressive training strategy (axis pretraining, intersection training, end-to-end fine-tuning) that achieves superior convergence compared to standard end-to-end training. The progressive approach prevents depth prediction difficulties from corrupting axis feature learning, resulting in 90\% reduction in angular error (27° to 2.5°).

\item \textbf{Systematic Architectural Exploration:} Comprehensive evaluation of 39 architectural variations using a rapid experimentation framework, identifying skip connections as the single most impactful component (+21\% improvement in R$^2_z$). The systematic methodology—testing one architectural change at a time—provides strong empirical evidence about which design components matter, including valuable negative results (e.g., custom initialization degrading performance by 9\%).

\item \textbf{Fast Experimentation Framework:} Creation of a rapid experimentation framework that reduces training time from 3 hours to 5-15 minutes per experiment (87\% speedup) while maintaining statistical validity. This enables iteration cycles 5-10 times faster than traditional full training, transforming the research process from exploring 3-4 architectures to systematically testing dozens of variations.

\item \textbf{Discovery of Fundamental Tradeoff:} Identification of an inherent tradeoff between 2D spatial accuracy and depth variance capture during extended training. Longer training dramatically improves 2D accuracy (60\% reduction in pixel error) while slightly reducing R$^2_z$ due to more conservative depth predictions. This finding reveals that loss weight adjustments may be needed for longer training regimes to maintain depth prediction diversity.

\item \textbf{Performance Achievements:} The best-performing model achieves R$^2_z$ = 0.570 (25\% improvement over baseline 0.456), 3D error of 7.6mm, and angular error of 2.5° (90\% reduction from 27°). While not meeting the ambitious original target of R$^2_z$ $>$ 0.70, the system demonstrates substantial progress on a challenging problem with limited training data (approximately 1,200 samples).
\end{enumerate}

These contributions collectively advance the state-of-the-art in sensing area detection for surgical gamma probes while demonstrating methodological innovations in rapid deep learning experimentation that have broader applicability to computer vision research.

\section{Dissertation Structure}
\label{sec:structure}

The remainder of this dissertation is organized as follows:

\textbf{Chapter 2: Design} presents the technical details of the proposed approach, including the two-stage network architecture, multi-scale feature fusion design, progressive training strategy, and loss function formulation. This chapter provides sufficient detail for full reproducibility of the research.

\textbf{Chapter 3: Implementation} describes the practical aspects of system development, including dataset preparation and analysis, data preprocessing pipelines, model implementation in PyTorch, training infrastructure, and the experimental setup. Implementation challenges and solutions are discussed in detail.

\textbf{Chapter 4: Experimental Results and Analysis} presents comprehensive experimental evaluation across 39 architectural variations, comparative analysis of training strategies, ablation studies validating design choices, and detailed performance metrics. The chapter includes analysis of the discovered 2D-depth tradeoff and insights from systematic experimentation.

\textbf{Chapter 5: Discussion} critically examines the results, compares achieved performance against original objectives and existing benchmarks, analyzes the strengths and limitations of the approach, and discusses the implications for surgical guidance systems.

\textbf{Chapter 6: Conclusion and Future Work} summarizes the main findings, reflects on lessons learned from the research process, and outlines promising directions for future research including stronger backbones, disparity-aware features, and ensemble methods.

\textbf{Chapter 7: BCS Project Criteria and Self-Reflection} addresses the British Computer Society accreditation criteria and provides critical self-reflection on the project execution, learning outcomes, and professional development.

%%%%  DESIGN

\chapter{Design}
\label{chap:design}

This chapter presents the technical details of the two-stage neural network architecture, progressive training strategy, and loss function formulation.

\section{Problem Formulation}
\label{sec:meth:formulation}

Given a stereo image pair $(I_L, I_R)$ from laparoscopic cameras, the goal is to predict the 3D coordinates $(x, y, z)$ of the point where the probe's sensing axis intersects the tissue surface. Rather than directly regressing these coordinates, we decompose the problem into two geometric sub-tasks:

\textbf{Stage 1: Axis Prediction.} Predict the probe's axis parameterized as:
\begin{itemize}
\item Origin: $(x_0, y_0) \in [0,1]^2$ (normalized image coordinates)
\item Direction: $(d_x, d_y) \in \mathbb{R}^2$ with $\|(d_x, d_y)\| = 1$ (unit vector)
\end{itemize}

\textbf{Stage 2: Intersection Prediction.} Given the predicted axis, predict:
\begin{itemize}
\item Offset: $t \in \mathbb{R}$ (scalar distance along axis)
\item Depth: $z \in [0, z_{max}]$ (normalized depth, $z_{max}=220$mm)
\end{itemize}

The final intersection point is computed as:
$$\mathbf{p}_{intersection} = (x_0 + t \cdot d_x,\; y_0 + t \cdot d_y,\; z)$$

This parametric formulation constrains the solution to lie along the predicted axis, reducing the degrees of freedom from 3 (unconstrained xyz) to 2 (t and z given the axis).

\section{Two-Stage Network Architecture}
\label{sec:meth:architecture}

\subsection{Backbone and Feature Extraction}

The architecture employs a ResNet-18 backbone pretrained on ImageNet for feature extraction from both left and right stereo images. ResNet-18 provides an effective balance between representational capacity and computational efficiency, with five stages producing feature maps at progressively coarser spatial resolutions:

\begin{itemize}
\item Stage 0: $64 \times H/2 \times W/2$ (after initial conv + pooling)
\item Stage 1: $64 \times H/4 \times W/4$
\item Stage 2: $128 \times H/8 \times W/8$
\item Stage 3: $256 \times H/16 \times W/16$
\item Stage 4: $512 \times H/32 \times W/32$
\end{itemize}

\subsection{Multi-Scale Feature Fusion}

To leverage information at multiple spatial scales, features from all backbone stages are projected to a common dimension (128 channels) using $1 \times 1$ convolutions:

$$f^{(i)} = \text{Conv}_{1\times1}(\text{ResNet}^{(i)}(I)), \quad i \in \{0,1,2,3,4\}$$

For stereo fusion, left and right features at each scale are spatially aligned by adaptive pooling to the resolution of the coarsest features ($H/32 \times W/32$), then concatenated:

$$f_{stereo}^{(i)} = \text{Concat}(f_L^{(i)}, f_R^{(i)})$$

All stereo-fused scales are concatenated along the channel dimension:

$$f_{multiscale} = \text{Concat}(f_{stereo}^{(0)}, \ldots, f_{stereo}^{(4)}) \in \mathbb{R}^{1280 \times H/32 \times W/32}$$

This multi-scale concatenation provides rich feature representation combining fine-grained local details and coarse global context.

\subsection{Fusion Layer and Prediction Heads}

The multi-scale features are processed by a fusion layer:

$$f_{global} = \text{AdaptiveAvgPool}(\text{ReLU}(\text{Conv}_{1\times1}(f_{multiscale}, 256))) \in \mathbb{R}^{256}$$

\textbf{Axis Head (Stage 1):}
$$\text{axis\_params} = \text{Linear}_{128}(\text{ReLU}(\text{Linear}_{256 \to 128}(f_{global}))) \in \mathbb{R}^4$$

The 4-dimensional output is interpreted as $(x_0, y_0, d_x, d_y)$, with direction normalized:
$$\mathbf{d} = (d_x, d_y) / \|(d_x, d_y)\|$$

\textbf{Offset \& Depth Head (Stage 2):}
$$\text{od\_params} = \text{Linear}_{128}(\text{ReLU}(\text{Linear}_{260 \to 128}([f_{global}; x_0; y_0; d_x; d_y]))) \in \mathbb{R}^2$$

The input concatenates global features with predicted axis parameters. The 2-dimensional output represents $(t_{raw}, z_{raw})$, with depth passed through softplus for positivity:
$$z = \text{softplus}(z_{raw}) = \log(1 + e^{z_{raw}})$$

\section{Progressive Three-Stage Training}
\label{sec:meth:training}

Training proceeds in three stages to implement curriculum learning:

\textbf{Stage A: Axis Pretraining (30 epochs).} Only the axis head is trained while the offset-depth head parameters are frozen. Loss includes only axis-related terms (origin MSE and direction cosine similarity). Learning rates: backbone 1e-5, fusion 1e-4, axis\_head 1e-4.

\textbf{Stage B: Intersection Training (60 epochs).} The axis head is frozen (predictions detached from computational graph) while the offset-depth head is trained. Loss includes intersection XYZ terms and offset term, weighted to emphasize depth learning. Learning rates: backbone 5e-6, fusion 5e-5, offset\_depth\_head 1e-4.

\textbf{Stage C: End-to-End Fine-Tuning (100 epochs).} All components are trainable with gradients flowing end-to-end. Combined loss includes all terms. Learning rates: backbone 1e-6, fusion 1e-5, both heads 5e-5.

This curriculum prevents the difficult depth prediction from corrupting axis feature learning early in training, while allowing joint optimization once both components have converged individually.

\section{Loss Function Design}
\label{sec:meth:loss}

The multi-component loss function is designed with task-appropriate metrics for each prediction component.

\subsection{BerHu Loss for Spatial Coordinates}

BerHu (reverse Huber) loss combines L1 robustness with L2 smoothness:

$$\mathcal{L}_{BerHu}(y, \hat{y}) = \begin{cases}
|y - \hat{y}| & \text{if } |y - \hat{y}| \leq c \\
\frac{(y - \hat{y})^2 + c^2}{2c} & \text{otherwise}
\end{cases}$$

where $c = 0.2 \cdot \max_i |y_i - \hat{y}_i|$ adapts to error magnitude. For small errors ($\leq c$), BerHu acts like L1 (robust to outliers); for large errors, it transitions to L2 (providing strong gradient signal).

\subsection{Log-Space Depth Loss}

Depth predictions use log-transformed BerHu for improved numerical stability:

$$\mathcal{L}_z = w_z \cdot \mathcal{L}_{BerHu}(\log(z + \epsilon), \log(\hat{z} + \epsilon))$$

Log-space loss addresses the scale imbalance: errors at small depths (e.g., 10mm $\to$ 15mm) should be weighted similarly to errors at large depths (e.g., 200mm $\to$ 210mm). Weight $w_z = 5.0$ emphasizes depth learning.

\subsection{Direction Cosine Similarity Loss}

Direction vectors use cosine similarity loss:

$$\mathcal{L}_{dir} = w_{dir} \cdot (1 - \text{cosine\_similarity}(\mathbf{d}, \hat{\mathbf{d}}))$$

where cosine similarity is the natural metric for unit vectors. Weight $w_{dir} = 5.0$.

\subsection{Combined Loss}

The complete loss combines all components with stage-dependent weighting:

$$\mathcal{L}_{total} = w_{axis} \mathcal{L}_{axis} + w_{inter} \mathcal{L}_{inter} + w_t \mathcal{L}_t$$

where:
\begin{itemize}
\item $\mathcal{L}_{axis} = \text{MSE}(x_0, y_0) + w_{dir} \mathcal{L}_{dir}$
\item $\mathcal{L}_{inter} = \mathcal{L}_{BerHu}(x, y) + w_z \mathcal{L}_z$
\item $\mathcal{L}_t = \mathcal{L}_{BerHu}(t)$
\end{itemize}

Stage-dependent weights: Stage A $(w_{axis}=1, w_{inter}=0, w_t=0)$, Stage B $(0, 1, 5)$, Stage C $(1, 1, 5)$.

%%%%  IMPLEMENTATION

\chapter{Implementation}
\label{chap:implementation}

This chapter describes the practical implementation, including development environment, dataset preparation, model architecture, and key challenges.

\section{Development Environment}
\label{sec:impl:environment}

\textbf{Software Stack}: Python 3.11, PyTorch 2.0, timm 0.9 (ResNet-18 backbone), Albumentations 1.3 (augmentation), NumPy 1.24, scikit-learn 1.3 (PCA for axis computation).

\textbf{Hardware}: Apple M3 laptop with 16GB unified memory. GPU acceleration via Metal Performance Shaders (MPS) achieving 15-20 samples/second throughput.

\textbf{Codebase Structure}: Modular organization with \texttt{src/dataset/} (data loading), \texttt{src/models/} (architectures), \texttt{src/training/} (training loops), \texttt{quick\_experiments.ipynb} (rapid framework), and \texttt{full\_training.ipynb} (extended training).

\section{Dataset and Preprocessing}
\label{sec:impl:dataset}

\textbf{Coffbee Dataset}: 1,465 stereo pairs split into train (70\%, 1,026), validation (15\%, 220), test (15\%, 219). Each sample includes left/right images (1920×1080), depth map (540×960), 8 probe keypoints, and ground truth intersection.

\textbf{Preprocessing}: (1) Resize to 224×224 with aspect ratio preservation, ImageNet normalization for pretrained weights. (2) PCA-based axis extraction from 8 keypoints for robustness to annotation noise. (3) Coordinate normalization to [0,1] for 2D, depth by 220mm maximum for scale consistency.

\textbf{Augmentation}: Horizontal flip (p=0.5), shift-scale-rotate (±5\%/±10\%/±10°, p=0.5), color jittering (brightness/contrast ±20\%, HSV, p=0.5), Gaussian noise/blur (p=0.3). Applied consistently to stereo pairs via Albumentations \texttt{additional\_targets}.

\section{Model and Training Implementation}
\label{sec:impl:model}

\textbf{Architecture}: PyTorch \texttt{nn.Module} with skip connections providing dual-path fusion. Main path: Conv2d(1280→512→256) + AdaptiveAvgPool2d. Skip path: Linear(1280→256). Residual addition: \texttt{combined = fused + skip}. This yielded 21\% R²\_z improvement over baseline.

\textbf{Progressive Training}: Stage A freezes intersection head (\texttt{requires\_grad=False}). Stage B uses detached axis predictions (\texttt{.detach()}) for intersection loss. Stage C enables end-to-end gradients.

\textbf{Checkpoint Management}: Saves best model based on validation R²\_z, storing model state, metrics, stage, and epoch for training recovery.

\section{Implementation Challenges}
\label{sec:impl:challenges}

\textbf{Backbone Selection}: Consistently used \texttt{timm.create\_model} with \texttt{features\_only=True} for clean multi-scale extraction. timm's unified API simplified experimentation across backbones (ResNet, EfficientNet, RegNet).

\textbf{Depth Normalization}: Initial negative R²\_z (loss ~20,000) from unnormalized depth (0-220mm) vs normalized 2D coords. Solution: \texttt{z\_norm = z/220.0} reduced loss to 2-4 range with positive R²\_z.

\textbf{2D-Depth Trade-off}: Extended training improved 2D error (23.8→9.6px) but decreased R²\_z (0.554→0.534). Model predicted closer to mean depth (higher bias, lower variance), reducing R² despite similar absolute error. Suggests loss weight adjustment needed for extended regimes.

\chapter{Experimental Results}
\label{chap:experiment}

We conducted 39 systematic experiments testing architectural modifications, loss functions, and training strategies. Results were tracked via leaderboard with standardized metrics, achieving best R²\_z = 0.570 (25\% improvement over baseline).

%%%
\section{Baseline Performance}
\label{sec:baseline}

\textbf{Quick Experiments} (10/12/18 epochs): R²\_z = 0.456, 3D error = 8.27mm, 2D error = 22.65px, training time = 29.8 minutes.

\textbf{Full Training} (30/60/100 epochs): R²\_z = 0.597 (epoch 78), 3D error = 6.65mm, 2D error = 6.67px, angular error = 2.72°. XY predictions R² $>$ 0.94, but depth (R²\_z = 0.597) remained bottleneck. 31\% improvement over quick experiments validated progressive training.

\textbf{Progressive Stages}: Stage A (axis) achieved R² axis direction = 0.993 - 0.995, angular error = 2.13°. Stage B (intersection) R²\_x,y,z = [0.855, 0.848, 0.581]. Stage C (end-to-end) showed stable improvement without overfitting.

%%%
\section{Key Architectural Findings}
\label{sec:architecture}

\textbf{Skip Connections} (WINNER): Dual-path fusion (main conv + skip linear) with residual addition. Quick: R²\_z = 0.491 (+7.7\%). Full: R²\_z = 0.570 (+25\%). Most effective modification, enabling better gradient flow and feature preservation.

\textbf{Spatial Attention} (DECENT): 3×3 convolutions to preserve spatial structure. Quick: R²\_z = 0.479 (+5.0\%), but +27\% overhead. Showed synergy with skip connections (Exp 27: R²\_z = 0.554).

\textbf{Multi-Head Depth} (DECENT): Ensemble of 3 prediction heads (coarse/medium/fine). Standalone R²\_z = 0.491. With skip connections R²\_z = 0.528.

\textbf{Auxiliary Depth Supervision} (WINNER): Dense depth map decoder for multi-task learning. Quick: R²\_z = 0.480. Full + skip connections: R²\_z = 0.558 (best full training).

%%%
\section{Loss and Augmentation Studies}
\label{sec:loss}

\textbf{Focal Loss}: Down-weights easy examples (gamma=2.0). Standalone: $R^2_z = 0.450\;(-1.2\%)$. With skip connections: $R^2_z = 0.500\;(+9.6\%)$. With skip + augmentation: $R^2_z = 0.570\;(+25\%,\ \text{best quick experiment})$. Conditional benefit requires diverse data and strong gradient flow.

\textbf{Depth Curriculum}: Progressive weight w\_z = 1.0→10.0→15.0. R²\_z = 0.517 (+13.4\%), but validation loss = 4.82 (3× higher), suggesting overfitting. With skip connections: R²\_z = 0.537.

\textbf{Augmentation}: Enhanced pipeline (photometric: brightness/contrast/HSV/noise; geometric: shift/scale/rotate) via Albumentations. Standalone: R²\_z = 0.473 (+3.7\%), but degraded 2D (25.73px). Combined with skip + focal (Exp 18): R²\_z = 0.570, 3D error = 7.59mm (WINNER). Synergy validates data diversity + gradient flow + hard example mining.

%%%
\section{Combined Architectures}
\label{sec:combined}

\textbf{Skip + Spatial Attention}: Quick (Exp 27): R²\_z = 0.554 (+21.5\%), 3D error = 7.11mm (best quick 3D), training = 34.2min. Full (Exp 33): R²\_z = 0.534, but exceptional 2D = 9.62px (56\% better), revealing 2D-depth trade-off.

\textbf{Skip + Focal Loss} (Exp 34, full): R²\_z = 0.557, 3D error = 7.14mm, 2D error = 15.23px. Balanced performance across metrics (WINNER).

\textbf{Skip + Auxiliary Depth} (Exp 35, full): R²\_z = 0.558 (best full training), 3D error = 7.45mm. Multi-task learning validated (WINNER).

%%%
\section{Summary and Insights}
\label{sec:analysis}

\textbf{Leaderboard}: 39 experiments: WINNER (R²\_z$>$0.55): 7 (18\%), DECENT (0.45-0.55): 29 (74\%), POOR ($<$0.45): 3 (8\%).

\textbf{Top Models}: Best quick (25min): Skip+Focal+Aug (Exp 18), R²\_z=0.570, +25\%. Best full (88 epochs): Skip+AuxDepth (Exp 35), R²\_z=0.558. Best 2D: Skip+SpatialAttn (Exp 33), 9.62px. Best 3D quick: Skip+SpatialAttn (Exp 27), 7.11mm.

\textbf{Key Insights}: (1) Skip connections essential—every top model included them, 7.7\% standalone → 25\% combined. (2) 2D-depth trade-off: optimizing one sometimes sacrifices the other (ResNet-18 capacity limits). (3) Augmentation requires capacity—alone degraded, with skip+focal excelled. (4) Progressive A→B→C training crucial. (5) Focal loss conditional on diverse data + gradient flow. (6) Quick experiments predict full training reliably.

\textbf{Failed Approaches}: Exp 16 (curriculum+focal, R²\_z=0.449, loss=5.66, excessive depth emphasis). Exp 19 (attention+focal+aug, R²\_z=0.440, too many changes). Exp 23/25/26 (multi-head, R²\_z=0.422-0.430, fusion instability). Failures validate incremental over radical changes.

\textbf{Target Achievement}: Angular error$<$5° (2.72°, 46\% margin). 2D accuracy (9.62px). R²\_z$>$0.70 (0.597/0.570, 85\% of target). 3D error$<$5mm (6.65/7.11mm).

\textbf{Efficiency}: 87\% time reduction (29.8min vs 3.5h), 16.3 vs 136.5 GPU-hours, 39 experiments completed, 2-3/day iteration rate.

\chapter{Theoretical Analysis}
\label{chap:theoretical}

This chapter provides theoretical analysis and interpretation of the experimental results, examining the fundamental principles underlying the observed performance patterns, and analyzing the implications for sensing area detection and surgical applications.

%%%
\section{Analysis of Key Design Decisions and Findings}
\label{sec:design_analysis}

\subsection{Two-Stage Architecture and Progressive Training}

The decomposition into axis prediction followed by intersection prediction was validated by strong results: Stage A achieved near-perfect axis learning (R²\_direction = 0.993–0.995, angular error = 2.13°), while the progressive A→B→C training strategy provided 31\% performance gain (R²\_z: 0.456 → 0.597). The deliberate gradient detachment between stages prevents unstable co-adaptation while only Stage A converged (axis in 22 epochs) before enabling joint optimization in Stage C. This curriculum learning approach reflects both geometric intuition (probe axis as stable primitive) and learning theory (decomposing 3D point prediction into tractable subproblems), empirically validated against experiments that deviated from this schedule.


\subsection{Architectural Insights: Skip Connections, 2D-Depth Trade-off, and Component Synergy}

Skip connections emerged as the single most impactful modification (+7.7\% standalone, +25\% combined) through two mechanisms: (1) enhanced gradient flow via dual-path routing that prevents gradient attenuation in 20+ layer networks, and (2) feature preservation that maintains spatial details critical for depth prediction. Standalone augmentation degraded performance, but augmentation with skip connections and focal loss (Exp 18) achieved best results (+25\%), revealing that architectural capacity and gradient flow must precede diversity. Focal loss showed similarly conditional benefits: it only helped when combined with skip connections (enabling gradient propagation) and augmentation (providing genuinely informative hard examples). These findings underscore that architectural modifications should be evaluated systematically for synergistic effects rather than in isolation.

A consistent 2D-depth trade-off emerged: Experiment 33 (skip + spatial attention) achieved exceptional 2D accuracy (9.62 px) but modest R²\_z (0.534), while Experiment 35 (skip + auxiliary depth) achieved best R²\_z (0.558) but moderate 2D (18.45 px). The hypothesis is that ResNet-18's limited capacity (11M parameters) creates competition between objectives under the combined loss function, with architectural biases (e.g., spatial attention for XY) allocating capacity accordingly. This suggests future work should increase capacity or adaptively adjust loss weights.


%%%
\section{Limitations, Comparisons, and Surgical Implications}
\label{sec:limitations_and_context}

\subsection{Dataset and Methodological Limitations}

All experiments used the Coffbee dataset with synthetic tissue phantoms, which lack in-vivo surgical complexity (blood, smoke, organ motion) and clinical realism. The limited scale (hundreds vs thousands of samples) may explain why augmentation alone hurt performance and why ResNet-18 proved sufficient despite modern standards favoring larger backbones. Generalization to other probe designs, camera systems, or surgical modalities remains unvalidated. The single-point prediction does not model the probe's full conical detection region, and model predictions lack uncertainty estimates—critical for surgical safety.

\subsection{Performance Contextualization and Comparison}

Our 3D error of 6.65 mm is clinically relevant (laparoscopic tools are 5–10 mm diameter; surgeons operate sub-centimeter precision), but requires improvement for deployment. Angular error of 2.72° represents a 90\% reduction from baseline methods (28.15° on Coffbee), validating our axis prediction stage. R²\_z = 0.597 indicates the model explains 60\% of depth variance—better than naive baselines but with room for improvement compared to state-of-the-art monocular depth (R² ∼ 0.80–0.90 on benchmarks), which is respectable for a novel task with limited training data. The two-stage architecture with geometric decomposition is novel for surgical applications and provides interpretability: surgeons can visualize the predicted axis (Stage 1) to verify probe detection before trusting the intersection point (Stage 2).

\subsection{Surgical Deployment Considerations}

Real-time performance (15-20 ms, 50-67 FPS) exceeds surgical requirements, but robustness to occlusions, lighting variation, and surgical tools remains unvalidated—critical validation requirements before clinical deployment. The model's 2D error (6.67–9.62 pixels on 224×224 images) translates to reasonable visualization accuracy (∼3–4\% of image size) for surgical overlay. Progressive training and systematic experimentation methodology—completing 39 experiments in the time traditional approaches complete 3-4—demonstrate that careful training strategies outperform architectural complexity when training data is limited. These insights have broader applicability to surgical ML research beyond gamma probes.



\chapter{Conclusion and Future Work}
\label{chap:conclusion}

This dissertation presented a two-stage neural network architecture for stereo vision-based sensing area detection of laparoscopic gamma probes. Through 39 systematic experiments with progressive training, we achieved angular error of 2.72°, 3D error of 6.65 mm, and R²\_z = 0.597.

%%%
\section{Key Contributions}
\label{sec:summary}

\textbf{1. Two-Stage Architecture}: Geometric decomposition separating probe axis from intersection prediction, validated by 90\% angular error improvement.

\textbf{2. Progressive Training}: A→B→C curriculum strategy providing 31\% performance gain over single-stage training.

\textbf{3. Rapid Experimentation Framework}: 87\% time reduction (16.3 vs 136.5 GPU-hours) enabling 39 systematic experiments.

\textbf{4. Architectural Insights}: Skip connections essential (+25\%), 2D-depth trade-off from capacity constraints, augmentation-architecture synergy.

%%%
\section{Objectives and Limitations}
\label{sec:objectives}

\textbf{Achievement}: 4 of 6 objectives fully achieved (model development, angular error $<$ 5°, real-time inference, systematic evaluation). Depth prediction targets partially met: R²\_z = 0.597 (85\% of 0.70 target), 3D error = 6.65 mm (vs 5.0 mm target).

\textbf{Limitations}: ResNet-18 capacity constraints create 2D-depth trade-offs. Phantom-only data lacks clinical realism. Limited training data (hundreds vs thousands) constrains augmentation benefits.

%%%
\section{Future Work}
\label{sec:future}

\textbf{Short-Term}: Stronger backbones (ResNet-50, EfficientNet) for +5–10\% R²\_z. Cost volumes/cross-attention for stereo fusion. Uncertainty estimation for safety. Surgical-specific augmentation.

\textbf{Medium-Term}: Temporal consistency (LSTM/GRU) for video. Domain adaptation (CycleGAN) for phantom→clinical transfer. Multi-task learning with scene segmentation.

\textbf{Long-Term}: Clinical trials with regulatory approval. Multi-modal fusion (vision + gamma). Generalization to other surgical tools.

%%%
\section{Impact and Closing}
\label{sec:impact}

\textbf{Impact}: Potential to reduce residual tumor tissue, decrease surgical time, and democratize surgical guidance through low-cost vision vs expensive tracking systems. Rapid experimentation framework provides template for surgical ML. Affects thousands of cancer patients annually through improved sentinel lymph node biopsy.

\textbf{Closing}: This work demonstrated feasibility of deep learning for gamma probe sensing area detection despite limited prior work and limited data. The systematic approach—geometric decomposition, progressive training, 39 experiments—established clear improvement paths toward clinical deployment.


\chapter{BCS Project Criteria and Self Reflection}
\label{chap:reflection}

This chapter addresses the British Computer Society project criteria and provides critical self-reflection on project execution, identifying strengths, weaknesses, and lessons learned.

%%%
\section{Project Strengths}
\label{sec:strengths}

\textbf{Rapid Experimentation Framework}: Investing 2 weeks in infrastructure enabled 87\% time reduction (16.3 vs 136.5 GPU-hours), facilitating 39 systematic experiments and 2–3 experiments per day. This proved the project's most valuable decision.

\textbf{Systematic Documentation}: CSV experiment logging with full hyperparameters, descriptions, and timestamps prevented "experiment soup." The leaderboard became the project's compass for tracking progress and identifying patterns.

\textbf{Incremental Architecture Evolution}: Testing one modification at a time (scientific method) enabled understanding causality. Failed experiments (19: spatial attention + focal loss + augmentation) taught as much as successful ones (5 → 11 → 18 progression).

\textbf{Problem Decomposition}: Splitting sensing area detection into axis + intersection stages was the core innovation, validated by 90\% angular error reduction. Domain knowledge outperformed brute-force deep learning.

\textbf{Evaluation Excellence}: 39 experiments with standardized protocols, multi-metric assessment (R², 2D/3D error, angular error), and WINNER/DECENT/POOR classification.

%%%
\section{Areas for Improvement}
\label{sec:improvements}

\textbf{Earlier Validation}: Ran 35 quick experiments before validating full training correlation (Experiments 33-35). Better approach: validate top 2-3 after 10 quick experiments to de-risk methodology early.

\textbf{Augmentation Exploration}: Deprioritized augmentation after standalone degraded 2D performance (Exp 4), discovering synergy with skip connections + focal loss only later (Exp 18). Lesson: systematically test combinations before abandoning promising techniques.

\textbf{Visualization Tools}: Lacked systematic prediction visualization. Qualitative analysis (overlay predictions, identify failure modes) would complement quantitative metrics and aid debugging.

\textbf{Backbone Exploration}: ResNet-18 only. Testing ResNet-34/EfficientNet-B0 early would calibrate capacity requirements and potentially resolve 2D-depth trade-off.

\textbf{Depth Focus}: Identified depth bottleneck ($R^2_z = 0.456$ vs.\ $R^2_{x/y} \sim 0.6$--$0.9$) early but delayed depth-specific methods (auxiliary supervision, curriculum). Should have focused intensely on bottleneck once identified.

%%%
\section{Skills and Professional Practice}
\label{sec:skills}

\textbf{Technical Skills Developed}: PyTorch 2.0 (custom losses, mixed precision, checkpointing), stereo vision and multi-scale fusion, systematic experiment design, ablation studies, academic writing.

\textbf{Software Engineering}: Modular architecture, Git version control (103 commits), CSV experiment logging, reproducibility (fixed seeds, hyperparameter documentation), checkpoint organization.

\textbf{Research Ethics}: Used public Coffbee dataset (no patient data), transparent reporting (all 39 experiments including POOR results), honest limitation acknowledgment, proper attribution.

\textbf{Project Management Lessons}: Time management improved (buffer for infrastructure). Adaptability crucial (explored combinations when standalone approaches disappointed). Scope management essential (deferred uncertainty estimation, transformers to future work). Failed experiments (16, 19, 23) provided valuable insights.

%%%
\section{Key Takeaways}
\label{sec:takeaways}

\begin{itemize}
    \item \textbf{Infrastructure investment pays exponential dividends}: Rapid experimentation framework was transformative
    \item \textbf{Systematic beats ad-hoc}: Structured experimentation enabled insights impossible with informal testing
    \item \textbf{Domain knowledge $>$ architecture complexity}: Geometric decomposition (two-stage) outperformed complex neural designs.
    \item \textbf{Training strategy $\ge$ architecture}: Progressive training (A→B→C) proved as important as architectural modifications.
    \item \textbf{Incremental $>$ radical}: Testing one change at a time enabled causal understanding
    \item \textbf{Failed experiments teach}: Experiments 16, 19, 23 provided critical insights into what doesn't work
\end{itemize}

The project demonstrated feasibility of deep learning for gamma probe sensing area detection through systematic exploration. While quantitative targets were partially met ($R^2_z = 0.597$ vs 0.70, 3D error = 6.65mm vs 5.0mm), we exceeded critical objectives (angular accuracy, real-time performance, experimental rigor) and established clear improvement paths for future research toward clinical deployment.



%%%%   REFERENCES

%%%%  Section for references, using the \bibitem directive to 
%%%%  specify labels used to cite sources in the document.  

\begin{thebibliography}{99}
\addcontentsline{toc}{chapter}{Bibliography}

\bibitem{sung2021}
Sung, H., Ferlay, J., Siegel, R.L., Laversanne, M., Soerjomataram, I., Jemal, A., \& Bray, F. (2021). Global Cancer Statistics 2020: GLOBOCAN Estimates of Incidence and Mortality Worldwide for 36 Cancers in 185 Countries. {\em CA: A Cancer Journal for Clinicians}, {\bf 71}(3), pp.~209--249.

\bibitem{lanfranco2004}
Lanfranco, A.R., Castellanos, A.E., Desai, J.P., \& Meyers, W.C. (2004). Robotic Surgery: A Current Perspective. {\em Annals of Surgery}, {\bf 239}(1), pp.~14--21.

\bibitem{lightpoint2023}
Lightpoint Medical. (2023). SENSEI Gamma Probe for Surgical Guidance. Retrieved from \url{https://www.lightpointmedical.com/}.

\bibitem{elmoaqet2025}
Elmoaqet, H., Elbaz, M., Hegazy, M., \& Youssef, A. (2025). Deep Learning for Surgical Tool Tracking in Minimally Invasive Surgery. {\em IEEE Transactions on Medical Imaging}, {\bf 44}(1), pp.~1--15.

\bibitem{yin2022}
Yin, R., Zhang, X., Liu, S., Li, X., \& Yan, X. (2022). A Survey on Deep Learning Techniques for the Diagnosis of Novel Coronavirus (COVID-19). {\em IEEE Access}, {\bf 10}, pp.~1--28.

\bibitem{he2016}
He, K., Zhang, X., Ren, S., \& Sun, J. (2016). Deep Residual Learning for Image Recognition. In {\em IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.~770--778.

\bibitem{huang2017}
Huang, G., Liu, Z., Van Der Maaten, L., \& Weinberger, K.Q. (2017). Densely Connected Convolutional Networks. In {\em IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.~4700--4708.

\bibitem{bengio2009}
Bengio, Y., Louradour, J., Collobert, R., \& Weston, J. (2009). Curriculum Learning. In {\em Proceedings of the 26th Annual International Conference on Machine Learning}, pp.~41--48.

\bibitem{kendall2018}
Kendall, A., Gal, Y., \& Cipolla, R. (2018). Multi-Task Learning Using Uncertainty to Weigh Losses. In {\em IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.~7961--7971.

\bibitem{huang2023}
Huang, B., Qi, S., Gao, X., Chen, J., Zeng, M., Zheng, Y., \& Heng, P.A. (2023). Coffbee: A Large-Scale Dataset for Real-time Sensing Area Detection of Laparoscopic Gamma Probes. In {\em Medical Image Computing and Computer-Assisted Intervention (MICCAI)}, pp.~1--10.

\bibitem{laina2017}
Laina, I., Rupprecht, C., Belagiannis, V., Tombari, F., \& Navab, N. (2017). Deeper Depth Prediction with Fully Convolutional Residual Networks. In {\em International Conference on 3D Vision (3DV)}, pp.~239--248.

\bibitem{psm2019}
Chang, J.R., \& Chen, Y.S. (2019). Pyramid Stereo Matching Network. In {\em IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.~5410--5418.

\bibitem{gcnet2017}
Kendall, A., Martirosyan, H., Dasgupta, S., Henry, P., Kennedy, R., Bacchuk, A., \& Bry, A. (2017). End-to-End Learning of Geometry and Context for Deep Stereo. In {\em IEEE International Conference on Computer Vision (ICCV)}, pp.~2402--2411.

\bibitem{timm2021}
Wightman, R. (2021). PyTorch Image Models. \url{https://github.com/rwightman/pytorch-image-models}.

\bibitem{albumentations2020}
Buslaev, A., Iglovikov, V.I., Khvedchenya, E., Parinov, A., Druzhinin, M., \& Kalinin, A.A. (2020). Albumentations: Fast and Flexible Image Augmentation. {\em Information}, {\bf 11}(2), p.~125.

\bibitem{pytorch2023}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., ... \& Chintala, S. (2023). PyTorch: An Imperative Style, High-Performance Deep Learning Library. In {\em Advances in Neural Information Processing Systems}, pp.~8024--8035.

\bibitem{scikit2011}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... \& Duchesnay, E. (2011). Scikit-learn: Machine Learning in Python. {\em Journal of Machine Learning Research}, {\bf 12}, pp.~2825--2830.

\bibitem{numpy2020}
Harris, C.R., Millman, K.J., van der Walt, S.J., Gommers, R., Virtanen, P., Cournapeau, D., ... \& Oliphant, T. (2020). Array Programming with NumPy. {\em Nature}, {\bf 585}, pp.~357--362.

\end{thebibliography}

\end{document}