{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Training Runs - Best Models\n",
    "\n",
    "**Created**: 2025-11-06 17:45\n",
    "\n",
    "## Purpose\n",
    "Full training runs of the best models identified in quick_experiments.ipynb\n",
    "\n",
    "## Top Models Selected:\n",
    "1. **Skip Connections + Focal Loss + Augmentation** (R¬≤_z: 0.5703)\n",
    "2. **Skip Connections + Spatial Attention** (R¬≤_z: 0.5540)\n",
    "3. **Skip Connections + Auxiliary Depth** (R¬≤_z: 0.5362)\n",
    "\n",
    "## Training Configuration:\n",
    "- **Stage A**: 20 epochs (vs 10 in quick experiments)\n",
    "- **Stage B**: 30 epochs (vs 15 in quick experiments)\n",
    "- **Stage C**: 40 epochs (vs 20 in quick experiments)\n",
    "- **Early Stopping**: Enabled with patience=5\n",
    "- **Checkpointing**: Save best model per stage\n",
    "- **Target**: R¬≤_z > 0.60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8576ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import timm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "# Create directories for outputs\n",
    "CHECKPOINT_DIR = Path('checkpoints')\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "print(f'Checkpoint directory: {CHECKPOINT_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3908c39",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Dataset class and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e38407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Training transformations - MATCH quick_experiments EXACTLY\n",
    "train_transformations = [\n",
    "    A.LongestMaxSize(max_size=224),\n",
    "    A.PadIfNeeded(min_height=224, min_width=224),\n",
    "    A.Normalize(),  # Uses default ImageNet normalization\n",
    "    ToTensorV2()\n",
    "]\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    train_transformations,\n",
    "    keypoint_params=A.KeypointParams(format='xy', remove_invisible=False),\n",
    "    additional_targets={'image_right': 'image', 'depth_map': 'mask'}\n",
    ")\n",
    "\n",
    "# Validation transform (no augmentation) - MATCH quick_experiments\n",
    "val_transformations = [\n",
    "    A.LongestMaxSize(max_size=224),\n",
    "    A.PadIfNeeded(min_height=224, min_width=224),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "]\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    val_transformations,\n",
    "    keypoint_params=A.KeypointParams(format='xy', remove_invisible=False),\n",
    "    additional_targets={'image_right': 'image', 'depth_map': 'mask'}\n",
    ")\n",
    "\n",
    "print('‚úÖ Augmentation transforms defined (MATCHING quick_experiments)')\n",
    "print('   - Using LongestMaxSize + PadIfNeeded (preserves aspect ratio)')\n",
    "print('   - Using default Normalize() parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ac79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pca_axis(points_2d):\n",
    "    \"\"\"\n",
    "    Compute the principal axis (origin, direction) from 2D probe points using PCA.\n",
    "    \n",
    "    Args:\n",
    "        points_2d: (N, 2) array of 2D points\n",
    "    \n",
    "    Returns:\n",
    "        origin: (2,) center point\n",
    "        direction: (2,) unit direction vector\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    mean = points_2d.mean(axis=0)\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(points_2d)\n",
    "    direction = pca.components_[0]\n",
    "    return mean, direction\n",
    "\n",
    "\n",
    "def find_valid_depth(depth_map, x_idx, y_idx, max_radius=10):\n",
    "    \"\"\"Find nearest valid depth if center point has invalid depth\"\"\"\n",
    "    for r in range(1, max_radius):\n",
    "        for dy in range(-r, r+1):\n",
    "            for dx in range(-r, r+1):\n",
    "                ny = np.clip(y_idx + dy, 0, depth_map.shape[0] - 1)\n",
    "                nx = np.clip(x_idx + dx, 0, depth_map.shape[1] - 1)\n",
    "                d = depth_map[ny, nx]\n",
    "                if d > 0:\n",
    "                    return d\n",
    "    return 1e-6\n",
    "\n",
    "\n",
    "class StereoIntersectionDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for stereo intersection detection with probe axis - EXACT match to quick_experiments\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None, max_depth=220.0):\n",
    "        self.left_img_paths = sorted(glob.glob(os.path.join(root_dir, \"left\", \"images\", \"*.jpg\")))\n",
    "        self.right_img_paths = sorted(glob.glob(os.path.join(root_dir, \"right\", \"images\", \"*.jpg\")))\n",
    "        self.probe_axis_paths = sorted(glob.glob(os.path.join(root_dir, \"left\", \"probe_axis\", \"*.txt\")))\n",
    "        self.depth_map_paths = sorted(glob.glob(os.path.join(root_dir, \"left\", \"depth_labels\", \"*.npy\")))\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.max_depth = max_depth\n",
    "        self.split = os.path.basename(root_dir.rstrip(\"/\"))\n",
    "\n",
    "        # Load ground truth x,y from CenterPt.txt\n",
    "        gt_xy = []\n",
    "        with open(os.path.join(root_dir, \"left\", \"labels\", \"CenterPt.txt\"), 'r') as f:\n",
    "            for line in f:\n",
    "                _, x_str, y_str = line.strip().split(\",\")\n",
    "                gt_xy.append((float(x_str), float(y_str)))\n",
    "        self.gt_xy = np.array(gt_xy, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.left_img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        left_img = cv2.imread(self.left_img_paths[idx])\n",
    "        right_img = cv2.imread(self.right_img_paths[idx])\n",
    "        left_img = cv2.cvtColor(left_img, cv2.COLOR_BGR2RGB)\n",
    "        right_img = cv2.cvtColor(right_img, cv2.COLOR_BGR2RGB)\n",
    "        depth_map = np.load(self.depth_map_paths[idx])\n",
    "        points = np.loadtxt(self.probe_axis_paths[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(\n",
    "                image=left_img,\n",
    "                image_right=right_img,\n",
    "                keypoints=[self.gt_xy[idx]] + points.tolist(),\n",
    "                depth_map=depth_map\n",
    "            )\n",
    "            left_img = transformed[\"image\"]\n",
    "            right_img = transformed[\"image_right\"]\n",
    "            keypoints = transformed[\"keypoints\"]\n",
    "            depth_map = transformed[\"depth_map\"]\n",
    "        else:\n",
    "            keypoints = [self.gt_xy[idx]] + points.tolist()\n",
    "            # Convert to tensors manually if no transform\n",
    "            left_img = torch.from_numpy(left_img).permute(2, 0, 1).float() / 255.0\n",
    "            right_img = torch.from_numpy(right_img).permute(2, 0, 1).float() / 255.0\n",
    "            depth_map = torch.from_numpy(depth_map).float()\n",
    "\n",
    "        probe_axis_mean, direction = compute_pca_axis(np.array(keypoints[1:]))\n",
    "\n",
    "        _, img_h, img_w = left_img.shape\n",
    "        \n",
    "        # Normalize coordinates by image dimensions\n",
    "        probe_axis_mean = np.array(probe_axis_mean, dtype=np.float32) / np.array([img_w, img_h], dtype=np.float32)\n",
    "        probe_axis = torch.tensor(probe_axis_mean, dtype=torch.float32)\n",
    "        probe_dir = torch.tensor(direction, dtype=torch.float32)\n",
    "        \n",
    "        # Get depth at center point (use y, x order for numpy arrays!)\n",
    "        x, y = keypoints[0]\n",
    "        x_idx = int(np.clip(round(x), 0, depth_map.shape[1] - 1 if depth_map.dim() == 2 else depth_map.shape[2] - 1))\n",
    "        y_idx = int(np.clip(round(y), 0, depth_map.shape[0] if depth_map.dim() == 2 else depth_map.shape[1] - 1))\n",
    "        \n",
    "        if depth_map.dim() == 3:\n",
    "            z = depth_map[0, y_idx, x_idx].item()\n",
    "        else:\n",
    "            z = depth_map[y_idx, x_idx]\n",
    "        \n",
    "        # Handle invalid depth\n",
    "        if z == 0.0:\n",
    "            z = find_valid_depth(depth_map.numpy() if isinstance(depth_map, torch.Tensor) else depth_map, x_idx, y_idx)\n",
    "        if z <= 0:\n",
    "            z = 1e-6\n",
    "        \n",
    "        # Normalize intersection coordinates and depth\n",
    "        intersect_norm = np.array(keypoints[0]) / np.array([img_w, img_h])\n",
    "        target = torch.tensor(intersect_norm.tolist() + [z/self.max_depth], dtype=torch.float32)\n",
    "\n",
    "        # Match quick_experiments return format\n",
    "        return {\n",
    "            \"left_img\": left_img,\n",
    "            \"right_img\": right_img,\n",
    "            \"origin\": probe_axis,\n",
    "            \"direction\": probe_dir,\n",
    "            \"intersection\": target,\n",
    "            \"depth_label\": depth_map / self.max_depth  # Normalize depth map for auxiliary training\n",
    "        }\n",
    "\n",
    "\n",
    "print('‚úÖ Dataset class defined (EXACT match to quick_experiments)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc537f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATA_DIR = Path('data/processed')\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "print('Loading datasets...')\n",
    "\n",
    "# Training dataset with augmentation\n",
    "train_dataset = StereoIntersectionDataset(\n",
    "    root_dir=str(DATA_DIR / 'train'),\n",
    "    transform=train_transform,\n",
    "    max_depth=220.0\n",
    ")\n",
    "\n",
    "# Validation dataset without augmentation\n",
    "val_dataset = StereoIntersectionDataset(\n",
    "    root_dir=str(DATA_DIR / 'val'),\n",
    "    transform=val_transform,\n",
    "    max_depth=220.0\n",
    ")\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f'Train samples: {len(train_dataset)}')\n",
    "print(f'Val samples: {len(val_dataset)}')\n",
    "print(f'Batch size: {BATCH_SIZE}')\n",
    "print(f'Train batches: {len(train_loader)}')\n",
    "print(f'Val batches: {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a605c4",
   "metadata": {},
   "source": [
    "## Model Architectures\n",
    "\n",
    "Base class and top-performing model variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4845773",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StereoTwoStageNet(nn.Module):\n",
    "    \"\"\"Baseline model - Original architecture without modifications\"\"\"\n",
    "    def __init__(self, backbone_name=\"resnet18\", pretrained=True, seed=42):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=pretrained, features_only=True)\n",
    "        self.feature_dims = [f[\"num_chs\"] for f in self.backbone.feature_info]\n",
    "\n",
    "        self.proj = nn.ModuleList([nn.Conv2d(c, 128, 1) for c in self.feature_dims])\n",
    "\n",
    "        fused_channels = 128 * len(self.feature_dims) * 2  \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(fused_channels, 256, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        self.axis_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 4)  # (x0, y0, dx, dy)\n",
    "        )\n",
    "\n",
    "        self.offset_depth_head = nn.Sequential(\n",
    "            nn.Linear(256 + 4, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 2)  # (t, z_raw)\n",
    "        )\n",
    "\n",
    "        self.softplus = nn.Softplus(beta=1.0)\n",
    "\n",
    "    def _fused_vec(self, left_img, right_img):\n",
    "        feats_l = self.backbone(left_img)\n",
    "        feats_r = self.backbone(right_img)\n",
    "        H, W = feats_l[-1].shape[2:]\n",
    "        \n",
    "        fused_scales = []\n",
    "        for fl, fr, proj in zip(feats_l, feats_r, self.proj):\n",
    "            fl = proj(fl)\n",
    "            fr = proj(fr)\n",
    "            fl = F.adaptive_avg_pool2d(fl, (H, W))\n",
    "            fr = F.adaptive_avg_pool2d(fr, (H, W))\n",
    "            fused_scales.append(torch.cat([fl, fr], dim=1))\n",
    "\n",
    "        x = torch.cat(fused_scales, dim=1)\n",
    "        x = self.fusion(x)\n",
    "        v = x.view(x.size(0), -1)\n",
    "        return v, x\n",
    "    \n",
    "    def forward(self, left_img, right_img):\n",
    "        fused_vec, fused_4d = self._fused_vec(left_img, right_img)\n",
    "        axis_params = self.axis_head(fused_4d)\n",
    "        origin = axis_params[:, :2]\n",
    "        direction = F.normalize(axis_params[:, 2:], dim=1)\n",
    "        \n",
    "        conditioned = torch.cat([fused_vec, origin.detach(), direction.detach()], dim=1)\n",
    "        od = self.offset_depth_head(conditioned)\n",
    "        offset_t = od[:, 0:1]\n",
    "        depth_z = self.softplus(od[:, 1:2])\n",
    "        \n",
    "        xy_inter = origin + offset_t * direction\n",
    "        intersection = torch.cat([xy_inter, depth_z], dim=1)\n",
    "        \n",
    "        return {\n",
    "            \"origin\": origin,\n",
    "            \"direction\": direction,\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": intersection\n",
    "        }\n",
    "\n",
    "    def forward_e2e(self, left_img, right_img):\n",
    "        fused_vec, fused_4d = self._fused_vec(left_img, right_img)\n",
    "        axis_params = self.axis_head(fused_4d)\n",
    "        origin = axis_params[:, :2]\n",
    "        direction = F.normalize(axis_params[:, 2:], dim=1)\n",
    "        conditioned = torch.cat([fused_vec, origin, direction], dim=1)  # no detach\n",
    "        od = self.offset_depth_head(conditioned)\n",
    "        offset_t = od[:, 0:1]\n",
    "        depth_z = self.softplus(od[:, 1:2])\n",
    "        xy_inter = origin + offset_t * direction\n",
    "        intersection = torch.cat([xy_inter, depth_z], dim=1)\n",
    "        return {\n",
    "            \"origin\": origin,\n",
    "            \"direction\": direction,\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": intersection\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ StereoTwoStageNet model defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b823fa90",
   "metadata": {},
   "source": [
    "### Model 1: Skip Connections + Focal Loss + Augmentation\n",
    "\n",
    "**Quick Experiment Result**: R¬≤_z = 0.5703 (WINNER)\n",
    "\n",
    "**Architecture**: Dual-path fusion with skip connections for better gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d49fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnectionNet(StereoTwoStageNet):\n",
    "    \"\"\"\n",
    "    Enhanced fusion with residual pathway for better gradient flow.\n",
    "    \n",
    "    Architecture:\n",
    "    - Main path: Concatenated features ‚Üí Conv 1280‚Üí512‚Üí256 ‚Üí Pool ‚Üí (B, 256)\n",
    "    - Skip path: Concatenated features ‚Üí Pool ‚Üí Linear 1280‚Üí256 ‚Üí (B, 256)\n",
    "    - Output: main + skip (residual addition)\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone_name=\"resnet18\", pretrained=True):\n",
    "        super().__init__(backbone_name, pretrained)\n",
    "        \n",
    "        # Enhanced fusion with increased capacity\n",
    "        fused_channels = 128 * len(self.feature_dims) * 2\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(fused_channels, 512, 1),  # Expanded capacity\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        \n",
    "        # Skip connection path - processes raw concatenated features\n",
    "        # This provides alternative gradient path and preserves feature information\n",
    "        self.skip_fusion = nn.Linear(fused_channels, 256)\n",
    "        \n",
    "    def _fused_vec(self, left_img, right_img):\n",
    "        \"\"\"Extract fused features with residual connection\"\"\"\n",
    "        feats_l = self.backbone(left_img)\n",
    "        feats_r = self.backbone(right_img)\n",
    "        H, W = feats_l[-1].shape[2:]\n",
    "        \n",
    "        # Project and fuse features at multiple scales (using inherited self.proj)\n",
    "        fused_scales = []\n",
    "        for fl, fr, proj in zip(feats_l, feats_r, self.proj):\n",
    "            # Project to consistent channels\n",
    "            fl_proj = proj(fl)\n",
    "            fr_proj = proj(fr)\n",
    "            # Resize to common spatial size\n",
    "            fl_proj = F.adaptive_avg_pool2d(fl_proj, (H, W))\n",
    "            fr_proj = F.adaptive_avg_pool2d(fr_proj, (H, W))\n",
    "            # Concatenate left-right stereo features\n",
    "            fused_scales.append(torch.cat([fl_proj, fr_proj], dim=1))\n",
    "\n",
    "        # Concatenate all scales: (B, 1280, H, W) for ResNet18\n",
    "        x = torch.cat(fused_scales, dim=1)\n",
    "        \n",
    "        # Main fusion path: Convolutional transformation\n",
    "        fused_4d = self.fusion(x)\n",
    "        fused_vec = fused_4d.view(fused_4d.size(0), -1)  # (B, 256)\n",
    "        \n",
    "        # Skip connection path: Direct linear transformation\n",
    "        # Pool the raw concatenated features and project to same dimension\n",
    "        skip_pooled = F.adaptive_avg_pool2d(x, 1).view(x.size(0), -1)  # (B, 1280)\n",
    "        skip_vec = self.skip_fusion(skip_pooled)  # (B, 256)\n",
    "        \n",
    "        # Residual addition: Combine both pathways\n",
    "        # This helps gradient flow and provides ensemble-like effect\n",
    "        combined_vec = fused_vec + skip_vec\n",
    "        \n",
    "        return combined_vec, fused_4d\n",
    "\n",
    "print(\"‚úÖ SkipConnectionNet model defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27409cee",
   "metadata": {},
   "source": [
    "### Model 2: Skip Connections + Spatial Attention\n",
    "\n",
    "**Quick Experiment Result**: R¬≤_z = 0.5540 (WINNER)\n",
    "\n",
    "**Architecture**: Combines skip connections with spatial attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453ca89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttentionModule(nn.Module):\n",
    "    \"\"\"Spatial attention to weight important regions for depth prediction\"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        # Channel reduction for attention map\n",
    "        self.attention_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // 4, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels // 4, 1, 1),\n",
    "            nn.Sigmoid()  # Attention weights [0, 1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        attn_map = self.attention_conv(x)  # (B, 1, H, W)\n",
    "        attended = x * attn_map  # Element-wise multiplication\n",
    "        return attended, attn_map\n",
    "\n",
    "\n",
    "class SpatialAttentionDepthNet(StereoTwoStageNet):\n",
    "    \"\"\"\n",
    "    Enhanced model with spatial attention for depth prediction\n",
    "    \n",
    "    Key improvements:\n",
    "    1. Applies learned attention to weight depth-relevant spatial regions\n",
    "    2. Dual-path processing: attended global context + attended spatial features\n",
    "    3. Returns attention maps for visualization and analysis\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone_name=\"resnet18\", pretrained=True):\n",
    "        super().__init__(backbone_name, pretrained)\n",
    "        \n",
    "        # Calculate fused channels\n",
    "        fused_channels = 128 * len(self.feature_dims) * 2\n",
    "        \n",
    "        # Spatial attention module\n",
    "        self.spatial_attention = SpatialAttentionModule(fused_channels)\n",
    "        \n",
    "        # Enhanced fusion with spatial pathway\n",
    "        self.spatial_fusion = nn.Sequential(\n",
    "            nn.Conv2d(fused_channels, 512, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Keep original global fusion for axis prediction\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(fused_channels, 256, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        \n",
    "        # Enhanced depth head with spatial features\n",
    "        # Combines: global features (256) + spatial features (256) + axis (4) = 516\n",
    "        self.offset_depth_head = nn.Sequential(\n",
    "            nn.Linear(256 + 256 + 4, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),  # Regularization for larger head\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 2)  # (offset_t, depth_z_raw)\n",
    "        )\n",
    "        \n",
    "    def _fused_vec(self, left_img, right_img):\n",
    "        \"\"\"Extract fused features with spatial attention\n",
    "        \n",
    "        Returns:\n",
    "            fused_vec: Concatenated global + spatial features (B, 512)\n",
    "            global_4d: Global features for axis head (B, 256, 1, 1)\n",
    "            attn_map: Attention map for visualization (B, 1, H, W)\n",
    "        \"\"\"\n",
    "        feats_l = self.backbone(left_img)\n",
    "        feats_r = self.backbone(right_img)\n",
    "        H, W = feats_l[-1].shape[2:]\n",
    "        \n",
    "        # Multi-scale fusion (same as baseline)\n",
    "        fused_scales = []\n",
    "        for fl, fr, proj in zip(feats_l, feats_r, self.proj):\n",
    "            fl = proj(fl)\n",
    "            fr = proj(fr)\n",
    "            fl = F.adaptive_avg_pool2d(fl, (H, W))\n",
    "            fr = F.adaptive_avg_pool2d(fr, (H, W))\n",
    "            fused_scales.append(torch.cat([fl, fr], dim=1))\n",
    "\n",
    "        x = torch.cat(fused_scales, dim=1)  # (B, 1280, H, W)\n",
    "        \n",
    "        # Apply spatial attention - FIX #2: Keep attention map for visualization\n",
    "        attended_x, attn_map = self.spatial_attention(x)\n",
    "        \n",
    "        # Spatial pathway: preserve spatial features for depth\n",
    "        spatial_features = self.spatial_fusion(attended_x)  # (B, 256, H, W)\n",
    "        spatial_vec = F.adaptive_avg_pool2d(spatial_features, 1).view(spatial_features.size(0), -1)  # (B, 256)\n",
    "        \n",
    "        # Global pathway: for axis prediction - FIX #1: Use attended features\n",
    "        global_4d = self.fusion(attended_x)  # FIXED: was self.fusion(x)\n",
    "        global_vec = global_4d.view(global_4d.size(0), -1)\n",
    "        \n",
    "        fused_vec = torch.cat([global_vec, spatial_vec], dim=1)\n",
    "        return fused_vec, global_4d, attn_map\n",
    "    \n",
    "    def forward(self, left_img, right_img):\n",
    "        fused_vec, fused_4d, attn_map = self._fused_vec(left_img, right_img)\n",
    "        \n",
    "        # Stage 1: Axis prediction (uses attended global features)\n",
    "        axis_params = self.axis_head(fused_4d)\n",
    "        origin = axis_params[:, :2]\n",
    "        direction = F.normalize(axis_params[:, 2:], dim=1)\n",
    "        \n",
    "        # Stage 2: Depth prediction (uses global + spatial + axis)\n",
    "        conditioned = torch.cat([fused_vec, origin.detach(), direction.detach()], dim=1)\n",
    "        \n",
    "        od = self.offset_depth_head(conditioned)\n",
    "        offset_t = od[:, 0:1]\n",
    "        depth_z = self.softplus(od[:, 1:2])\n",
    "        \n",
    "        xy_inter = origin + offset_t * direction\n",
    "        intersection = torch.cat([xy_inter, depth_z], dim=1)\n",
    "        \n",
    "        return {\n",
    "            \"origin\": origin,\n",
    "            \"direction\": direction,\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": intersection,\n",
    "            \"attn_map\": attn_map  # Return attention map for analysis\n",
    "        }\n",
    "\n",
    "    def forward_e2e(self, left_img, right_img):\n",
    "        \"\"\"End-to-end forward pass (no gradient detachment)\"\"\"\n",
    "        fused_vec, fused_4d, attn_map = self._fused_vec(left_img, right_img)\n",
    "        \n",
    "        axis_params = self.axis_head(fused_4d)\n",
    "        origin = axis_params[:, :2]\n",
    "        direction = F.normalize(axis_params[:, 2:], dim=1)\n",
    "        \n",
    "        conditioned = torch.cat([fused_vec, origin, direction], dim=1)\n",
    "        \n",
    "        od = self.offset_depth_head(conditioned)\n",
    "        offset_t = od[:, 0:1]\n",
    "        depth_z = self.softplus(od[:, 1:2])\n",
    "        \n",
    "        xy_inter = origin + offset_t * direction\n",
    "        intersection = torch.cat([xy_inter, depth_z], dim=1)\n",
    "        \n",
    "        return {\n",
    "            \"origin\": origin,\n",
    "            \"direction\": direction,\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": intersection,\n",
    "            \"attn_map\": attn_map  # Return attention map for analysis\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ SpatialAttentionDepthNet model defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a6f257",
   "metadata": {},
   "source": [
    "### Model 3: Skip Connections + Auxiliary Depth\n",
    "\n",
    "**Quick Experiment Result**: R¬≤_z = 0.5362 (DECENT)\n",
    "\n",
    "**Architecture**: Skip connections with auxiliary depth map supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea41946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnectionAuxiliaryDepthNet(StereoTwoStageNet):\n",
    "    \"\"\"Skip connections + Auxiliary depth supervision\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone_name=\"resnet18\", pretrained=True):\n",
    "        super().__init__(backbone_name, pretrained)\n",
    "        \n",
    "        # Skip connection fusion - preserve spatial dimensions\n",
    "        fused_channels = 128 * len(self.feature_dims) * 2\n",
    "        self.fusion_conv = nn.Sequential(\n",
    "            nn.Conv2d(fused_channels, 512, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.skip_fusion = nn.Linear(fused_channels, 256)\n",
    "        \n",
    "        # Auxiliary depth head - starts from 7x7 feature map\n",
    "        self.depth_map_head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 7x7 -> 14x14\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 14x14 -> 28x28\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),    # 28x28 -> 56x56\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 1, kernel_size=3, padding=1),                        # 56x56 -> 56x56\n",
    "            nn.Softplus(beta=1.0)  # Ensure positive depth values\n",
    "        )\n",
    "    \n",
    "    def _fused_vec_with_4d(self, left_img, right_img):\n",
    "        feats_l = self.backbone(left_img)\n",
    "        feats_r = self.backbone(right_img)\n",
    "        H, W = feats_l[-1].shape[2:]\n",
    "        \n",
    "        fused_scales = []\n",
    "        for fl, fr, proj in zip(feats_l, feats_r, self.proj):\n",
    "            fl_proj = proj(fl)\n",
    "            fr_proj = proj(fr)\n",
    "            fl_proj = F.adaptive_avg_pool2d(fl_proj, (H, W))\n",
    "            fr_proj = F.adaptive_avg_pool2d(fr_proj, (H, W))\n",
    "            fused_scales.append(torch.cat([fl_proj, fr_proj], dim=1))\n",
    "        \n",
    "        x = torch.cat(fused_scales, dim=1)  # (B, 1280, H, W) - H,W typically 7x7\n",
    "        fused_4d = self.fusion_conv(x)      # (B, 256, H, W)\n",
    "        \n",
    "        # Skip connection\n",
    "        skip_pooled = F.adaptive_avg_pool2d(x, 1).view(x.size(0), -1)\n",
    "        skip_vec = self.skip_fusion(skip_pooled)\n",
    "        \n",
    "        # Pooled vector for intersection head\n",
    "        fused_vec = F.adaptive_avg_pool2d(fused_4d, 1).view(fused_4d.size(0), -1)\n",
    "        combined_vec = fused_vec + skip_vec  # Residual addition\n",
    "        \n",
    "        return combined_vec, fused_4d\n",
    "    \n",
    "    def forward(self, left_img, right_img):\n",
    "        fused_vec, fused_4d = self._fused_vec_with_4d(left_img, right_img)\n",
    "        \n",
    "        pooled = F.adaptive_avg_pool2d(fused_4d, 1)\n",
    "        axis_params = self.axis_head(pooled)\n",
    "        origin = axis_params[:, :2]\n",
    "        direction = F.normalize(axis_params[:, 2:], dim=1)\n",
    "        \n",
    "        conditioned = torch.cat([fused_vec, origin.detach(), direction.detach()], dim=1)\n",
    "        od = self.offset_depth_head(conditioned)\n",
    "        offset_t = od[:, 0:1]\n",
    "        depth_z = self.softplus(od[:, 1:2])\n",
    "        \n",
    "        xy_inter = origin + offset_t * direction\n",
    "        intersection = torch.cat([xy_inter, depth_z], dim=1)\n",
    "        depth_map = self.depth_map_head(fused_4d)\n",
    "        \n",
    "        return {\n",
    "            \"origin\": origin,\n",
    "            \"direction\": direction,\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": intersection,\n",
    "            \"depth_map\": depth_map\n",
    "        }\n",
    "    \n",
    "    def forward_e2e(self, left_img, right_img):\n",
    "        fused_vec, fused_4d = self._fused_vec_with_4d(left_img, right_img)\n",
    "        \n",
    "        pooled = F.adaptive_avg_pool2d(fused_4d, 1)\n",
    "        axis_params = self.axis_head(pooled)\n",
    "        origin = axis_params[:, :2]\n",
    "        direction = F.normalize(axis_params[:, 2:], dim=1)\n",
    "        \n",
    "        conditioned = torch.cat([fused_vec, origin, direction], dim=1)\n",
    "        od = self.offset_depth_head(conditioned)\n",
    "        offset_t = od[:, 0:1]\n",
    "        depth_z = self.softplus(od[:, 1:2])\n",
    "        \n",
    "        xy_inter = origin + offset_t * direction\n",
    "        intersection = torch.cat([xy_inter, depth_z], dim=1)\n",
    "        depth_map = self.depth_map_head(fused_4d)\n",
    "        \n",
    "        return {\n",
    "            \"origin\": origin,\n",
    "            \"direction\": direction,\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": intersection,\n",
    "            \"depth_map\": depth_map\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ SkipConnectionAuxiliaryDepthNet model defined (FIXED)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb81e229",
   "metadata": {},
   "source": [
    "## Training Infrastructure\n",
    "\n",
    "Utilities for full training with checkpointing and early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a2cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling hard samples\"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=1.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        mse = (pred - target) ** 2\n",
    "        pt = torch.exp(-mse)\n",
    "        focal_weight = self.alpha * (1 - pt) ** self.gamma\n",
    "        loss = focal_weight * mse\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def auxiliary_depth_loss(pred_depth_map, target_depth_map, valid_mask=None):\n",
    "    \"\"\"\n",
    "    Auxiliary depth map loss (log-space L1).\n",
    "    \n",
    "    Args:\n",
    "        pred_depth_map: (B, 1, H, W) predicted depth\n",
    "        target_depth_map: (B, 1, H, W) target depth\n",
    "        valid_mask: (B, 1, H, W) mask for valid depth values\n",
    "    \"\"\"\n",
    "    if valid_mask is None:\n",
    "        valid_mask = target_depth_map > 0.1  # Filter invalid depths\n",
    "    \n",
    "    if valid_mask.sum() == 0:\n",
    "        return torch.tensor(0.0, device=pred_depth_map.device)\n",
    "    \n",
    "    # Log-space L1 for numerical stability\n",
    "    pred_log = torch.log(pred_depth_map[valid_mask] + 1e-6)\n",
    "    target_log = torch.log(target_depth_map[valid_mask] + 1e-6)\n",
    "    loss = F.l1_loss(pred_log, target_log)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_metrics(pred, target):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        pred: dict with keys ['origin', 'direction', 'intersection']\n",
    "        target: dict with keys ['origin', 'direction', 'intersection']\n",
    "    \n",
    "    Returns:\n",
    "        dict with metrics\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # R¬≤ for depth (z coordinate)\n",
    "        z_pred = pred['intersection'][:, 2]\n",
    "        z_true = target['intersection'][:, 2]\n",
    "        z_var = torch.var(z_true)\n",
    "        z_mse = F.mse_loss(z_pred, z_true)\n",
    "        r2_z = 1 - (z_mse / (z_var + 1e-8))\n",
    "        \n",
    "        # 3D Euclidean error (mm)\n",
    "        e3d = torch.norm(pred['intersection'] - target['intersection'], dim=1).mean()\n",
    "        \n",
    "        # 2D pixel error\n",
    "        e2d_origin = torch.norm(pred['origin'] - target['origin'], dim=1).mean()\n",
    "        e2d_inter = torch.norm(pred['intersection'][:, :2] - target['intersection'][:, :2], dim=1).mean()\n",
    "        e2d = (e2d_origin + e2d_inter) / 2\n",
    "        \n",
    "        # Angular error (degrees)\n",
    "        # Normalize both directions first\n",
    "        pred_dir = F.normalize(pred['direction'], dim=1)\n",
    "        target_dir = F.normalize(target['direction'], dim=1)\n",
    "        cos_sim = (pred_dir * target_dir).sum(dim=1)\n",
    "        cos_sim = torch.clamp(cos_sim, -1.0 + 1e-7, 1.0 - 1e-7)\n",
    "        ang_error = torch.acos(cos_sim).mean() * 180 / np.pi\n",
    "    \n",
    "    return {\n",
    "        'r2_z': r2_z.item(),\n",
    "        'e3d': e3d.item(),\n",
    "        'e2d': e2d.item(),\n",
    "        'ang_deg': ang_error.item()\n",
    "    }\n",
    "\n",
    "print('‚úÖ Loss functions and metrics defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointManager:\n",
    "    \"\"\"Manage model checkpoints\"\"\"\n",
    "    def __init__(self, save_dir, model_name):\n",
    "        self.save_dir = Path(save_dir) / model_name\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.best_scores = {}\n",
    "    \n",
    "    def save(self, model, optimizer, epoch, stage, metrics, is_best=False):\n",
    "        \"\"\"Save checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'stage': stage,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        \n",
    "        # Save latest\n",
    "        latest_path = self.save_dir / f'stage_{stage}_latest.pt'\n",
    "        torch.save(checkpoint, latest_path)\n",
    "        \n",
    "        # Save best\n",
    "        if is_best:\n",
    "            best_path = self.save_dir / f'stage_{stage}_best.pt'\n",
    "            torch.save(checkpoint, best_path)\n",
    "            self.best_scores[stage] = metrics['r2_z']\n",
    "            print(f'   üíæ Saved best model for stage {stage} (R¬≤_z: {metrics[\"r2_z\"]:.4f})')\n",
    "    \n",
    "    def load_best(self, model, optimizer, stage):\n",
    "        \"\"\"Load best checkpoint for a stage\"\"\"\n",
    "        best_path = self.save_dir / f'stage_{stage}_best.pt'\n",
    "        if best_path.exists():\n",
    "            checkpoint = torch.load(best_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(f'   üìÇ Loaded best model from stage {stage}')\n",
    "            return checkpoint['metrics']\n",
    "        return None\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.should_stop = False\n",
    "    \n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.should_stop\n",
    "\n",
    "print('‚úÖ Checkpoint and early stopping managers defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f09c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_full_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    model_name,\n",
    "    stage_A_epochs=20,\n",
    "    stage_B_epochs=30,\n",
    "    stage_C_epochs=40,\n",
    "    use_focal=False,\n",
    "    use_auxiliary_depth=False,\n",
    "    w_depth_map=None,\n",
    "    early_stopping_patience=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Full three-stage training with checkpointing and early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        model_name: Name for saving checkpoints\n",
    "        stage_A_epochs: Epochs for Stage A (axis pretraining)\n",
    "        stage_B_epochs: Epochs for Stage B (intersection training)\n",
    "        stage_C_epochs: Epochs for Stage C (end-to-end fine-tuning)\n",
    "        use_focal: Whether to use Focal Loss\n",
    "        use_auxiliary_depth: Whether model has auxiliary depth head\n",
    "        w_depth_map: Dict with depth map weights per stage (for auxiliary)\n",
    "        early_stopping_patience: Patience for early stopping\n",
    "    \n",
    "    Returns:\n",
    "        dict: Final metrics and training history\n",
    "    \"\"\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    checkpoint_mgr = CheckpointManager(CHECKPOINT_DIR, model_name)\n",
    "    \n",
    "    # Loss functions\n",
    "    if use_focal:\n",
    "        depth_loss_fn = FocalLoss(alpha=0.25, gamma=1.5)\n",
    "    else:\n",
    "        depth_loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # Default depth map weights\n",
    "    if w_depth_map is None:\n",
    "        w_depth_map = {'A': 0.0, 'B': 0.5, 'C': 1.0}\n",
    "    \n",
    "    history = {'A': [], 'B': [], 'C': []}\n",
    "    \n",
    "    # ==================== STAGE A: Axis Pretraining ====================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"STAGE A: Axis Pretraining ({stage_A_epochs} epochs)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Training: axis_head only\")\n",
    "    print(\"Frozen: offset_depth_head\")\n",
    "    \n",
    "    # Freeze intersection head\n",
    "    for param in model.offset_depth_head.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=stage_A_epochs)\n",
    "    early_stop = EarlyStopping(patience=early_stopping_patience)\n",
    "    \n",
    "    best_val_r2z = -float('inf')\n",
    "    \n",
    "    for epoch in range(stage_A_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{stage_A_epochs}\"):\n",
    "            left_img = batch['left_img'].to(device)\n",
    "            right_img = batch['right_img'].to(device)\n",
    "            origin = batch['origin'].to(device)\n",
    "            direction = batch['direction'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(left_img, right_img)\n",
    "            \n",
    "            # Stage A loss: axis only\n",
    "            loss_origin = F.mse_loss(pred['origin'], origin)\n",
    "            loss_dir = F.mse_loss(pred['direction'], direction)\n",
    "            loss = loss_origin + loss_dir\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_metrics = {'r2_z': 0, 'e3d': 0, 'e2d': 0, 'ang_deg': 0}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                left_img = batch['left_img'].to(device)\n",
    "                right_img = batch['right_img'].to(device)\n",
    "                \n",
    "                target = {\n",
    "                    'origin': batch['origin'].to(device),\n",
    "                    'direction': batch['direction'].to(device),\n",
    "                    'intersection': batch['intersection'].to(device)\n",
    "                }\n",
    "                \n",
    "                pred = model(left_img, right_img)\n",
    "                metrics = compute_metrics(pred, target)\n",
    "                \n",
    "                for k in val_metrics:\n",
    "                    val_metrics[k] += metrics[k]\n",
    "        \n",
    "        for k in val_metrics:\n",
    "            val_metrics[k] /= len(val_loader)\n",
    "        \n",
    "        history['A'].append({'epoch': epoch+1, 'train_loss': train_loss, **val_metrics})\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{stage_A_epochs} | Loss: {train_loss:.4f} | R¬≤_z: {val_metrics['r2_z']:.4f} | Ang: {val_metrics['ang_deg']:.2f}¬∞\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        is_best = val_metrics['r2_z'] > best_val_r2z\n",
    "        if is_best:\n",
    "            best_val_r2z = val_metrics['r2_z']\n",
    "        checkpoint_mgr.save(model, optimizer, epoch, 'A', val_metrics, is_best)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stop(val_metrics['r2_z']):\n",
    "            print(f\"   ‚è∏Ô∏è Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best Stage A model\n",
    "    checkpoint_mgr.load_best(model, optimizer, 'A')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Stage A complete. Best R¬≤_z: {best_val_r2z:.4f}\")\n",
    "    \n",
    "    # ==================== STAGE B: Intersection Training ====================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"STAGE B: Intersection Training ({stage_B_epochs} epochs)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Training: offset_depth_head only\")\n",
    "    print(\"Frozen: backbone, axis_head\")\n",
    "    \n",
    "    # Freeze backbone and axis head\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.axis_head.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Unfreeze intersection head\n",
    "    for param in model.offset_depth_head.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=stage_B_epochs)\n",
    "    early_stop = EarlyStopping(patience=early_stopping_patience)\n",
    "    \n",
    "    best_val_r2z = -float('inf')\n",
    "    \n",
    "    for epoch in range(stage_B_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{stage_B_epochs}\"):\n",
    "            left_img = batch['left_img'].to(device)\n",
    "            right_img = batch['right_img'].to(device)\n",
    "            intersection = batch['intersection'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(left_img, right_img)\n",
    "            \n",
    "            # Stage B loss: intersection with weighted depth\n",
    "            loss_xy = F.mse_loss(pred['intersection'][:, :2], intersection[:, :2])\n",
    "            loss_z = depth_loss_fn(pred['intersection'][:, 2:3], intersection[:, 2:3])\n",
    "            loss = loss_xy + 5.0 * loss_z  # Match quick_experiments default\n",
    "            \n",
    "            # Auxiliary depth loss\n",
    "            if use_auxiliary_depth and 'depth_map' in pred and 'depth_label' in batch:\n",
    "                depth_label = batch['depth_label'].to(device)\n",
    "                loss_depth_map = auxiliary_depth_loss(pred['depth_map'], depth_label)\n",
    "                loss = loss + w_depth_map['B'] * loss_depth_map\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation (same as Stage A)\n",
    "        model.eval()\n",
    "        val_metrics = {'r2_z': 0, 'e3d': 0, 'e2d': 0, 'ang_deg': 0}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                left_img = batch['left_img'].to(device)\n",
    "                right_img = batch['right_img'].to(device)\n",
    "                \n",
    "                target = {\n",
    "                    'origin': batch['origin'].to(device),\n",
    "                    'direction': batch['direction'].to(device),\n",
    "                    'intersection': batch['intersection'].to(device)\n",
    "                }\n",
    "                \n",
    "                pred = model(left_img, right_img)\n",
    "                metrics = compute_metrics(pred, target)\n",
    "                \n",
    "                for k in val_metrics:\n",
    "                    val_metrics[k] += metrics[k]\n",
    "        \n",
    "        for k in val_metrics:\n",
    "            val_metrics[k] /= len(val_loader)\n",
    "        \n",
    "        history['B'].append({'epoch': epoch+1, 'train_loss': train_loss, **val_metrics})\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{stage_B_epochs} | Loss: {train_loss:.4f} | R¬≤_z: {val_metrics['r2_z']:.4f} | Ang: {val_metrics['ang_deg']:.2f}¬∞\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        is_best = val_metrics['r2_z'] > best_val_r2z\n",
    "        if is_best:\n",
    "            best_val_r2z = val_metrics['r2_z']\n",
    "        checkpoint_mgr.save(model, optimizer, epoch, 'B', val_metrics, is_best)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stop(val_metrics['r2_z']):\n",
    "            print(f\"   ‚è∏Ô∏è Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best Stage B model\n",
    "    checkpoint_mgr.load_best(model, optimizer, 'B')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Stage B complete. Best R¬≤_z: {best_val_r2z:.4f}\")\n",
    "    \n",
    "    # ==================== STAGE C: End-to-End Fine-tuning ====================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"STAGE C: End-to-End Fine-tuning ({stage_C_epochs} epochs)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Training: All parameters\")\n",
    "    \n",
    "    # Unfreeze all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=stage_C_epochs)\n",
    "    early_stop = EarlyStopping(patience=early_stopping_patience)\n",
    "    \n",
    "    best_val_r2z = -float('inf')\n",
    "    \n",
    "    for epoch in range(stage_C_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{stage_C_epochs}\"):\n",
    "            left_img = batch['left_img'].to(device)\n",
    "            right_img = batch['right_img'].to(device)\n",
    "            origin = batch['origin'].to(device)\n",
    "            direction = batch['direction'].to(device)\n",
    "            intersection = batch['intersection'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model.forward_e2e(left_img, right_img)  # End-to-end forward\n",
    "            \n",
    "            # Full loss\n",
    "            loss_origin = F.mse_loss(pred['origin'], origin)\n",
    "            loss_dir = F.mse_loss(pred['direction'], direction)\n",
    "            loss_xy = F.mse_loss(pred['intersection'][:, :2], intersection[:, :2])\n",
    "            loss_z = depth_loss_fn(pred['intersection'][:, 2:3], intersection[:, 2:3])\n",
    "            \n",
    "            loss = loss_origin + loss_dir + loss_xy + 5.0 * loss_z  # Match quick_experiments default\n",
    "            \n",
    "            # Auxiliary depth loss\n",
    "            if use_auxiliary_depth and 'depth_map' in pred and 'depth_label' in batch:\n",
    "                depth_label = batch['depth_label'].to(device)\n",
    "                loss_depth_map = auxiliary_depth_loss(pred['depth_map'], depth_label)\n",
    "                loss = loss + w_depth_map['C'] * loss_depth_map\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_metrics = {'r2_z': 0, 'e3d': 0, 'e2d': 0, 'ang_deg': 0}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                left_img = batch['left_img'].to(device)\n",
    "                right_img = batch['right_img'].to(device)\n",
    "                \n",
    "                target = {\n",
    "                    'origin': batch['origin'].to(device),\n",
    "                    'direction': batch['direction'].to(device),\n",
    "                    'intersection': batch['intersection'].to(device)\n",
    "                }\n",
    "                \n",
    "                pred = model.forward_e2e(left_img, right_img)\n",
    "                metrics = compute_metrics(pred, target)\n",
    "                \n",
    "                for k in val_metrics:\n",
    "                    val_metrics[k] += metrics[k]\n",
    "        \n",
    "        for k in val_metrics:\n",
    "            val_metrics[k] /= len(val_loader)\n",
    "        \n",
    "        history['C'].append({'epoch': epoch+1, 'train_loss': train_loss, **val_metrics})\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{stage_C_epochs} | Loss: {train_loss:.4f} | R¬≤_z: {val_metrics['r2_z']:.4f} | E3D: {val_metrics['e3d']:.2f}mm\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        is_best = val_metrics['r2_z'] > best_val_r2z\n",
    "        if is_best:\n",
    "            best_val_r2z = val_metrics['r2_z']\n",
    "        checkpoint_mgr.save(model, optimizer, epoch, 'C', val_metrics, is_best)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stop(val_metrics['r2_z']):\n",
    "            print(f\"   ‚è∏Ô∏è Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best Stage C model\n",
    "    final_metrics = checkpoint_mgr.load_best(model, optimizer, 'C')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Stage C complete. Best R¬≤_z: {best_val_r2z:.4f}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Final Results:\")\n",
    "    print(f\"  R¬≤_z: {final_metrics['r2_z']:.4f}\")\n",
    "    print(f\"  3D Error: {final_metrics['e3d']:.2f} mm\")\n",
    "    print(f\"  2D Error: {final_metrics['e2d']:.2f} px\")\n",
    "    print(f\"  Angular Error: {final_metrics['ang_deg']:.2f}¬∞\")\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'final_metrics': final_metrics,\n",
    "        'history': history\n",
    "    }\n",
    "\n",
    "print('‚úÖ Full training function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253cdffb",
   "metadata": {},
   "source": [
    "### Resume/Extend Training\n",
    "\n",
    "Functions to continue training if you need more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff35111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_training(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    model_name,\n",
    "    stage,\n",
    "    additional_epochs=10,\n",
    "    use_focal=False,\n",
    "    use_auxiliary_depth=False,\n",
    "    w_depth_map=None,\n",
    "    early_stopping_patience=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Continue training from a saved checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        model: The model (must match saved checkpoint)\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        model_name: Name used when saving checkpoints\n",
    "        stage: Which stage to continue ('A', 'B', or 'C')\n",
    "        additional_epochs: How many more epochs to train\n",
    "        use_focal: Whether to use Focal Loss\n",
    "        use_auxiliary_depth: Whether model has auxiliary depth head\n",
    "        w_depth_map: Dict with depth map weights per stage\n",
    "        early_stopping_patience: Patience for early stopping\n",
    "    \n",
    "    Returns:\n",
    "        dict: Updated metrics and training history\n",
    "    \"\"\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    checkpoint_mgr = CheckpointManager(CHECKPOINT_DIR, model_name)\n",
    "    \n",
    "    # Load best checkpoint from the stage\n",
    "    checkpoint_path = checkpoint_mgr.save_dir / f'stage_{stage}_best.pt'\n",
    "    \n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"‚ùå No checkpoint found for stage {stage}\")\n",
    "        print(f\"   Looking for: {checkpoint_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    previous_best = checkpoint['metrics']['r2_z']\n",
    "    \n",
    "    print(f\"\\nüìÇ Loaded checkpoint from stage {stage}\")\n",
    "    print(f\"   Starting from epoch: {start_epoch}\")\n",
    "    print(f\"   Previous best R¬≤_z: {previous_best:.4f}\")\n",
    "    print(f\"   Training for {additional_epochs} more epochs...\\n\")\n",
    "    \n",
    "    # Setup loss functions\n",
    "    if use_focal:\n",
    "        depth_loss_fn = FocalLoss(alpha=0.25, gamma=1.5)\n",
    "    else:\n",
    "        depth_loss_fn = nn.MSELoss()\n",
    "    \n",
    "    if w_depth_map is None:\n",
    "        w_depth_map = {'A': 0.0, 'B': 0.5, 'C': 1.0}\n",
    "    \n",
    "    # Configure training based on stage\n",
    "    if stage == 'A':\n",
    "        # Freeze intersection head\n",
    "        for param in model.offset_depth_head.parameters():\n",
    "            param.requires_grad = False\n",
    "        lr = 1e-4\n",
    "        print(\"Stage A: Training axis_head only\")\n",
    "        \n",
    "    elif stage == 'B':\n",
    "        # Freeze backbone and axis head\n",
    "        for param in model.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.axis_head.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.offset_depth_head.parameters():\n",
    "            param.requires_grad = True\n",
    "        lr = 5e-5\n",
    "        print(\"Stage B: Training offset_depth_head only\")\n",
    "        \n",
    "    elif stage == 'C':\n",
    "        # Unfreeze all\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        lr = 1e-5\n",
    "        print(\"Stage C: Training all parameters (end-to-end)\")\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid stage: {stage}. Must be 'A', 'B', or 'C'\")\n",
    "    \n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    \n",
    "    # Load optimizer state if available\n",
    "    if 'optimizer_state_dict' in checkpoint:\n",
    "        try:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(\"‚úÖ Loaded optimizer state\")\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è  Could not load optimizer state, using fresh optimizer\")\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=additional_epochs)\n",
    "    early_stop = EarlyStopping(patience=early_stopping_patience)\n",
    "    \n",
    "    best_val_r2z = previous_best\n",
    "    history = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"CONTINUING STAGE {stage} TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for epoch in range(additional_epochs):\n",
    "        actual_epoch = start_epoch + epoch + 1\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {actual_epoch} ({epoch+1}/{additional_epochs})\"):\n",
    "            left_img = batch['left_img'].to(device)\n",
    "            right_img = batch['right_img'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass based on stage\n",
    "            if stage == 'C':\n",
    "                pred = model.forward_e2e(left_img, right_img)\n",
    "            else:\n",
    "                pred = model(left_img, right_img)\n",
    "            \n",
    "            # Compute loss based on stage\n",
    "            if stage == 'A':\n",
    "                origin = batch['origin'].to(device)\n",
    "                direction = batch['direction'].to(device)\n",
    "                loss_origin = F.mse_loss(pred['origin'], origin)\n",
    "                loss_dir = F.mse_loss(pred['direction'], direction)\n",
    "                loss = loss_origin + loss_dir\n",
    "                \n",
    "            elif stage == 'B':\n",
    "                intersection = batch['intersection'].to(device)\n",
    "                loss_xy = F.mse_loss(pred['intersection'][:, :2], intersection[:, :2])\n",
    "                loss_z = depth_loss_fn(pred['intersection'][:, 2:3], intersection[:, 2:3])\n",
    "                loss = loss_xy + 5.0 * loss_z  # Match quick_experiments\n",
    "                \n",
    "                if use_auxiliary_depth and 'depth_map' in pred and 'depth_label' in batch:\n",
    "                    depth_label = batch['depth_label'].to(device)\n",
    "                    loss_depth_map = auxiliary_depth_loss(pred['depth_map'], depth_label)\n",
    "                    loss = loss + w_depth_map['B'] * loss_depth_map\n",
    "                    \n",
    "            else:  # stage == 'C'\n",
    "                origin = batch['origin'].to(device)\n",
    "                direction = batch['direction'].to(device)\n",
    "                intersection = batch['intersection'].to(device)\n",
    "                \n",
    "                loss_origin = F.mse_loss(pred['origin'], origin)\n",
    "                loss_dir = F.mse_loss(pred['direction'], direction)\n",
    "                loss_xy = F.mse_loss(pred['intersection'][:, :2], intersection[:, :2])\n",
    "                loss_z = depth_loss_fn(pred['intersection'][:, 2:3], intersection[:, 2:3])\n",
    "                loss = loss_origin + loss_dir + loss_xy + 5.0 * loss_z  # Match quick_experiments\n",
    "                \n",
    "                if use_auxiliary_depth and 'depth_map' in pred and 'depth_label' in batch:\n",
    "                    depth_label = batch['depth_label'].to(device)\n",
    "                    loss_depth_map = auxiliary_depth_loss(pred['depth_map'], depth_label)\n",
    "                    loss = loss + w_depth_map['C'] * loss_depth_map\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_metrics = {'r2_z': 0, 'e3d': 0, 'e2d': 0, 'ang_deg': 0}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                left_img = batch['left_img'].to(device)\n",
    "                right_img = batch['right_img'].to(device)\n",
    "                \n",
    "                target = {\n",
    "                    'origin': batch['origin'].to(device),\n",
    "                    'direction': batch['direction'].to(device),\n",
    "                    'intersection': batch['intersection'].to(device)\n",
    "                }\n",
    "                \n",
    "                if stage == 'C':\n",
    "                    pred = model.forward_e2e(left_img, right_img)\n",
    "                else:\n",
    "                    pred = model(left_img, right_img)\n",
    "                    \n",
    "                metrics = compute_metrics(pred, target)\n",
    "                \n",
    "                for k in val_metrics:\n",
    "                    val_metrics[k] += metrics[k]\n",
    "        \n",
    "        for k in val_metrics:\n",
    "            val_metrics[k] /= len(val_loader)\n",
    "        \n",
    "        history.append({'epoch': actual_epoch, 'train_loss': train_loss, **val_metrics})\n",
    "        \n",
    "        print(f\"Epoch {actual_epoch} | Loss: {train_loss:.4f} | R¬≤_z: {val_metrics['r2_z']:.4f} | E3D: {val_metrics['e3d']:.2f}mm\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        is_best = val_metrics['r2_z'] > best_val_r2z\n",
    "        if is_best:\n",
    "            best_val_r2z = val_metrics['r2_z']\n",
    "            print(f\"   üéØ New best R¬≤_z: {best_val_r2z:.4f} (improvement: +{best_val_r2z - previous_best:.4f})\")\n",
    "        checkpoint_mgr.save(model, optimizer, actual_epoch, stage, val_metrics, is_best)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stop(val_metrics['r2_z']):\n",
    "            print(f\"   ‚è∏Ô∏è Early stopping triggered at epoch {actual_epoch}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model from extended training\n",
    "    final_metrics = checkpoint_mgr.load_best(model, optimizer, stage)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"EXTENDED TRAINING COMPLETE FOR STAGE {stage}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Previous best R¬≤_z: {previous_best:.4f}\")\n",
    "    print(f\"New best R¬≤_z: {best_val_r2z:.4f}\")\n",
    "    print(f\"Improvement: {best_val_r2z - previous_best:+.4f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'stage': stage,\n",
    "        'previous_best': previous_best,\n",
    "        'new_best': best_val_r2z,\n",
    "        'improvement': best_val_r2z - previous_best,\n",
    "        'final_metrics': final_metrics,\n",
    "        'history': history\n",
    "    }\n",
    "\n",
    "print('‚úÖ Continue training function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc706f",
   "metadata": {},
   "source": [
    "## Run Full Training\n",
    "\n",
    "Execute full training runs for each best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7892bb9d",
   "metadata": {},
   "source": [
    "### Train Model 1: Skip Connections + Focal Loss + Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e68ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model1 = SkipConnectionNet(backbone_name=\"resnet18\", pretrained=True)\n",
    "\n",
    "# Run full training\n",
    "result1 = train_full_model(\n",
    "    model=model1,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name=\"skip_connections_focal_augmentation_FULL\",\n",
    "    stage_A_epochs=20,\n",
    "    stage_B_epochs=30,\n",
    "    stage_C_epochs=40,\n",
    "    use_focal=True,\n",
    "    use_auxiliary_depth=False,\n",
    "    early_stopping_patience=5\n",
    ")\n",
    "\n",
    "print(\"\\nüéØ Model 1 Training Complete!\")\n",
    "print(f\"Quick Experiment R¬≤_z: 0.5703\")\n",
    "print(f\"Full Training R¬≤_z: {result1['final_metrics']['r2_z']:.4f}\")\n",
    "print(f\"Improvement: {result1['final_metrics']['r2_z'] - 0.5703:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model 2: Skip Connections + Spatial Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model2 = SkipConnectionSpatialAttentionNet(backbone_name=\"resnet18\", pretrained=True)\n",
    "\n",
    "# Run full training\n",
    "result2 = train_full_model(\n",
    "    model=model2,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name=\"skip_connections_spatial_attention_FULL\",\n",
    "    stage_A_epochs=20,\n",
    "    stage_B_epochs=30,\n",
    "    stage_C_epochs=40,\n",
    "    use_focal=False,\n",
    "    use_auxiliary_depth=False,\n",
    "    early_stopping_patience=5\n",
    ")\n",
    "\n",
    "print(\"\\nüéØ Model 2 Training Complete!\")\n",
    "print(f\"Quick Experiment R¬≤_z: 0.5540\")\n",
    "print(f\"Full Training R¬≤_z: {result2['final_metrics']['r2_z']:.4f}\")\n",
    "print(f\"Improvement: {result2['final_metrics']['r2_z'] - 0.5540:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model 3: Skip Connections + Auxiliary Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model3 = SkipConnectionAuxiliaryDepthNet(backbone_name=\"resnet18\", pretrained=True)\n",
    "\n",
    "# Run full training\n",
    "result3 = train_full_model(\n",
    "    model=model3,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name=\"skip_connections_auxiliary_depth_FULL\",\n",
    "    stage_A_epochs=20,\n",
    "    stage_B_epochs=30,\n",
    "    stage_C_epochs=40,\n",
    "    use_focal=False,\n",
    "    use_auxiliary_depth=True,\n",
    "    w_depth_map={'A': 0.0, 'B': 0.5, 'C': 1.0},\n",
    "    early_stopping_patience=5\n",
    ")\n",
    "\n",
    "print(\"\\nüéØ Model 3 Training Complete!\")\n",
    "print(f\"Quick Experiment R¬≤_z: 0.5362\")\n",
    "print(f\"Full Training R¬≤_z: {result3['final_metrics']['r2_z']:.4f}\")\n",
    "print(f\"Improvement: {result3['final_metrics']['r2_z'] - 0.5362:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Comparison and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "all_results = [\n",
    "    {'name': 'Model 1: Skip+Focal+Aug', 'quick': 0.5703, 'full': result1['final_metrics']['r2_z']},\n",
    "    {'name': 'Model 2: Skip+Spatial', 'quick': 0.5540, 'full': result2['final_metrics']['r2_z']},\n",
    "    {'name': 'Model 3: Skip+AuxDepth', 'quick': 0.5362, 'full': result3['final_metrics']['r2_z']}\n",
    "]\n",
    "\n",
    "# Print comparison table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FULL TRAINING RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<30} {'Quick Exp':<12} {'Full Train':<12} {'Improvement':<12} {'Status'}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for r in all_results:\n",
    "    improvement = r['full'] - r['quick']\n",
    "    status = 'üü¢ WINNER' if r['full'] > 0.60 else 'üü° DECENT' if r['full'] > 0.55 else 'üî¥ POOR'\n",
    "    print(f\"{r['name']:<30} {r['quick']:.4f}       {r['full']:.4f}       {improvement:+.4f}      {status}\")\n",
    "\n",
    "# Find best model\n",
    "best = max(all_results, key=lambda x: x['full'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üèÜ BEST MODEL: {best['name']}\")\n",
    "print(f\"   R¬≤_z: {best['full']:.4f}\")\n",
    "print(f\"   Improvement over quick experiment: {best['full'] - best['quick']:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "fig.suptitle('Full Training Results: All Models', fontsize=16, fontweight='bold')\n",
    "\n",
    "results = [result1, result2, result3]\n",
    "titles = ['Model 1: Skip+Focal+Aug', 'Model 2: Skip+Spatial', 'Model 3: Skip+AuxDepth']\n",
    "\n",
    "for row, (result, title) in enumerate(zip(results, titles)):\n",
    "    # Combine all stages\n",
    "    all_epochs = []\n",
    "    all_r2z = []\n",
    "    all_e3d = []\n",
    "    all_loss = []\n",
    "    \n",
    "    offset = 0\n",
    "    for stage in ['A', 'B', 'C']:\n",
    "        for entry in result['history'][stage]:\n",
    "            all_epochs.append(entry['epoch'] + offset)\n",
    "            all_r2z.append(entry['r2_z'])\n",
    "            all_e3d.append(entry['e3d'])\n",
    "            all_loss.append(entry['train_loss'])\n",
    "        offset += len(result['history'][stage])\n",
    "    \n",
    "    # Plot R¬≤_z\n",
    "    axes[row, 0].plot(all_epochs, all_r2z, 'b-', linewidth=2)\n",
    "    axes[row, 0].set_xlabel('Epoch')\n",
    "    axes[row, 0].set_ylabel('R¬≤_z')\n",
    "    axes[row, 0].set_title(f'{title} - R¬≤_z')\n",
    "    axes[row, 0].grid(True, alpha=0.3)\n",
    "    axes[row, 0].axhline(y=0.60, color='g', linestyle='--', alpha=0.5, label='Target (0.60)')\n",
    "    axes[row, 0].legend()\n",
    "    \n",
    "    # Plot 3D Error\n",
    "    axes[row, 1].plot(all_epochs, all_e3d, 'r-', linewidth=2)\n",
    "    axes[row, 1].set_xlabel('Epoch')\n",
    "    axes[row, 1].set_ylabel('3D Error (mm)')\n",
    "    axes[row, 1].set_title(f'{title} - 3D Error')\n",
    "    axes[row, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot Loss\n",
    "    axes[row, 2].plot(all_epochs, all_loss, 'purple', linewidth=2)\n",
    "    axes[row, 2].set_xlabel('Epoch')\n",
    "    axes[row, 2].set_ylabel('Training Loss')\n",
    "    axes[row, 2].set_title(f'{title} - Loss')\n",
    "    axes[row, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('full_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "print('‚úÖ Saved training curves to full_training_curves.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "results_file = 'full_training_results.csv'\n",
    "\n",
    "# Check if file exists\n",
    "file_exists = Path(results_file).exists()\n",
    "\n",
    "with open(results_file, 'a', newline='') as f:\n",
    "    fieldnames = ['model_name', 'r2_z', 'e3d_mm', 'e2d_px', 'ang_deg', 'total_epochs', 'timestamp']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    \n",
    "    if not file_exists:\n",
    "        writer.writeheader()\n",
    "    \n",
    "    # Write all results\n",
    "    for result in [result1, result2, result3]:\n",
    "        total_epochs = sum(len(result['history'][stage]) for stage in ['A', 'B', 'C'])\n",
    "        writer.writerow({\n",
    "            'model_name': result['model_name'],\n",
    "            'r2_z': f\"{result['final_metrics']['r2_z']:.6f}\",\n",
    "            'e3d_mm': f\"{result['final_metrics']['e3d']:.6f}\",\n",
    "            'e2d_px': f\"{result['final_metrics']['e2d']:.6f}\",\n",
    "            'ang_deg': f\"{result['final_metrics']['ang_deg']:.6f}\",\n",
    "            'total_epochs': total_epochs,\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        })\n",
    "\n",
    "print(f'‚úÖ Results saved to {results_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend Training (Add More Epochs)\n",
    "\n",
    "Use these cells if you need to add more epochs to any stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Add 20 more epochs to Stage C\n",
    "\n",
    "Most common use case - extend the final fine-tuning stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Continue training Model 1 Stage C for 20 more epochs\n",
    "# (Only run this if you've already trained model1)\n",
    "\n",
    "extended_result = continue_training(\n",
    "    model=model1,  # The model you want to continue training\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name=\"skip_connections_focal_augmentation_FULL\",  # Must match original name\n",
    "    stage='C',  # Which stage to continue\n",
    "    additional_epochs=20,  # How many more epochs\n",
    "    use_focal=True,  # Same settings as original training\n",
    "    use_auxiliary_depth=False,\n",
    "    early_stopping_patience=5\n",
    ")\n",
    "\n",
    "if extended_result:\n",
    "    print(f\"\\n‚úÖ Extended training complete!\")\n",
    "    print(f\"   Improvement: {extended_result['improvement']:+.4f}\")\n",
    "    print(f\"   New R¬≤_z: {extended_result['new_best']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Add epochs to any stage\n",
    "\n",
    "You can extend Stage A, B, or C independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extend Stage B (intersection training) for Model 2\n",
    "\n",
    "# extended_result_B = continue_training(\n",
    "#     model=model2,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     model_name=\"skip_connections_spatial_attention_FULL\",\n",
    "#     stage='B',  # Continue Stage B\n",
    "#     additional_epochs=15,\n",
    "#     use_focal=False,\n",
    "#     use_auxiliary_depth=False,\n",
    "#     early_stopping_patience=5\n",
    "# )\n",
    "\n",
    "# For Model 3 (with auxiliary depth):\n",
    "# extended_result_C = continue_training(\n",
    "#     model=model3,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     model_name=\"skip_connections_auxiliary_depth_FULL\",\n",
    "#     stage='C',\n",
    "#     additional_epochs=20,\n",
    "#     use_focal=False,\n",
    "#     use_auxiliary_depth=True,\n",
    "#     w_depth_map={'A': 0.0, 'B': 0.5, 'C': 1.0},\n",
    "#     early_stopping_patience=5\n",
    "# )\n",
    "\n",
    "print(\"Uncomment the code above to extend training for any model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Train new model with custom epochs\n",
    "\n",
    "Start fresh with different epoch counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Train a model with more epochs from the start\n",
    "\n",
    "# model_custom = SkipConnectionNet(backbone_name=\"resnet18\", pretrained=True)\n",
    "\n",
    "# result_custom = train_full_model(\n",
    "#     model=model_custom,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     model_name=\"skip_connections_EXTENDED\",\n",
    "#     stage_A_epochs=30,  # More epochs!\n",
    "#     stage_B_epochs=50,\n",
    "#     stage_C_epochs=60,\n",
    "#     use_focal=True,\n",
    "#     use_auxiliary_depth=False,\n",
    "#     early_stopping_patience=7  # Higher patience for longer training\n",
    "# )\n",
    "\n",
    "print(\"Uncomment the code above to train with custom epoch counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for Adding More Epochs\n",
    "\n",
    "**When to add more epochs:**\n",
    "- ‚úÖ Training curves still improving (not plateaued)\n",
    "- ‚úÖ Validation R¬≤_z increasing steadily\n",
    "- ‚úÖ No signs of overfitting (train/val gap small)\n",
    "\n",
    "**When NOT to add more epochs:**\n",
    "- ‚ùå Validation R¬≤_z plateaued for 5+ epochs (early stopping will handle this)\n",
    "- ‚ùå Large train/val gap (overfitting)\n",
    "- ‚ùå Training loss still decreasing but val loss increasing\n",
    "\n",
    "**How many epochs to add:**\n",
    "- **Small improvement needed (+0.01)**: Add 10-15 epochs\n",
    "- **Moderate improvement (+0.02-0.03)**: Add 20-30 epochs\n",
    "- **Still learning**: Add 40-50 epochs\n",
    "\n",
    "**Important Notes:**\n",
    "1. The `continue_training()` function automatically:\n",
    "   - Loads the best checkpoint from the specified stage\n",
    "   - Uses the same training configuration\n",
    "   - Saves new checkpoints if performance improves\n",
    "\n",
    "2. You can continue training multiple times:\n",
    "   ```python\n",
    "   # First extension: +20 epochs\n",
    "   continue_training(model, ..., additional_epochs=20)\n",
    "   \n",
    "   # Second extension: +10 more epochs\n",
    "   continue_training(model, ..., additional_epochs=10)\n",
    "   ```\n",
    "\n",
    "3. Model checkpoints are preserved:\n",
    "   - Old best: Kept as backup\n",
    "   - New best: Saved if performance improves\n",
    "   - You can always load previous checkpoints manually\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on your full training results:\n",
    "\n",
    "### If best model achieves R¬≤_z > 0.60:\n",
    "1. **Deploy the model** - You have a production-ready solution\n",
    "2. **Test on held-out test set** - Validate generalization\n",
    "3. **Create inference pipeline** - Package for production use\n",
    "\n",
    "### If best model is 0.55 < R¬≤_z < 0.60:\n",
    "1. **Build ensemble** - Combine top 2-3 models for 0.02-0.04 boost\n",
    "2. **Try ResNet34/50** - Stronger backbone may help\n",
    "3. **Hyperparameter tuning** - Fine-tune learning rates, weights\n",
    "\n",
    "### If best model < 0.55:\n",
    "1. **Investigate training curves** - Look for overfitting/underfitting\n",
    "2. **Check data quality** - Verify labels and augmentation\n",
    "3. **Try unexplored combinations** - E.g., skip+spatial+focal+aug\n",
    "\n",
    "### Model Checkpoints\n",
    "All trained models are saved in: `checkpoints/`\n",
    "- `stage_A_best.pt` - Best axis predictor\n",
    "- `stage_B_best.pt` - Best intersection predictor  \n",
    "- `stage_C_best.pt` - Best end-to-end model (use this for deployment)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensing-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
