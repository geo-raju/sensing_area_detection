{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "experiment_header",
   "metadata": {},
   "source": [
    "# Quick Experimentation Framework\n",
    "\n",
    "Fast prototyping and testing of model improvements for sensing area detection.\n",
    "\n",
    "**Goal**: Test 15+ approaches in hours instead of days\n",
    "**Strategy**: Reduced epochs + subset data + early stopping + quick metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geo/Documents/sensing_area_detection/sensing-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, math, copy, glob, random, time\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import timm\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Reproducibility setup\"\"\"\n",
    "    # Python random\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy random\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch random\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "# Apply seeding\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "## Quick Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT CONFIGURATION\n",
    "QUICK_CONFIG = {\n",
    "    # Training schedule\n",
    "    'epochs': {\n",
    "        'A': 10,  # Axis pretraining\n",
    "        'B': 15,  # Intersection training\n",
    "        'C': 20   # End-to-end fine-tuning\n",
    "    },\n",
    "    'batch_size': 16,\n",
    "    'early_stop_patience': 3,  # Note: Early stopping currently DISABLED\n",
    "    \n",
    "    # Learning rates per stage\n",
    "    'lr': {\n",
    "        'A': {'backbone': 1e-4, 'axis': 3e-4, 'inter': 0.0, 'spatial': 0.0},\n",
    "        'B': {'backbone': 1e-4, 'axis': 0.0, 'inter': 3e-4, 'spatial': 3e-4},\n",
    "        'C': {'backbone': 1e-5, 'axis': 1e-5, 'inter': 5e-5, 'spatial': 5e-5}\n",
    "    },\n",
    "    \n",
    "    # Loss weights per stage (for training configuration)\n",
    "    'stage_loss_weights': {\n",
    "        'A': {'w_inter': 0.0, 'w_axis': 1.0, 'w_t': 0.0},\n",
    "        'B': {'w_inter': 1.0, 'w_axis': 0.0, 'w_t': 0.0},\n",
    "        'C': {'w_inter': 5.0, 'w_axis': 1.0, 'w_t': 5.0}\n",
    "    },\n",
    "    \n",
    "    # Global loss weights (for stereo_two_stage_loss)\n",
    "    'global_loss_weights': {\n",
    "        'w_origin': 1.0,\n",
    "        'w_dir': 5.0,\n",
    "        'w_xy': 1.0,\n",
    "        'w_z': 5.0,\n",
    "        'use_log_depth': True\n",
    "    },\n",
    "    \n",
    "    # Camera intrinsics\n",
    "    'camera': {\n",
    "        'alpha': 3.0374e+03,\n",
    "        'beta': 3.0335e+03,\n",
    "        'ox': 1.0001e+03,\n",
    "        'oy': 1.0744e+03\n",
    "    },\n",
    "    \n",
    "    # Thresholds (already defined in THRESHOLDS, kept for reference)\n",
    "    'decision_threshold': 5.5,\n",
    "    'max_depth': 220.0,\n",
    "    \n",
    "    # Optimizer\n",
    "    'weight_decay': 1e-5,\n",
    "\n",
    "    # Decision thresholds for experiment progression\n",
    "    # NOTE: Adaptive Stage C scheduling is currently DISABLED - all experiments run full 20 epochs\n",
    "    'thresholds': {\n",
    "        'stage_B_marginal': 0.10,    # Minimum R¬≤_z to attempt Stage C (DISABLED - kept for reference)\n",
    "        'stage_B_promising': 0.40,   # R¬≤_z threshold for full Stage C (DISABLED - kept for reference)\n",
    "        'stage_C_winner': 0.55,      # R¬≤_z for \"winner\" classification\n",
    "        'stage_C_decent': 0.45,      # R¬≤_z for \"decent\" classification\n",
    "        'angular_threshold': 4.0     # Angular error threshold (degrees)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83485a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage configuration created\n"
     ]
    }
   ],
   "source": [
    "# STAGE CONFIGURATION\n",
    "STAGE_CONFIG = {\n",
    "    'A': {\n",
    "        'freeze': ['offset_depth_head', 'spatial_attention', 'spatial_fusion', 'depth_map_head'],\n",
    "        'unfreeze': ['axis_head', 'backbone', 'fusion'],\n",
    "        'e2e': False\n",
    "    },\n",
    "    'B': {\n",
    "        'freeze': ['axis_head'],\n",
    "        'unfreeze': ['offset_depth_head', 'backbone', 'fusion', 'spatial_attention', 'spatial_fusion', 'depth_map_head'],\n",
    "        'e2e': False\n",
    "    },\n",
    "    'C': {\n",
    "        'freeze': [],\n",
    "        'unfreeze': 'all',\n",
    "        'e2e': True\n",
    "    }\n",
    "}\n",
    "\n",
    "def configure_model_for_stage(model, stage):\n",
    "    \"\"\"Configure model parameters for training stage\"\"\"\n",
    "    config = STAGE_CONFIG[stage]\n",
    "\n",
    "    # Freeze components\n",
    "    for component_name in config['freeze']:\n",
    "        if hasattr(model, component_name):\n",
    "            for p in getattr(model, component_name).parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    # Unfreeze components\n",
    "    if config['unfreeze'] == 'all':\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "    else:\n",
    "        for component_name in config['unfreeze']:\n",
    "            if hasattr(model, component_name):\n",
    "                for p in getattr(model, component_name).parameters():\n",
    "                    p.requires_grad = True\n",
    "\n",
    "    return config['e2e']\n",
    "\n",
    "print(\"‚úÖ Stage configuration created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_header",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "data_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "transformations = [A.LongestMaxSize(max_size=224),\n",
    "                   A.PadIfNeeded(min_height=224, min_width=224),\n",
    "                   A.Normalize(),\n",
    "                   A.ToTensorV2()]\n",
    "\n",
    "transform = A.Compose(transformations,\n",
    "                      seed=42,\n",
    "                      keypoint_params=A.KeypointParams(format='xy', remove_invisible=False),\n",
    "                      additional_targets={\n",
    "                          'image_right': 'image',\n",
    "                          'depth_map': 'mask'\n",
    "                      })\n",
    "\n",
    "def compute_pca_axis(points):\n",
    "    \"\"\"Fit PCA to 2D/3D points and return origin, direction.\"\"\"\n",
    "    points_mean = points.mean(axis=0)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(points)\n",
    "    direction = pca.components_[0]  # unit vector\n",
    "    origin = points_mean[:2]  # take x,y as origin\n",
    "    return origin, direction\n",
    "\n",
    "def find_valid_depth(depth_map, x, y, max_search=60):\n",
    "    for r in range(1, max_search+1):\n",
    "        for dx in range(-r, r+1):\n",
    "            for dy in range(-r, r+1):\n",
    "                nx, ny = x+dx, y+dy\n",
    "                if 0 <= nx < depth_map.shape[1] and 0 <= ny < depth_map.shape[0]:\n",
    "                    val = depth_map[ny, nx]\n",
    "                    if val > 0:\n",
    "                        return val\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d305619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AUGMENTATION OPTIONS - Toggle ON/OFF\n",
    "# ============================================================\n",
    "\n",
    "# Set use_augmentation = True or False\n",
    "use_augmentation = False  # Change this to test with/without augmentation\n",
    "\n",
    "if use_augmentation:\n",
    "    print(\"üîÑ Using AUGMENTATION\")\n",
    "    transformations = [\n",
    "        # Geometric augmentations\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.05,\n",
    "            scale_limit=0.1,\n",
    "            rotate_limit=10,\n",
    "            border_mode=0,\n",
    "            p=0.5\n",
    "        ),\n",
    "        \n",
    "        # Color augmentations\n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),\n",
    "            A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=20, p=1.0),\n",
    "            A.RandomGamma(gamma_limit=(80, 120), p=1.0),\n",
    "        ], p=0.5),\n",
    "        \n",
    "        # Noise and blur\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),\n",
    "            A.GaussianBlur(blur_limit=(3, 5), p=1.0),\n",
    "            A.MotionBlur(blur_limit=5, p=1.0),\n",
    "        ], p=0.3),\n",
    "        \n",
    "        # Base transforms (always applied)\n",
    "        A.LongestMaxSize(max_size=224),\n",
    "        A.PadIfNeeded(min_height=224, min_width=224),\n",
    "        A.Normalize(),\n",
    "        A.ToTensorV2()\n",
    "    ]\n",
    "else:\n",
    "    print(\"‚ùå NO AUGMENTATION - Basic transforms only\")\n",
    "    transformations = [\n",
    "        A.LongestMaxSize(max_size=224),\n",
    "        A.PadIfNeeded(min_height=224, min_width=224),\n",
    "        A.Normalize(),\n",
    "        A.ToTensorV2()\n",
    "    ]\n",
    "\n",
    "# Update transform\n",
    "transform = A.Compose(\n",
    "    transformations,\n",
    "    seed=42,\n",
    "    keypoint_params=A.KeypointParams(format='xy', remove_invisible=False),\n",
    "    additional_targets={\n",
    "        'image_right': 'image',\n",
    "        'depth_map': 'mask'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Transforms configured: {len(transformations)} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dataset_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StereoIntersectionDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, max_depth=220.0):\n",
    "        self.left_img_paths = sorted(glob.glob(os.path.join(root_dir, \"left\", \"images\", \"*.jpg\")))\n",
    "        self.right_img_paths = sorted(glob.glob(os.path.join(root_dir, \"right\", \"images\", \"*.jpg\")))\n",
    "        self.probe_axis_paths = sorted(glob.glob(os.path.join(root_dir, \"left\", \"probe_axis\", \"*.txt\")))\n",
    "        self.depth_map_paths = sorted(glob.glob(os.path.join(root_dir, \"left\", \"depth_labels\", \"*.npy\")))\n",
    "        \n",
    "        # Store filenames for optional return\n",
    "        self.transform = transform\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        # Infer split from root_dir (e.g., \"data/processed/train\" -> \"train\")\n",
    "        self.split = os.path.basename(root_dir.rstrip(\"/\"))\n",
    "\n",
    "        # Load ground truth x,y from CenterPt.txt\n",
    "        gt_xy = []\n",
    "        with open(os.path.join(root_dir, \"left\", \"labels\", \"CenterPt.txt\"), 'r') as f:\n",
    "            for line in f:\n",
    "                _, x_str, y_str = line.strip().split(\",\")\n",
    "                gt_xy.append((float(x_str), float(y_str)))\n",
    "        self.gt_xy = np.array(gt_xy, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.left_img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        left_img = cv2.imread(self.left_img_paths[idx])\n",
    "        right_img = cv2.imread(self.right_img_paths[idx])\n",
    "        left_img = cv2.cvtColor(left_img, cv2.COLOR_BGR2RGB)\n",
    "        right_img = cv2.cvtColor(right_img, cv2.COLOR_BGR2RGB)\n",
    "        depth_map = np.load(self.depth_map_paths[idx]) \n",
    "        points = np.loadtxt(self.probe_axis_paths[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(\n",
    "                image=left_img,\n",
    "                image_right=right_img,\n",
    "                keypoints=[self.gt_xy[idx]] + points.tolist(),\n",
    "                depth_map=depth_map\n",
    "            )\n",
    "            left_img = transformed[\"image\"]\n",
    "            right_img = transformed[\"image_right\"]\n",
    "            keypoints = transformed[\"keypoints\"]\n",
    "            depth_map = transformed[\"depth_map\"]\n",
    "\n",
    "        probe_axis_mean, direction = compute_pca_axis(np.array(keypoints[1:]))\n",
    "\n",
    "        _, img_h, img_w = left_img.shape\n",
    "        \n",
    "        probe_axis_mean = np.array(probe_axis_mean, dtype=np.float32) / np.array([img_w, img_h], dtype=np.float32)\n",
    "        probe_axis = torch.tensor(probe_axis_mean, dtype=torch.float32)\n",
    "        probe_dir = torch.tensor(direction, dtype=torch.float32)\n",
    "        \n",
    "        x, y = keypoints[0]\n",
    "        x_idx = int(np.clip(round(x), 0, depth_map.shape[1] - 1))\n",
    "        y_idx = int(np.clip(round(y), 0, depth_map.shape[0] - 1))\n",
    "        z = depth_map[y_idx, x_idx]\n",
    "        if z == 0.0:\n",
    "            z = find_valid_depth(depth_map, x_idx, y_idx)\n",
    "        if z <= 0:\n",
    "            z = 1e-6\n",
    "        intersect_norm = np.array(keypoints[0]) / np.array([img_w, img_h])\n",
    "        target = torch.tensor(intersect_norm.tolist() + [z/self.max_depth], dtype=torch.float32)\n",
    "\n",
    "        batch_dict = {\n",
    "            \"left\": left_img,\n",
    "            \"right\": right_img,\n",
    "            \"gt_origin\": probe_axis,\n",
    "            \"gt_dir\": probe_dir,\n",
    "            \"gt_intersection\": target,\n",
    "            \"depth_map\": depth_map\n",
    "        }\n",
    "\n",
    "        return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "data_loaders",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "Train: 940 | Val: 118\n"
     ]
    }
   ],
   "source": [
    "# Load full datasets\n",
    "train_dataset = StereoIntersectionDataset(\"data/processed/train\", transform=transform)\n",
    "val_dataset = StereoIntersectionDataset(\"data/processed/val\", transform=transform)\n",
    "test_dataset = StereoIntersectionDataset(\"data/processed/test\", transform=transform)\n",
    "\n",
    "# Create generator for reproducible shuffling\n",
    "g_train = torch.Generator()\n",
    "g_train.manual_seed(42)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=QUICK_CONFIG['batch_size'], \n",
    "    shuffle=True,\n",
    "    generator=g_train,  # Reproducible shuffle\n",
    "    num_workers=0  # For reproducibility\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=QUICK_CONFIG['batch_size'], \n",
    "    shuffle=False,\n",
    "    num_workers=0  # For reproducibility\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=QUICK_CONFIG['batch_size'], \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Data loaded successfully\")\n",
    "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline_model_header",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baseline_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ StereoTwoStageNet model defined.\n"
     ]
    }
   ],
   "source": [
    "class StereoTwoStageNet(nn.Module):\n",
    "    \"\"\"Baseline model - Original architecture without modifications\"\"\"\n",
    "    def __init__(self, backbone_name=\"resnet18\", pretrained=True, seed=42):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=pretrained, features_only=True)\n",
    "        self.feature_dims = [f[\"num_chs\"] for f in self.backbone.feature_info]\n",
    "\n",
    "        self.proj = nn.ModuleList([nn.Conv2d(c, 128, 1) for c in self.feature_dims])\n",
    "\n",
    "        fused_channels = 128 * len(self.feature_dims) * 2  \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(fused_channels, 256, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        self.axis_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 4)  # (x0, y0, dx, dy)\n",
    "        )\n",
    "\n",
    "        self.offset_depth_head = nn.Sequential(\n",
    "            nn.Linear(256 + 4, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 2)  # (t, z_raw)\n",
    "        )\n",
    "\n",
    "        self.softplus = nn.Softplus(beta=1.0)\n",
    "\n",
    "    def _fused_vec(self, left_img, right_img):\n",
    "        feats_l = self.backbone(left_img)\n",
    "        feats_r = self.backbone(right_img)\n",
    "        H, W = feats_l[-1].shape[2:]\n",
    "        \n",
    "        fused_scales = []\n",
    "        for fl, fr, proj in zip(feats_l, feats_r, self.proj):\n",
    "            fl = proj(fl)\n",
    "            fr = proj(fr)\n",
    "            fl = F.adaptive_avg_pool2d(fl, (H, W))\n",
    "            fr = F.adaptive_avg_pool2d(fr, (H, W))\n",
    "            fused_scales.append(torch.cat([fl, fr], dim=1))\n",
    "\n",
    "        x = torch.cat(fused_scales, dim=1)\n",
    "        x = self.fusion(x)\n",
    "        v = x.view(x.size(0), -1)\n",
    "        return v, x\n",
    "    \n",
    "    def forward(self, left_img, right_img):\n",
    "        fused_vec, fused_4d = self._fused_vec(left_img, right_img)\n",
    "        axis_params = self.axis_head(fused_4d)\n",
    "        origin = axis_params[:, :2]\n",
    "        direction = F.normalize(axis_params[:, 2:], dim=1)\n",
    "        \n",
    "        conditioned = torch.cat([fused_vec, origin.detach(), direction.detach()], dim=1)\n",
    "        od = self.offset_depth_head(conditioned)\n",
    "        offset_t = od[:, 0:1]\n",
    "        depth_z = self.softplus(od[:, 1:2])\n",
    "        \n",
    "        xy_inter = origin + offset_t * direction\n",
    "        intersection = torch.cat([xy_inter, depth_z], dim=1)\n",
    "        \n",
    "        return {\n",
    "            \"origin\": origin,\n",
    "            \"direction\": direction,\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": intersection\n",
    "        }\n",
    "\n",
    "    def forward_e2e(self, left_img, right_img):\n",
    "        fused_vec, fused_4d = self._fused_vec(left_img, right_img)\n",
    "        axis_params = self.axis_head(fused_4d)\n",
    "        origin = axis_params[:, :2]\n",
    "        direction = F.normalize(axis_params[:, 2:], dim=1)\n",
    "        conditioned = torch.cat([fused_vec, origin, direction], dim=1)  # no detach\n",
    "        od = self.offset_depth_head(conditioned)\n",
    "        offset_t = od[:, 0:1]\n",
    "        depth_z = self.softplus(od[:, 1:2])\n",
    "        xy_inter = origin + offset_t * direction\n",
    "        intersection = torch.cat([xy_inter, depth_z], dim=1)\n",
    "        return {\n",
    "            \"origin\": origin,\n",
    "            \"direction\": direction,\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": intersection\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ StereoTwoStageNet model defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utils_header",
   "metadata": {},
   "source": [
    "## Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "loss_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loss functions defined.\n"
     ]
    }
   ],
   "source": [
    "def berhu_loss(pred, target, eps=1e-6, use_focal=False, focal_alpha=0.25, focal_gamma=1.5):\n",
    "    \"\"\"\n",
    "    BerHu (Reverse Huber) loss with optional focal weighting\n",
    "    \n",
    "    Args:\n",
    "        pred: predictions\n",
    "        target: ground truth\n",
    "        eps: small epsilon for numerical stability\n",
    "        use_focal: if True, apply focal weighting to focus on hard samples\n",
    "        focal_alpha: focal loss alpha parameter (default: 0.25) - modulation strength\n",
    "        focal_gamma: focal loss gamma parameter (default: 1.5) - focus sharpness\n",
    "    \n",
    "    Returns:\n",
    "        Mean of the (optionally focal-weighted) BerHu loss (scalar)\n",
    "    \n",
    "    FIXED: Corrected focal loss formula for regression\n",
    "    - Old: weight = alpha * (normalized_loss)^gamma (too aggressive, ignored easy samples)\n",
    "    - New: weight = 1.0 + alpha * (normalized_loss)^gamma (balanced modulation)\n",
    "    \"\"\"\n",
    "    x = pred - target\n",
    "    abs_x = torch.abs(x)\n",
    "    c = 0.2 * abs_x.max().clamp(min=eps)\n",
    "    l1_mask = abs_x <= c\n",
    "    l1_loss = abs_x\n",
    "    l2_loss = (x ** 2 + c ** 2) / (2 * c + eps)\n",
    "    loss = torch.where(l1_mask, l1_loss, l2_loss)\n",
    "\n",
    "    # Apply focal weighting if requested\n",
    "    if use_focal:\n",
    "        # Normalize loss to [0, 1] range for stable weighting\n",
    "        max_loss = loss.detach().max() + eps\n",
    "        normalized_loss = loss / max_loss\n",
    "        \n",
    "        # Focal modulation: gradually up-weight hard samples\n",
    "        # Easy samples (loss‚âà0): weight ‚âà 1.0\n",
    "        # Hard samples (loss‚âàmax): weight ‚âà 1.0 + alpha\n",
    "        focal_weight = 1.0 + focal_alpha * torch.pow(normalized_loss, focal_gamma)\n",
    "        loss = focal_weight * loss\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "def compute_gt_t(gt_inter, gt_origin, gt_dir, eps=1e-6):\n",
    "    xy_diff = gt_inter[:, :2] - gt_origin[:, :2]\n",
    "    gt_dir_norm = F.normalize(gt_dir, dim=1)\n",
    "    t_gt = (xy_diff * gt_dir_norm).sum(dim=1, keepdim=True) / (gt_dir_norm.norm(dim=1, keepdim=True)**2 + eps)\n",
    "    return t_gt\n",
    "\n",
    "def stereo_two_stage_loss(outputs, targets, **kwargs):\n",
    "    w_origin = kwargs.get('w_origin', 1.0)\n",
    "    w_dir = kwargs.get('w_dir', 5.0)\n",
    "    w_xy = kwargs.get('w_xy', 1.0)\n",
    "    w_z = kwargs.get('w_z', 5.0)\n",
    "    w_inter = kwargs.get('w_inter', 1.0)\n",
    "    w_axis = kwargs.get('w_axis', 0.2)\n",
    "    w_t = kwargs.get('w_t', 1.0)\n",
    "    use_log_depth = kwargs.get('use_log_depth', True)\n",
    "    eps = kwargs.get('eps', 1e-6)\n",
    "\n",
    "    # Focal loss parameters\n",
    "    use_focal = kwargs.get('use_focal', False)\n",
    "    focal_alpha = kwargs.get('focal_alpha', 0.25)\n",
    "    focal_gamma = kwargs.get('focal_gamma', 1.5)\n",
    "\n",
    "    pred_inter = outputs[\"intersection\"]\n",
    "    pred_z = outputs[\"depth_z\"]\n",
    "    pred_origin = outputs[\"origin\"]\n",
    "    pred_dir = outputs[\"direction\"]\n",
    "    pred_t = outputs[\"offset_t\"]\n",
    "\n",
    "    gt_inter = targets[\"gt_intersection\"]\n",
    "    gt_origin = targets[\"gt_origin\"]\n",
    "    gt_dir = targets[\"gt_dir\"]\n",
    "\n",
    "    # Intersection losses\n",
    "    loss_xy = berhu_loss(pred_inter[:, :2], gt_inter[:, :2], eps=eps, \n",
    "                        use_focal=use_focal, focal_alpha=focal_alpha, focal_gamma=focal_gamma)\n",
    "\n",
    "    # Z loss\n",
    "    if use_log_depth:\n",
    "        pred_log_z = torch.log(pred_z + eps)\n",
    "        gt_log_z = torch.log(gt_inter[:, 2:] + eps)\n",
    "        loss_z = berhu_loss(pred_log_z, gt_log_z, eps=eps, use_focal=use_focal,\n",
    "                           focal_alpha=focal_alpha, focal_gamma=focal_gamma)\n",
    "    else:\n",
    "        loss_z = berhu_loss(pred_z.squeeze(1), gt_inter[:, 2], eps=eps, use_focal=use_focal,\n",
    "                           focal_alpha=focal_alpha, focal_gamma=focal_gamma)\n",
    "\n",
    "    loss_inter = w_xy * loss_xy + w_z * loss_z\n",
    "\n",
    "    # Axis losses\n",
    "    lo = ((pred_origin - gt_origin) ** 2).sum(dim=1).mean()\n",
    "    pd = F.normalize(pred_dir, dim=1)\n",
    "    gd = F.normalize(gt_dir, dim=1)\n",
    "    cos = (pd * gd).sum(dim=1).clamp(-1+eps, 1-eps)\n",
    "    ld = (1.0 - cos).mean()\n",
    "    loss_axis = w_origin * lo + w_dir * ld\n",
    "\n",
    "    # Offset loss\n",
    "    gt_t = compute_gt_t(gt_inter, gt_origin, gt_dir, eps=eps)\n",
    "    loss_t = ((pred_t - gt_t) ** 2).mean()\n",
    "\n",
    "    total = w_inter * loss_inter + w_axis * loss_axis + w_t * loss_t\n",
    "    return total\n",
    "\n",
    "print(\"‚úÖ Loss functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "evaluation_utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_device(batch, device):\n",
    "    \"\"\"\n",
    "    Move batch to device. Handles dict batches and tuple/list batches.\n",
    "    \n",
    "    When return_filename=True, DataLoader collates as:\n",
    "    - batch = (batch_dict, [filenames...], (split, split, ...)) [3-element tuple]\n",
    "      where split is collated into a tuple by DataLoader\n",
    "    \"\"\"\n",
    "    # Handle tuple/list format from return_filename=True\n",
    "    if isinstance(batch, (list, tuple)):\n",
    "        # 3 elements: (batch_dict, filenames, splits)\n",
    "        if len(batch) == 3 and isinstance(batch[0], dict):\n",
    "            batch_dict, filenames, splits = batch\n",
    "            \n",
    "            # Move tensors to device\n",
    "            out = {}\n",
    "            for k, v in batch_dict.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    out[k] = v.to(device)\n",
    "                else:\n",
    "                    out[k] = v\n",
    "            \n",
    "            # Add filenames\n",
    "            out['filename'] = list(filenames) if not isinstance(filenames, list) else filenames\n",
    "            \n",
    "            # Handle splits - DataLoader collates them into a tuple/list\n",
    "            # Convert to list if it's a tuple\n",
    "            if isinstance(splits, (tuple, list)):\n",
    "                out['split'] = list(splits)\n",
    "            else:\n",
    "                # Single split value - replicate for all items\n",
    "                out['split'] = [splits] * len(out['filename'])\n",
    "            \n",
    "            return out\n",
    "        \n",
    "        # 2 elements: (batch_dict, filenames) - legacy support\n",
    "        elif len(batch) == 2 and isinstance(batch[0], dict):\n",
    "            batch_dict, filenames = batch\n",
    "            \n",
    "            # Move tensors to device\n",
    "            out = {}\n",
    "            for k, v in batch_dict.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    out[k] = v.to(device)\n",
    "                else:\n",
    "                    out[k] = v\n",
    "            \n",
    "            # Add filenames and infer split as 'train' (fallback)\n",
    "            out['filename'] = list(filenames) if not isinstance(filenames, list) else filenames\n",
    "            out['split'] = ['train'] * len(out['filename'])\n",
    "            \n",
    "            return out\n",
    "    \n",
    "    # Handle standard dict format\n",
    "    if isinstance(batch, dict):\n",
    "        out = {}\n",
    "        for k, v in batch.items():\n",
    "            if torch.is_tensor(v):\n",
    "                out[k] = v.to(device)\n",
    "            else:\n",
    "                out[k] = v\n",
    "        return out\n",
    "    \n",
    "    # Fallback error\n",
    "    raise TypeError(f\"Unexpected batch type: {type(batch)} with {len(batch) if isinstance(batch, (list, tuple)) else 'N/A'} elements. Expected dict or tuple/list of (dict, filenames[, split])\")\n",
    "\n",
    "def transform2Dto3D_torch(Z, uv, alpha=3.0374e+03, beta=3.0335e+03, ox=1.0001e+03, oy=1.0744e+03):\n",
    "    u, v = uv[:, 0], uv[:, 1]\n",
    "    X = (Z * (u - ox)) / alpha\n",
    "    Y = (Z * (v - oy)) / beta\n",
    "    return torch.stack([X, Y, Z], dim=1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def r2_score_torch(pred, target):\n",
    "    \"\"\"Torch-based R¬≤ score computation\"\"\"\n",
    "    ss_res = ((pred - target)**2).sum(dim=0)\n",
    "    mean_t = target.mean(dim=0, keepdim=True)\n",
    "    ss_tot = ((target - mean_t)**2).sum(dim=0)\n",
    "    r2 = 1.0 - ss_res / (ss_tot + 1e-12)\n",
    "    return r2\n",
    "\n",
    "@torch.no_grad()\n",
    "def angle_deg(pred_dir, gt_dir):\n",
    "    pd = F.normalize(pred_dir, dim=1)\n",
    "    gd = F.normalize(gt_dir, dim=1)\n",
    "    cos = (pd * gd).sum(dim=1).clamp(-1+1e-6, 1-1e-6)\n",
    "    return torch.acos(cos).mean().item() * 180.0 / math.pi\n",
    "\n",
    "@torch.no_grad()\n",
    "def errors_2d_3d(pred_inter, gt_inter, img_w=256, img_h=256, max_depth=220.0):\n",
    "    pred_px = torch.empty_like(pred_inter)\n",
    "    gt_px = torch.empty_like(gt_inter)\n",
    "\n",
    "    pred_px[:, 0] = pred_inter[:, 0] * img_w\n",
    "    pred_px[:, 1] = pred_inter[:, 1] * img_h  \n",
    "    pred_px[:, 2] = pred_inter[:, 2] * max_depth\n",
    "\n",
    "    gt_px[:, 0] = gt_inter[:, 0] * img_w\n",
    "    gt_px[:, 1] = gt_inter[:, 1] * img_h\n",
    "    gt_px[:, 2] = gt_inter[:, 2] * max_depth\n",
    "\n",
    "    e2 = torch.norm(pred_px[:, :2] - gt_px[:, :2], dim=1).mean()\n",
    "    \n",
    "    pred3d = transform2Dto3D_torch(pred_px[:, 2], pred_px[:, :2])\n",
    "    gt3d = transform2Dto3D_torch(gt_px[:, 2], gt_px[:, :2])\n",
    "    e3 = torch.norm(pred3d - gt3d, dim=1).mean()\n",
    "    \n",
    "    return e2.item(), e3.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f50cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optim(model, stage, lr_spatial=None):\n",
    "    \"\"\"\n",
    "    Create optimizer with stage-specific learning rates\n",
    "    \n",
    "    Args:\n",
    "        model: Model to optimize\n",
    "        stage: Training stage ('A', 'B', or 'C')\n",
    "        lr_spatial: Optional learning rate for spatial attention components\n",
    "    \n",
    "    Returns:\n",
    "        torch.optim.Adam optimizer\n",
    "    \"\"\"\n",
    "    lr_config = QUICK_CONFIG['lr'][stage]\n",
    "    wd = QUICK_CONFIG['weight_decay']\n",
    "\n",
    "    # Base parameter groups\n",
    "    param_groups = [\n",
    "        {\"params\": model.backbone.parameters(),\n",
    "         \"lr\": lr_config['backbone'], \"weight_decay\": wd},\n",
    "        {\"params\": model.axis_head.parameters(),\n",
    "         \"lr\": lr_config['axis'], \"weight_decay\": wd}\n",
    "    ]\n",
    "\n",
    "    # Handle multi-head depth predictor\n",
    "    if hasattr(model, 'head_coarse'):\n",
    "        # Multi-head model: Add each head with intersection learning rate\n",
    "        param_groups.extend([\n",
    "            {\"params\": model.head_coarse.parameters(),\n",
    "             \"lr\": lr_config['inter'], \"weight_decay\": wd},\n",
    "            {\"params\": model.head_medium.parameters(),\n",
    "             \"lr\": lr_config['inter'], \"weight_decay\": wd},\n",
    "            {\"params\": model.head_fine.parameters(),\n",
    "             \"lr\": lr_config['inter'], \"weight_decay\": wd}\n",
    "        ])\n",
    "    elif hasattr(model, 'offset_depth_head'):\n",
    "        # Single head model\n",
    "        param_groups.append(\n",
    "            {\"params\": model.offset_depth_head.parameters(),\n",
    "             \"lr\": lr_config['inter'], \"weight_decay\": wd}\n",
    "        )\n",
    "\n",
    "    # Add spatial attention if present\n",
    "    if hasattr(model, 'spatial_attention'):\n",
    "        lr_spatial_actual = lr_spatial if lr_spatial is not None else lr_config.get('spatial', 0.0)\n",
    "        if lr_spatial_actual > 0:\n",
    "            param_groups.extend([\n",
    "                {\"params\": model.spatial_attention.parameters(),\n",
    "                 \"lr\": lr_spatial_actual, \"weight_decay\": wd},\n",
    "                {\"params\": model.spatial_fusion.parameters(),\n",
    "                 \"lr\": lr_spatial_actual, \"weight_decay\": wd}\n",
    "            ])\n",
    "\n",
    "    # Add skip connection fusion layer if present\n",
    "    if hasattr(model, 'skip_fusion'):\n",
    "        param_groups.append(\n",
    "            {\"params\": model.skip_fusion.parameters(),\n",
    "             \"lr\": lr_config['backbone'], \"weight_decay\": wd}\n",
    "        )\n",
    "    \n",
    "    # Add depth map head for auxiliary depth models\n",
    "    if hasattr(model, 'depth_map_head'):\n",
    "        param_groups.append(\n",
    "            {\"params\": model.depth_map_head.parameters(),\n",
    "             \"lr\": lr_config['inter'], \"weight_decay\": wd}\n",
    "        )\n",
    "    \n",
    "    return torch.optim.Adam(param_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick_framework_header",
   "metadata": {},
   "source": [
    "## ‚ö° Quick Experimentation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "209a2628",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, loader, device, e2e=False, strategy='standard', **kwargs):\n",
    "    \"\"\"\n",
    "    Validate model on dataset with error handling and detailed metrics.\n",
    "    Args:\n",
    "        model: Model to validate\n",
    "        loader: DataLoader for validation data\n",
    "        device: Device to run on\n",
    "        e2e: If True, use end-to-end forward pass\n",
    "        strategy: 'standard', 'MultiHead', or 'auxiliary' for loss handling\n",
    "        **kwargs: Additional args for loss functions\n",
    "    Returns:\n",
    "        Dictionary of validation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Validation accumulators\n",
    "    metrics = {\n",
    "        'val_loss': 0.0,\n",
    "        'batch_count': 0,\n",
    "        'samples_seen': 0,\n",
    "        'head_metrics': defaultdict(lambda: defaultdict(list)) if strategy == 'MultiHead' else None,\n",
    "        'depth_map_error': [] if strategy == 'auxiliary' else None  # Initialize for auxiliary\n",
    "    }\n",
    "    \n",
    "    # Prediction collectors\n",
    "    predictions = defaultdict(list)\n",
    "    targets = defaultdict(list)\n",
    "    \n",
    "    try:\n",
    "        for batch_idx, batch in enumerate(loader):\n",
    "            batch = batch_to_device(batch, device)\n",
    "            L, R = batch[\"left\"], batch[\"right\"]\n",
    "            \n",
    "            # Ground truth\n",
    "            go, gd = batch[\"gt_origin\"], batch[\"gt_dir\"]\n",
    "            gi = batch[\"gt_intersection\"]\n",
    "\n",
    "            # Load auxiliary depth maps if needed\n",
    "            if strategy == 'auxiliary' and 'depth_map' in batch:\n",
    "                batch['gt_depth_map'] = torch.stack([\n",
    "                    load_depth_map_for_batch(depth).to(device)\n",
    "                    for depth in batch['depth_map']\n",
    "                ])\n",
    "            \n",
    "            # Forward pass with strategy-specific handling\n",
    "            try:\n",
    "                out = model.forward_e2e(L, R) if e2e else model(L, R)\n",
    "\n",
    "                # Compute loss based on strategy\n",
    "                if strategy == 'auxiliary':\n",
    "                    loss = auxiliary_depth_loss(out, batch, **kwargs)\n",
    "                    \n",
    "                    # Track depth map error if available\n",
    "                    if 'depth_map' in out and 'gt_depth_map' in batch:\n",
    "                        pred_depth = out['depth_map']\n",
    "                        gt_depth = batch['gt_depth_map']\n",
    "                        valid_mask = gt_depth > 0.1\n",
    "                        if valid_mask.sum() > 10:\n",
    "                            depth_error = F.l1_loss(pred_depth[valid_mask], gt_depth[valid_mask])\n",
    "                            metrics['depth_map_error'].append(depth_error.item())\n",
    "                            \n",
    "                elif strategy == 'MultiHead':\n",
    "                    loss, head_losses = multihead_loss(out, batch, **kwargs)\n",
    "                    \n",
    "                    # Track per-head predictions\n",
    "                    if 'head_preds' in out:\n",
    "                        for head_name, preds in out['head_preds'].items():\n",
    "                            # Get intersection point for this head\n",
    "                            head_intersection = torch.cat([\n",
    "                                out[\"origin\"] + preds[:, 0:1] * F.normalize(out[\"direction\"], dim=1),\n",
    "                                F.softplus(preds[:, 1:2])\n",
    "                            ], dim=1)\n",
    "                            \n",
    "                            # Store predictions and losses\n",
    "                            metrics['head_metrics'][head_name]['predictions'].append(head_intersection)\n",
    "                            metrics['head_metrics'][head_name]['losses'].append(head_losses[head_name].item())\n",
    "                else:\n",
    "                    loss = stereo_two_stage_loss(out, batch, **kwargs)\n",
    "                \n",
    "                # Update metrics\n",
    "                metrics['val_loss'] += float(loss.item())\n",
    "                metrics['batch_count'] += 1\n",
    "                metrics['samples_seen'] += L.size(0)\n",
    "                \n",
    "                # Collect predictions and targets\n",
    "                predictions['origin'].append(out[\"origin\"])\n",
    "                predictions['direction'].append(out[\"direction\"])\n",
    "                predictions['intersection'].append(out[\"intersection\"])\n",
    "                targets['origin'].append(go)\n",
    "                targets['direction'].append(gd)\n",
    "                targets['intersection'].append(gi)\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"\\nWarning: Error in batch {batch_idx}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during validation: {str(e)}\")\n",
    "        return {\n",
    "            \"val_loss\": float('inf'),\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "    \n",
    "    # Compute final metrics\n",
    "    try:\n",
    "        # Concatenate collected tensors\n",
    "        PO = torch.cat(predictions['origin'])\n",
    "        PD = torch.cat(predictions['direction'])\n",
    "        PI = torch.cat(predictions['intersection'])\n",
    "        GO = torch.cat(targets['origin'])\n",
    "        GD = torch.cat(targets['direction'])\n",
    "        GI = torch.cat(targets['intersection'])\n",
    "        \n",
    "        # Main metrics\n",
    "        final_metrics = {\n",
    "            \"val_loss\": metrics['val_loss'] / max(1, metrics['batch_count']),\n",
    "            \"ang_deg\": angle_deg(PD, GD),\n",
    "            \"r2o\": r2_score_torch(PO, GO).tolist()[:2],\n",
    "            \"r2d\": r2_score_torch(PD, F.normalize(GD, dim=1)).tolist()[:2],\n",
    "            \"r2xyz\": r2_score_torch(PI, GI).tolist()[:3],\n",
    "            \"samples_validated\": metrics['samples_seen']\n",
    "        }\n",
    "\n",
    "        # 2D/3D errors\n",
    "        e2, e3 = errors_2d_3d(PI, GI)\n",
    "        final_metrics.update({\"e2d\": e2, \"e3d\": e3})\n",
    "\n",
    "        # Add auxiliary depth metrics if available\n",
    "        if strategy == 'auxiliary' and metrics['depth_map_error']:\n",
    "            final_metrics['depth_map_error'] = sum(metrics['depth_map_error']) / len(metrics['depth_map_error'])\n",
    "        \n",
    "        # Strategy-specific metrics\n",
    "        if strategy == 'MultiHead' and metrics['head_metrics']:\n",
    "            head_metrics = {}\n",
    "            for head_name, head_data in metrics['head_metrics'].items():\n",
    "                if head_data['predictions']:\n",
    "                    P = torch.cat(head_data['predictions'])\n",
    "                    avg_loss = sum(head_data['losses']) / len(head_data['losses'])\n",
    "                    r2xyz = r2_score_torch(P, GI).tolist()[:3]\n",
    "                    e2, e3 = errors_2d_3d(P, GI)\n",
    "                    \n",
    "                    head_metrics[head_name] = {\n",
    "                        'loss': avg_loss,\n",
    "                        'r2_z': r2xyz[2],\n",
    "                        'r2_xy': (r2xyz[0] + r2xyz[1]) / 2,\n",
    "                        'e2d': e2,\n",
    "                        'e3d': e3\n",
    "                    }\n",
    "            final_metrics['head_metrics'] = head_metrics\n",
    "            \n",
    "        # Memory cleanup\n",
    "        del predictions, targets\n",
    "        if 'head_metrics' in metrics:\n",
    "            del metrics['head_metrics']\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        return final_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError computing final metrics: {str(e)}\")\n",
    "        return {\n",
    "            \"val_loss\": metrics['val_loss'] / max(1, metrics['batch_count']),\n",
    "            \"error\": str(e),\n",
    "            \"partial_results\": True\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early_stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    \"\"\"Early stopping based on validation metrics\"\"\"\n",
    "    def __init__(self, patience=3, min_delta=0.001, stage='A'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.stage = stage\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_metric = -float('inf') if stage in ['A', 'B'] else float('inf')\n",
    "        \n",
    "    def should_stop(self, logs):\n",
    "        \"\"\"Return True if should stop training\"\"\"\n",
    "        if self.stage == 'A':\n",
    "            # Stage A: Stop if angular error < 8¬∞ or no improvement\n",
    "            metric = logs['ang_deg']\n",
    "            if metric < 8.0:  # Good enough angular error\n",
    "                return True\n",
    "            improved = logs['val_loss'] < (self.best_loss - self.min_delta)\n",
    "        elif self.stage == 'B':\n",
    "            # Stage B: Stop if R¬≤_z > 0.3 or no improvement  \n",
    "            metric = logs['r2xyz'][2]\n",
    "            if metric > 0.3:  # Decent R¬≤_z\n",
    "                return True\n",
    "            improved = logs['val_loss'] < (self.best_loss - self.min_delta)\n",
    "        else:  # Stage C\n",
    "            # Stage C: Stop if trend is clear\n",
    "            improved = logs['val_loss'] < (self.best_loss - self.min_delta)\n",
    "            \n",
    "        if improved:\n",
    "            self.best_loss = logs['val_loss']\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            \n",
    "        return self.counter >= self.patience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff363e32",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Early Stopping Status: DISABLED\n",
    "\n",
    "Early stopping has been temporarily disabled to allow experiments to run for the full configured number of epochs. This ensures consistent training across all experiments and makes results more comparable.\n",
    "\n",
    "The `EarlyStopper` class is still defined below but is not actively used in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9053100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_epoch_progress(epoch, max_epochs, train_metrics, val_metrics, strategy, epoch_time, best_epoch, stage, current_weights=None):\n",
    "    \"\"\"Helper function to print epoch progress - single clean line per epoch\"\"\"\n",
    "    avg_loss = train_metrics['loss'] / len(train_loader)\n",
    "    \n",
    "    if strategy == 'MultiHead':\n",
    "        # MultiHead: Show angular error in Stage A, depth metrics in later stages\n",
    "        if stage == 'A' or 'r2xyz' not in val_metrics:\n",
    "            ang = val_metrics.get('ang_deg', 0)\n",
    "            msg = f\"Epoch {epoch}/{max_epochs} | Loss: {avg_loss:.4f} | Val: {val_metrics['val_loss']:.4f} | Ang: {ang:.2f}¬∞\\n\"\n",
    "        else:\n",
    "            r2_z = val_metrics['r2xyz'][2]\n",
    "            e3d = val_metrics.get('e3d', 0)\n",
    "            msg = f\"Epoch {epoch}/{max_epochs} | Loss: {avg_loss:.4f} | Val: {val_metrics['val_loss']:.4f} | R¬≤_z: {r2_z:.3f} | 3D: {e3d:.2f}mm | Time: {epoch_time:.1f}s\\n\"\n",
    "        print(msg)\n",
    "\n",
    "        # Per-head metrics (only show in stages B/C when available)\n",
    "        if 'head_metrics' in val_metrics and stage != 'A' and 'r2xyz' in val_metrics:\n",
    "            head_summary = \" | \".join([f\"{name}: {metrics['r2_z']:.3f}\" \n",
    "                                       for name, metrics in val_metrics['head_metrics'].items()])\n",
    "            msg = f\"  ‚Ü≥ Heads: {head_summary}\\n\"\n",
    "            print(msg)\n",
    "    \n",
    "    elif strategy == 'curriculum':\n",
    "        # Curriculum: Show unweighted metrics and current w_z\n",
    "        if stage == 'A':\n",
    "            ang = val_metrics.get('ang_deg', 0)\n",
    "            msg = f\"Epoch {epoch}/{max_epochs} | Loss: {avg_loss:.4f} | Val: {val_metrics['val_loss']:.4f} | Ang: {ang:.2f}¬∞\\n\"\n",
    "        else:\n",
    "            r2_z = val_metrics['r2xyz'][2]\n",
    "            e3d = val_metrics.get('e3d', 0)\n",
    "            \n",
    "            # Get current w_z if available\n",
    "            w_z = current_weights.get('w_z', 5.0) if current_weights else 5.0\n",
    "            \n",
    "            # Show both weighted loss and unweighted metrics\n",
    "            # Note: Loss is weighted and NOT comparable across epochs\n",
    "            msg = f\"Epoch {epoch}/{max_epochs} | Loss: {avg_loss:.4f} (w_z={w_z:.1f}) | R¬≤_z: {r2_z:.3f} | 3D: {e3d:.2f}mm\\n\"\n",
    "        print(msg)\n",
    "\n",
    "    else:  # Standard progress - single line\n",
    "        if stage == 'A':\n",
    "            ang = val_metrics.get('ang_deg', 0)\n",
    "            msg = f\"Epoch {epoch}/{max_epochs} | Loss: {avg_loss:.4f} | Val: {val_metrics['val_loss']:.4f} | Ang: {ang:.2f}¬∞\\n\"\n",
    "        else:\n",
    "            r2_z = val_metrics['r2xyz'][2]\n",
    "            e3d = val_metrics.get('e3d', 0)\n",
    "            msg = f\"Epoch {epoch}/{max_epochs} | Loss: {avg_loss:.4f} | Val: {val_metrics['val_loss']:.4f} | R¬≤_z: {r2_z:.3f} | 3D: {e3d:.2f}mm\\n\"\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d359e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage(model, train_loader, val_loader, stage, max_epochs, device,\n",
    "                use_amp=False, use_focal=False, focal_alpha=0.25, focal_gamma=1.5,\n",
    "                strategy='standard', **strategy_kwargs):\n",
    "    \"\"\"\n",
    "    Enhanced training function with better progress tracking, memory management,\n",
    "    and error handling.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to train\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        stage: Training stage ('A', 'B', or 'C')\n",
    "        max_epochs: Maximum number of epochs\n",
    "        device: Device to train on\n",
    "        use_amp: Use automatic mixed precision\n",
    "        use_focal: Use focal loss for depth prediction\n",
    "        focal_alpha: Focal loss alpha parameter\n",
    "        focal_gamma: Focal loss gamma parameter\n",
    "        strategy: Training strategy (standard/progressive/auxiliary/curriculum/spatial)\n",
    "        **strategy_kwargs: Strategy-specific parameters\n",
    "    \n",
    "    Returns:\n",
    "        dict: Best validation metrics including epoch number\n",
    "    \"\"\"\n",
    "    # Initialize tracking\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'r2_z': [],\n",
    "        'e3d': [],\n",
    "        'lr': [],\n",
    "        'speed': [],\n",
    "        'epoch_times': []\n",
    "    }\n",
    "    \n",
    "    if strategy == 'MultiHead':\n",
    "        for head in ['coarse', 'medium', 'fine']:\n",
    "            history[f'{head}_loss'] = []\n",
    "            history[f'{head}_r2z'] = []\n",
    "            history[f'{head}_e3d'] = []\n",
    "\n",
    "    # Configure training\n",
    "    e2e = configure_model_for_stage(model, stage)\n",
    "    optim = make_optim(model, stage)\n",
    "    scaler = GradScaler(enabled=use_amp)\n",
    "    # early_stopper = EarlyStopper(patience=QUICK_CONFIG['early_stop_patience'], stage=stage)  # DISABLED\n",
    "    \n",
    "    # Get loss weights and update with strategy-specific settings\n",
    "    loss_weights = QUICK_CONFIG['stage_loss_weights'][stage].copy()\n",
    "    loss_weights.update(QUICK_CONFIG['global_loss_weights'])\n",
    "    \n",
    "    try:\n",
    "        # Strategy-specific setup\n",
    "        if strategy == 'progressive':\n",
    "            if stage == 'C':\n",
    "                total_epochs = max_epochs\n",
    "            else:\n",
    "                model.detach_alpha = torch.tensor(1.0, device=device)\n",
    "        elif strategy == 'auxiliary':\n",
    "            w_depth_map_dict = strategy_kwargs.get('w_depth_map', {\n",
    "                'A': 0.0, 'B': 0.5, 'C': 1.0\n",
    "            })\n",
    "            w_depth_map = w_depth_map_dict.get(stage, 0.5)\n",
    "            loss_weights['w_depth_map'] = w_depth_map\n",
    "        elif strategy == 'MultiHead':\n",
    "            head_weights = strategy_kwargs.get('head_weights', {\n",
    "                'coarse': 0.2, 'medium': 0.3, 'fine': 0.5\n",
    "            })\n",
    "            if abs(sum(head_weights.values()) - 1.0) > 1e-6:\n",
    "                print(f\"‚ö†Ô∏è Head weights sum to {sum(head_weights.values()):.3f}, not 1.0\")\n",
    "            loss_weights['head_weights'] = head_weights\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Strategy setup error: {str(e)}\")\n",
    "        print(\"Falling back to standard training\")\n",
    "        strategy = 'standard'\n",
    "\n",
    "    # Training state\n",
    "    best_metrics = None\n",
    "    best_epoch = 0\n",
    "    best_head_metrics = None\n",
    "    total_batches = len(train_loader)\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Strategy updates\n",
    "        if strategy == 'progressive' and stage == 'C':\n",
    "            alpha = 1.0 - epoch / total_epochs\n",
    "            if hasattr(model, 'detach_alpha'):\n",
    "                model.detach_alpha = torch.tensor(alpha, device=device)\n",
    "        elif strategy == 'curriculum':\n",
    "            loss_weights['w_z'] = get_curriculum_weights(stage, epoch-1, max_epochs)['w_z']\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_metrics = defaultdict(float)\n",
    "        batch_times = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            batch_start = time.time()\n",
    "            \n",
    "            # Process batch\n",
    "            batch = batch_to_device(batch, device)\n",
    "            \n",
    "            # Process depth maps if using auxiliary strategy\n",
    "            if strategy == 'auxiliary' and 'depth_map' in batch:\n",
    "                batch['gt_depth_map'] = torch.stack([\n",
    "                    load_depth_map_for_batch(depth).to(device)\n",
    "                    for depth in batch['depth_map']\n",
    "                ])\n",
    "\n",
    "            # Forward pass\n",
    "            optim.zero_grad()\n",
    "            with autocast(device_type=device.type, enabled=use_amp):\n",
    "                out = model.forward_e2e(batch[\"left\"], batch[\"right\"]) if e2e else model(batch[\"left\"], batch[\"right\"])\n",
    "                \n",
    "                # Compute loss based on strategy\n",
    "                if strategy == 'auxiliary':\n",
    "                    loss = auxiliary_depth_loss(out, batch, **loss_weights)\n",
    "                elif strategy == 'MultiHead':\n",
    "                    loss, head_losses = multihead_loss(out, batch, **loss_weights)\n",
    "                    for name, hloss in head_losses.items():\n",
    "                        train_metrics[f'{name}_loss'] += hloss.item()\n",
    "                else:\n",
    "                    loss = stereo_two_stage_loss(out, batch, **loss_weights)\n",
    "\n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optim)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Update metrics\n",
    "            train_metrics['loss'] += loss.item()\n",
    "            batch_time = time.time() - batch_start\n",
    "            batch_times.append(batch_time)\n",
    "\n",
    "        # Validation (no batch progress, cleaner output)\n",
    "        val_metrics = validate(\n",
    "            model, val_loader, device,\n",
    "            e2e=e2e, strategy=strategy,\n",
    "            use_focal=use_focal,\n",
    "            focal_alpha=focal_alpha,\n",
    "            focal_gamma=focal_gamma,\n",
    "            **loss_weights\n",
    "        )\n",
    "        \n",
    "        # Update history\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        avg_speed = len(train_loader) / epoch_time\n",
    "\n",
    "        # Update history in train_stage()\n",
    "        try:\n",
    "            # Basic metrics with safe defaults\n",
    "            history['train_loss'].append(train_metrics['loss'] / len(train_loader))\n",
    "            history['val_loss'].append(val_metrics.get('val_loss', float('inf')))\n",
    "            history['r2_z'].append(val_metrics.get('r2xyz', [0, 0, 0])[2] if 'r2xyz' in val_metrics else 0)\n",
    "            history['e3d'].append(val_metrics.get('e3d', float('inf')))\n",
    "            history['lr'].append(optim.param_groups[0]['lr'])\n",
    "            history['speed'].append(avg_speed)\n",
    "            history['epoch_times'].append(epoch_time)\n",
    "            \n",
    "            # Strategy-specific metrics\n",
    "            if strategy == 'MultiHead':\n",
    "                for head in ['coarse', 'medium', 'fine']:\n",
    "                    if head in val_metrics.get('head_metrics', {}):\n",
    "                        head_metrics = val_metrics['head_metrics'][head]\n",
    "                        history[f'{head}_loss'].append(train_metrics.get(f'{head}_loss', 0) / len(train_loader))\n",
    "                        history[f'{head}_r2z'].append(head_metrics.get('r2_z', 0))\n",
    "                        history[f'{head}_e3d'].append(head_metrics.get('e3d', float('inf')))\n",
    "            elif strategy == 'auxiliary':\n",
    "                # Initialize depth_map_error list if not exists\n",
    "                if 'depth_map_error' not in history:\n",
    "                    history['depth_map_error'] = []\n",
    "                if 'w_depth_map' not in history:\n",
    "                    history['w_depth_map'] = []\n",
    "                    \n",
    "                history['depth_map_error'].append(val_metrics.get('depth_map_error', float('inf')))\n",
    "                history['w_depth_map'].append(loss_weights.get('w_depth_map', 0))\n",
    "            elif strategy == 'curriculum':\n",
    "                # Track curriculum weight changes\n",
    "                if 'w_z_curriculum' not in history:\n",
    "                    history['w_z_curriculum'] = []\n",
    "                history['w_z_curriculum'].append(loss_weights.get('w_z', 5.0))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nWarning: Error updating history - {str(e)}\")\n",
    "            print(\"Continuing training...\")\n",
    "\n",
    "        # Update best metrics\n",
    "        if best_metrics is None or val_metrics['val_loss'] < best_metrics['val_loss']:\n",
    "            best_metrics = val_metrics.copy()\n",
    "            best_epoch = epoch\n",
    "            if 'head_metrics' in val_metrics:\n",
    "                best_head_metrics = val_metrics['head_metrics'].copy()\n",
    "\n",
    "        # Print progress\n",
    "        print_epoch_progress(epoch, max_epochs, train_metrics, val_metrics, \n",
    "                    strategy, epoch_time, best_epoch, stage, current_weights=loss_weights)\n",
    "\n",
    "        # Early stopping check - DISABLED for now\n",
    "        # if early_stopper.should_stop(val_metrics):\n",
    "        #     print(f\"  ‚Üí Early stopping at epoch {epoch} (best: epoch {best_epoch})\")\n",
    "        #     break\n",
    "\n",
    "    # Finalize training\n",
    "    try:\n",
    "        # Add training statistics\n",
    "        best_metrics.update({\n",
    "            'epoch': best_epoch,\n",
    "            'epochs_trained': epoch,\n",
    "            'training_completed': True,\n",
    "            'training_history': history\n",
    "        })\n",
    "        \n",
    "        # Add head-specific metrics for multi-head models\n",
    "        if strategy == 'MultiHead' and best_head_metrics is not None:\n",
    "            best_metrics['head_metrics'] = best_head_metrics\n",
    "            best_metrics['best_head_r2z'] = best_head_metrics['fine']['r2_z']\n",
    "            best_metrics['best_head_e3d'] = best_head_metrics['fine']['e3d']\n",
    "        \n",
    "        return best_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Error finalizing metrics: {str(e)}\")\n",
    "        return {\n",
    "            'val_loss': float('inf'),\n",
    "            'r2xyz': [0, 0, 0],\n",
    "            'e3d': float('inf'),\n",
    "            'epoch': epoch,\n",
    "            'epochs_trained': epoch,\n",
    "            'training_completed': False,\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98737315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(model_name, model_class, train_loader, val_loader, description=\"\", \n",
    "               use_focal=False, focal_alpha=0.25, focal_gamma=1.5, strategy=\"standard\",\n",
    "               **strategy_kwargs):\n",
    "    \"\"\"\n",
    "    Conduct a quick experiment with the specified model and training strategy.\n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        model_class: Class of the model to instantiate\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        description: Description of the experiment\n",
    "        use_focal: Use focal loss for depth prediction\n",
    "        focal_alpha: Focal loss alpha parameter\n",
    "        focal_gamma: Focal loss gamma parameter\n",
    "        strategy: Training strategy (standard/progressive/auxiliary/curriculum/spatial/MultiHead)\n",
    "        **strategy_kwargs: Additional strategy-specific parameters (e.g., head_weights for MultiHead)\n",
    "    Returns:\n",
    "        dict: Final evaluation metrics\n",
    "    \"\"\"\n",
    "    # Reset seed for reproducibility\n",
    "    set_seed(42)\n",
    "\n",
    "    # Setup\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else\n",
    "                         \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\\nüöÄ EXPERIMENT: {model_name}\\n{'='*60}\\nDescription: {description}\\nDevice: {device} | Expected time: ~5-15 minutes\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initialize model\n",
    "    model = model_class().to(device)\n",
    "\n",
    "    # For multi-head strategy, configure head weights\n",
    "    if strategy == \"MultiHead\":\n",
    "        head_weights = strategy_kwargs.get('head_weights', {\n",
    "            'coarse': 0.2, 'medium': 0.3, 'fine': 0.5\n",
    "        })\n",
    "        print(f\"\\nüìä Multi-Head Configuration:\")\n",
    "        print(f\"   Head Weights: coarse={head_weights['coarse']:.1f}, \"\n",
    "              f\"medium={head_weights['medium']:.1f}, \"\n",
    "              f\"fine={head_weights['fine']:.1f}\")\n",
    "        strategy_kwargs['head_weights'] = head_weights\n",
    "\n",
    "    # Stage A: Quick axis pretraining\n",
    "    print(f\"\\n{'='*60}\\nSTAGE A: Axis Pretraining ({QUICK_CONFIG['epochs']['A']} epochs)\\n{'='*60}\\n\")\n",
    "    logs_A = train_stage(model, train_loader, val_loader, 'A', \n",
    "                        QUICK_CONFIG['epochs']['A'], device,\n",
    "                        use_focal=use_focal, focal_alpha=focal_alpha, \n",
    "                        focal_gamma=focal_gamma,\n",
    "                        strategy=strategy,\n",
    "                        **strategy_kwargs)\n",
    "\n",
    "    # Stage B: Quick intersection training\n",
    "    print(f\"\\n{'='*60}\\nSTAGE B: Intersection Training ({QUICK_CONFIG['epochs']['B']} epochs)\\n{'='*60}\\n\")\n",
    "    logs_B = train_stage(model, train_loader, val_loader, 'B',\n",
    "                        QUICK_CONFIG['epochs']['B'], device,\n",
    "                        use_focal=use_focal, focal_alpha=focal_alpha, \n",
    "                        focal_gamma=focal_gamma,\n",
    "                        strategy=strategy,\n",
    "                        **strategy_kwargs)\n",
    "\n",
    "    # For multi-head, use best fine head R¬≤_z for decision\n",
    "    if strategy == \"MultiHead\" and 'head_metrics' in logs_B:\n",
    "        r2_z_B = logs_B['head_metrics']['fine']['r2_z']\n",
    "        print(f\"\\nüìä Stage B Head Performance:\")\n",
    "        for head, metrics in logs_B['head_metrics'].items():\n",
    "            print(f\"   ‚Ä¢ {head}: R¬≤_z={metrics['r2_z']:.3f}, 3D Error={metrics['e3d']:.2f}mm\")\n",
    "    else:\n",
    "        r2_z_B = logs_B['r2xyz'][2]\n",
    "    \n",
    "    # ALWAYS run full Stage C (adaptive scheduling DISABLED)\n",
    "    print(f\"\\nüü¢ Stage B R¬≤_z: {r2_z_B:.3f}. Running FULL Stage C ({QUICK_CONFIG['epochs']['C']} epochs).\")\n",
    "    stage_C_epochs = QUICK_CONFIG['epochs']['C']\n",
    "\n",
    "    # Stage C: End-to-end fine-tuning\n",
    "    if stage_C_epochs > 0:\n",
    "        print(f\"\\n{'='*60}\\nSTAGE C: End-to-End Fine-tuning ({stage_C_epochs} epochs)\\n{'='*60}\\n\")\n",
    "        logs_C = train_stage(model, train_loader, val_loader, 'C',\n",
    "                           stage_C_epochs, device,\n",
    "                           use_focal=use_focal, focal_alpha=focal_alpha, \n",
    "                           focal_gamma=focal_gamma,\n",
    "                           strategy=strategy,\n",
    "                           **strategy_kwargs)\n",
    "\n",
    "    # Final results\n",
    "    runtime = time.time() - start_time\n",
    "    \n",
    "    # Use best fine head metrics for multi-head models\n",
    "    if strategy == \"MultiHead\" and 'head_metrics' in logs_C:\n",
    "        best_head = logs_C['head_metrics']['fine']\n",
    "        r2_z_final = best_head['r2_z']\n",
    "        e3d_final = best_head['e3d']\n",
    "        e2d_final = best_head.get('e2d', 0.0)\n",
    "    else:\n",
    "        r2_z_final = logs_C['r2xyz'][2]\n",
    "        e3d_final = logs_C['e3d']\n",
    "        e2d_final = logs_C['e2d']\n",
    "\n",
    "    result = {\n",
    "        'model_name': model_name,\n",
    "        'description': description,\n",
    "        'r2_z': r2_z_final,\n",
    "        'r2_x': logs_C['r2xyz'][0],\n",
    "        'r2_y': logs_C['r2xyz'][1],\n",
    "        '3d_error_mm': e3d_final,\n",
    "        '2d_error_px': e2d_final,\n",
    "        'val_loss': logs_C['val_loss'],\n",
    "        'runtime_min': runtime / 60,\n",
    "        'stage_A_epochs': logs_A.get('epoch', QUICK_CONFIG['epochs']['A']),  \n",
    "        'stage_B_epochs': logs_B.get('epoch', QUICK_CONFIG['epochs']['B']),\n",
    "        'stage_C_epochs': logs_C.get('epoch', stage_C_epochs) if stage_C_epochs > 0 else 0,\n",
    "        'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'decision': '',\n",
    "        'color': ''\n",
    "    }\n",
    "\n",
    "    # For multi-head, include per-head metrics\n",
    "    if strategy == \"MultiHead\" and 'head_metrics' in logs_C:\n",
    "        result['head_metrics'] = logs_C['head_metrics']\n",
    "\n",
    "    # Decision\n",
    "    if r2_z_final > QUICK_CONFIG['thresholds']['stage_C_winner']:\n",
    "        result['decision'] = \"üü¢ WINNER\"\n",
    "        result['color'] = \"green\"\n",
    "    elif r2_z_final > QUICK_CONFIG['thresholds']['stage_C_decent']:\n",
    "        result['decision'] = \"üü° DECENT\"\n",
    "        result['color'] = \"yellow\"\n",
    "    else:\n",
    "        result['decision'] = \"üî¥ POOR\"\n",
    "        result['color'] = \"red\"\n",
    "\n",
    "    # Print summary with head-specific metrics for multi-head models\n",
    "    runtime_min = runtime / 60\n",
    "    print(f\"\\n{'='*60}\\n‚úÖ EXPERIMENT COMPLETE: {model_name}\\n{result['decision']} | R¬≤_z: {r2_z_final:.3f} | 3D Error: {e3d_final:.2f}mm | Time: {runtime_min:.1f}min\\n{'='*60}\\n\")\n",
    "\n",
    "    if strategy == \"MultiHead\" and 'head_metrics' in result:\n",
    "        print(\"Head Performance:\")\n",
    "        for head, metrics in result['head_metrics'].items():\n",
    "            print(f\"  ‚Ä¢ {head}: R¬≤_z={metrics['r2_z']:.3f}, 3D Error={metrics['e3d']:.2f}mm\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b224d5",
   "metadata": {},
   "source": [
    "### üéØ Training Configuration Changes\n",
    "\n",
    "**Current Settings:**\n",
    "- ‚úÖ **Early Stopping**: DISABLED - All stages run for full configured epochs\n",
    "- ‚úÖ **Adaptive Stage C**: DISABLED - Stage C always runs full 20 epochs regardless of Stage B performance\n",
    "\n",
    "**Why These Changes?**\n",
    "1. **Consistency**: All experiments get the same training budget for fair comparison\n",
    "2. **Late Improvements**: Some models improve significantly in later epochs\n",
    "3. **Reproducibility**: Fixed epoch counts make results more reproducible\n",
    "\n",
    "**Training Schedule:**\n",
    "- Stage A: 10 epochs (axis pretraining)\n",
    "- Stage B: 15 epochs (intersection training)\n",
    "- Stage C: **20 epochs** (end-to-end fine-tuning) - ALWAYS runs full 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment_tracker_header",
   "metadata": {},
   "source": [
    "## üìä Experiment Tracking & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "experiment_tracker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Experiment tracking system ready!\n"
     ]
    }
   ],
   "source": [
    "# Global experiment results tracker\n",
    "experiment_results = []\n",
    "\n",
    "# Global baseline reference - will be set after baseline experiment runs\n",
    "BASELINE_REFERENCE = {\n",
    "    'r2_z': None,\n",
    "    '3d_error_mm': None,\n",
    "    'model_name': None\n",
    "}\n",
    "\n",
    "def set_baseline_reference(result):\n",
    "    \"\"\"Set the baseline reference from baseline experiment result\"\"\"\n",
    "    BASELINE_REFERENCE['r2_z'] = result['r2_z']\n",
    "    BASELINE_REFERENCE['3d_error_mm'] = result['3d_error_mm']\n",
    "    BASELINE_REFERENCE['model_name'] = result['model_name']\n",
    "    print(f\"‚úÖ Baseline reference set: R¬≤_z={result['r2_z']:.3f}, 3D Error={result['3d_error_mm']:.2f}mm\")\n",
    "\n",
    "def log_experiment(result):\n",
    "    \"\"\"Log experiment result and save to file\"\"\"\n",
    "    experiment_results.append(result)\n",
    "    \n",
    "    # Auto-set baseline if this is the first baseline experiment\n",
    "    if result['model_name'] == 'baseline' and BASELINE_REFERENCE['r2_z'] is None:\n",
    "        set_baseline_reference(result)\n",
    "    \n",
    "    # Save to CSV for persistence\n",
    "    df = pd.DataFrame(experiment_results)\n",
    "    df.to_csv('quick_experiment_results.csv', index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Experiment logged: {result['model_name']}\")\n",
    "\n",
    "def show_leaderboard():\n",
    "    \"\"\"Display current leaderboard of experiments\"\"\"\n",
    "    if not experiment_results:\n",
    "        print(\"No experiments run yet!\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(experiment_results)\n",
    "    df_sorted = df.sort_values('r2_z', ascending=False)\n",
    "    \n",
    "    print(\"\\nüèÜ EXPERIMENT LEADERBOARD\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Rank':<4} {'Model':<20} {'R¬≤_z':<8} {'3D_err':<8} {'2D_err':<8} {'Time':<8} {'Decision':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, (_, row) in enumerate(df_sorted.iterrows(), 1):\n",
    "        print(f\"{i:<4} {row['model_name']:<20} {row['r2_z']:<8.3f} {row['3d_error_mm']:<8.1f} {row['2d_error_px']:<8.1f} {row['runtime_min']:<8.1f} {row['decision']:<12}\")\n",
    "    \n",
    "    print(\"\\nüìà BEST PERFORMERS:\")\n",
    "    top_3 = df_sorted.head(3)\n",
    "    \n",
    "    # Use actual baseline reference if available\n",
    "    baseline_ref = BASELINE_REFERENCE['r2_z'] if BASELINE_REFERENCE['r2_z'] is not None else 0.597\n",
    "    \n",
    "    for _, row in top_3.iterrows():\n",
    "        improvement = ((row['r2_z'] - baseline_ref) / baseline_ref * 100)\n",
    "        print(f\"   {row['decision']} {row['model_name']}: R¬≤_z={row['r2_z']:.3f} ({improvement:+.1f}% vs baseline)\")\n",
    "\n",
    "def compare_with_baseline(result, baseline_r2z=None, baseline_3d=None):\n",
    "    \"\"\"\n",
    "    Compare result with baseline and return improvement metrics\n",
    "    \n",
    "    Args:\n",
    "        result: Experiment result dict\n",
    "        baseline_r2z: Baseline R¬≤_z (if None, uses BASELINE_REFERENCE)\n",
    "        baseline_3d: Baseline 3D error (if None, uses BASELINE_REFERENCE)\n",
    "    \n",
    "    Returns:\n",
    "        dict with improvement metrics\n",
    "    \"\"\"\n",
    "    # Use actual baseline reference if available and not overridden\n",
    "    if baseline_r2z is None:\n",
    "        if BASELINE_REFERENCE['r2_z'] is not None:\n",
    "            baseline_r2z = BASELINE_REFERENCE['r2_z']\n",
    "            print(f\"üìç Using actual baseline reference: R¬≤_z={baseline_r2z:.3f}\")\n",
    "        else:\n",
    "            baseline_r2z = 0.597  # Fallback to documentation value\n",
    "            print(f\"‚ö†Ô∏è  No baseline reference set, using documentation value: R¬≤_z={baseline_r2z:.3f}\")\n",
    "    \n",
    "    if baseline_3d is None:\n",
    "        if BASELINE_REFERENCE['3d_error_mm'] is not None:\n",
    "            baseline_3d = BASELINE_REFERENCE['3d_error_mm']\n",
    "        else:\n",
    "            baseline_3d = 6.65  # Fallback to documentation value\n",
    "    \n",
    "    r2z_improvement = result['r2_z'] - baseline_r2z\n",
    "    r2z_improvement_pct = (r2z_improvement / baseline_r2z) * 100\n",
    "    \n",
    "    error_improvement = baseline_3d - result['3d_error_mm']\n",
    "    error_improvement_pct = (error_improvement / baseline_3d) * 100\n",
    "    \n",
    "    return {\n",
    "        'r2z_improvement': r2z_improvement,\n",
    "        'r2z_improvement_pct': r2z_improvement_pct,\n",
    "        'error_improvement_mm': error_improvement,\n",
    "        'error_improvement_pct': error_improvement_pct,\n",
    "        'is_better': r2z_improvement > QUICK_CONFIG['decision_threshold'],\n",
    "        'baseline_r2z_used': baseline_r2z,\n",
    "        'baseline_3d_used': baseline_3d\n",
    "    }\n",
    "\n",
    "print(\"üìä Experiment tracking system ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "yh0nvvdw9v",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 33 existing experiments from CSV\n",
      "\n",
      "üèÜ EXPERIMENT LEADERBOARD\n",
      "================================================================================\n",
      "Rank Model                R¬≤_z     3D_err   2D_err   Time     Decision    \n",
      "--------------------------------------------------------------------------------\n",
      "1    skip_connections + focal_loss + augmentation 0.570    7.6      26.3     24.0     üü¢ WINNER    \n",
      "2    skip_connections + focal_loss (full training) 0.557    7.1      15.2     49.9     üü¢ WINNER    \n",
      "3    skip_connections + spatial_attention 0.554    7.1      23.8     34.2     üü¢ WINNER    \n",
      "4    depth_curriculum + skip_connections 0.537    7.6      24.8     26.0     üü° DECENT    \n",
      "5    skip_connections + auxiliary_depth 0.536    7.6      24.3     31.9     üü° DECENT    \n",
      "6    skip_connections + spatial_attention (full training) 0.534    7.0      9.6      60.0     üü° DECENT    \n",
      "7    skip_connections + multihead_depth 0.528    7.6      23.5     27.0     üü° DECENT    \n",
      "8    depth_curriculum     0.517    7.8      19.5     23.6     üü° DECENT    \n",
      "9    skip_connections + multihead_depth + curriculum 0.516    8.1      26.7     25.2     COMBINED    \n",
      "10   spatial_attention + focal_loss 0.515    7.9      24.7     25.3     üü° DECENT    \n",
      "11   skip_connections + focal_loss 0.500    8.1      24.1     23.0     üü° DECENT    \n",
      "12   depth_curriculum + skip_connections + focal_loss 0.496    8.2      25.2     27.8     üü° DECENT    \n",
      "13   skip_connections     0.491    8.0      23.7     35.7     üü° DECENT    \n",
      "14   multihead_depth      0.491    7.9      21.4     24.4     üü° DECENT    \n",
      "15   auxiliary_depth      0.480    7.9      22.6     31.4     üü° DECENT    \n",
      "16   spatial_attention    0.479    8.2      23.2     37.7     üü° DECENT    \n",
      "17   depth_curriculum + augmentation 0.475    8.2      26.4     25.8     üü° DECENT    \n",
      "18   augmentation         0.473    8.4      25.7     24.1     üü° DECENT    \n",
      "19   augmentation + focal_loss 0.473    8.4      25.7     26.5     üü° DECENT    \n",
      "20   multihead_depth + focal_loss 0.472    7.9      21.3     25.6     üü° DECENT    \n",
      "21   depth_curriculum + focal_loss + augmentation 0.465    8.2      27.0     24.0     üü° DECENT    \n",
      "22   auxiliary_depth + focal_loss + augmentation 0.465    8.0      25.3     25.6     üü° DECENT    \n",
      "23   progressive_gradient + focal_loss + augmentation 0.461    8.6      26.6     24.1     üü° DECENT    \n",
      "24   auxiliary_depth + focal_loss 0.460    8.3      22.6     29.1     üü° DECENT    \n",
      "25   progressive_gradient unblocking + focal_loss 0.458    8.4      23.3     126.9    üü° DECENT    \n",
      "26   baseline             0.456    8.3      22.6     29.8     üü° DECENT    \n",
      "27   progressive_gradient 0.455    8.4      23.1     30.7     üü° DECENT    \n",
      "28   focal_loss           0.450    8.3      20.7     31.5     üü° DECENT    \n",
      "29   depth_curriculum + focal_loss 0.449    8.1      20.8     25.3     üî¥ POOR      \n",
      "30   spatial_attention + focal_loss + augmentation 0.440    8.5      21.3     25.3     üî¥ POOR      \n",
      "31   multihead_depth + focal_loss + augmentation 0.430    8.8      24.2     24.1     üî¥ POOR      \n",
      "32   multihead_depth + augmentation 0.422    8.8      24.2     28.0     üî¥ POOR      \n",
      "33   auxiliary_depth + augmentation 0.417    8.7      25.6     29.3     üî¥ POOR      \n",
      "\n",
      "üìà BEST PERFORMERS:\n",
      "   üü¢ WINNER skip_connections + focal_loss + augmentation: R¬≤_z=0.570 (-4.5% vs baseline)\n",
      "   üü¢ WINNER skip_connections + focal_loss (full training): R¬≤_z=0.557 (-6.7% vs baseline)\n",
      "   üü¢ WINNER skip_connections + spatial_attention: R¬≤_z=0.554 (-7.2% vs baseline)\n"
     ]
    }
   ],
   "source": [
    "# Load existing experiment results from CSV (if they exist)\n",
    "import os\n",
    "if os.path.exists('quick_experiment_results.csv'):\n",
    "    df_existing = pd.read_csv('quick_experiment_results.csv')\n",
    "    # Filter out NaN entries\n",
    "    df_existing = df_existing[df_existing['r2_z'].notna()]\n",
    "    experiment_results = df_existing.to_dict('records')\n",
    "    print(f\"‚úÖ Loaded {len(experiment_results)} existing experiments from CSV\")\n",
    "else:\n",
    "    experiment_results = []\n",
    "    print(\"üìä Starting fresh - no existing results found\")\n",
    "\n",
    "# Display current leaderboard\n",
    "show_leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline_test_header",
   "metadata": {},
   "source": [
    "## üéØ Test Framework with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the framework with baseline model first\n",
    "# print(\"üî• Testing Quick Experiment Framework\")\n",
    "# print(\"Running baseline experiment to validate framework...\")\n",
    "\n",
    "# baseline_result = experiment(\n",
    "#     model_name=\"baseline\",\n",
    "#     model_class=StereoTwoStageNet,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     description=\"Original model, no modifications - framework validation\"\n",
    "# )\n",
    "\n",
    "# log_experiment(baseline_result)\n",
    "# show_leaderboard()\n",
    "\n",
    "# print(f\"\\n‚úÖ Framework validation complete!\")\n",
    "# print(f\"Baseline quick result: R¬≤_z={baseline_result['r2_z']:.3f} in {baseline_result['runtime_min']:.1f} minutes\")\n",
    "# print(f\"Expected full baseline: R¬≤_z‚âà0.597 (from documentation)\")\n",
    "# print(f\"Quick vs Full difference: ~{abs(baseline_result['r2_z'] - 0.597):.3f} R¬≤_z units\")\n",
    "# print(f\"\\nüéØ Framework is ready for testing the 15 improvement approaches!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4ede3",
   "metadata": {},
   "source": [
    "## üß™ Experiment 1: Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8quaw7xbtrn",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLossNet(StereoTwoStageNet):\n",
    "    \"\"\"\n",
    "    Wrapper around StereoTwoStageNet for focal loss experiments.\n",
    "    \n",
    "    Architecture: Identical to baseline StereoTwoStageNet\n",
    "    Difference: Uses focal loss weighting during training (applied in loss function)\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone_name=\"resnet18\", pretrained=True):\n",
    "        super().__init__(backbone_name, pretrained)\n",
    "        \n",
    "print(\"‚úÖ FocalLossNet model defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v9lhm6wbftf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run focal loss experiment\n",
    "focal_result = experiment(\n",
    "    model_name=\"focal_loss w/ augmentation\",\n",
    "    model_class=FocalLossNet,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    description=\"Focal Loss for depth prediction to handle hard samples\",\n",
    "    use_focal=True,\n",
    "    focal_alpha=0.25,\n",
    "    focal_gamma=1.5\n",
    ")\n",
    "\n",
    "log_experiment(focal_result)\n",
    "show_leaderboard()\n",
    "\n",
    "# Compare with baseline\n",
    "comparison = compare_with_baseline(focal_result)  # Use quick baseline\n",
    "print(f\"\\nüìä FOCAL LOSS ANALYSIS:\")\n",
    "print(f\"R¬≤_z change: {comparison['r2z_improvement']:+.3f} ({comparison['r2z_improvement_pct']:+.1f}%)\")\n",
    "print(f\"3D error change: {comparison['error_improvement_mm']:+.2f}mm ({comparison['error_improvement_pct']:+.1f}%)\")\n",
    "print(f\"Better than baseline? {'‚úÖ YES' if comparison['is_better'] else '‚ùå NO'}\")\n",
    "\n",
    "# Decision on next approach\n",
    "if comparison['r2z_improvement'] > 0.05:  # >5% improvement\n",
    "    print(\"üü¢ EXCELLENT! Focal loss significantly improved depth prediction.\")\n",
    "    print(\"   ‚Üí Next: Try Spatial Attention to further enhance feature representation\")\n",
    "elif comparison['r2z_improvement'] > 0.02:  # 2-5% improvement\n",
    "    print(\"üü° GOOD! Focal loss shows promise.\")\n",
    "    print(\"   ‚Üí Next: Combine with Spatial Attention or try Multi-scale Depth Heads\")\n",
    "else:\n",
    "    print(\"üî¥ MINIMAL IMPROVEMENT. Loss function change not sufficient.\")\n",
    "    print(\"   ‚Üí Next: Try architectural changes like Spatial Attention or Stronger Backbone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6agc5n43mzi",
   "metadata": {},
   "source": [
    "## üß™ Experiment 2: Skip Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ie2f89fjhgq",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnectionNet(StereoTwoStageNet):\n",
    "    \"\"\"\n",
    "    Enhanced fusion with residual pathway for better gradient flow.\n",
    "    \n",
    "    Architecture:\n",
    "    - Main path: Concatenated features ‚Üí Conv 1280‚Üí512‚Üí256 ‚Üí Pool ‚Üí (B, 256)\n",
    "    - Skip path: Concatenated features ‚Üí Pool ‚Üí Linear 1280‚Üí256 ‚Üí (B, 256)\n",
    "    - Output: main + skip (residual addition)\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone_name=\"resnet18\", pretrained=True):\n",
    "        super().__init__(backbone_name, pretrained)\n",
    "        \n",
    "        # Enhanced fusion with increased capacity\n",
    "        fused_channels = 128 * len(self.feature_dims) * 2\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(fused_channels, 512, 1),  # Expanded capacity\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        \n",
    "        # Skip connection path - processes raw concatenated features\n",
    "        # This provides alternative gradient path and preserves feature information\n",
    "        self.skip_fusion = nn.Linear(fused_channels, 256)\n",
    "        \n",
    "    def _fused_vec(self, left_img, right_img):\n",
    "        \"\"\"Extract fused features with residual connection\"\"\"\n",
    "        feats_l = self.backbone(left_img)\n",
    "        feats_r = self.backbone(right_img)\n",
    "        H, W = feats_l[-1].shape[2:]\n",
    "        \n",
    "        # Project and fuse features at multiple scales (using inherited self.proj)\n",
    "        fused_scales = []\n",
    "        for fl, fr, proj in zip(feats_l, feats_r, self.proj):\n",
    "            # Project to consistent channels\n",
    "            fl_proj = proj(fl)\n",
    "            fr_proj = proj(fr)\n",
    "            # Resize to common spatial size\n",
    "            fl_proj = F.adaptive_avg_pool2d(fl_proj, (H, W))\n",
    "            fr_proj = F.adaptive_avg_pool2d(fr_proj, (H, W))\n",
    "            # Concatenate left-right stereo features\n",
    "            fused_scales.append(torch.cat([fl_proj, fr_proj], dim=1))\n",
    "\n",
    "        # Concatenate all scales: (B, 1280, H, W) for ResNet18\n",
    "        x = torch.cat(fused_scales, dim=1)\n",
    "        \n",
    "        # Main fusion path: Convolutional transformation\n",
    "        fused_4d = self.fusion(x)\n",
    "        fused_vec = fused_4d.view(fused_4d.size(0), -1)  # (B, 256)\n",
    "        \n",
    "        # Skip connection path: Direct linear transformation\n",
    "        # Pool the raw concatenated features and project to same dimension\n",
    "        skip_pooled = F.adaptive_avg_pool2d(x, 1).view(x.size(0), -1)  # (B, 1280)\n",
    "        skip_vec = self.skip_fusion(skip_pooled)  # (B, 256)\n",
    "        \n",
    "        # Residual addition: Combine both pathways\n",
    "        # This helps gradient flow and provides ensemble-like effect\n",
    "        combined_vec = fused_vec + skip_vec\n",
    "        \n",
    "        return combined_vec, fused_4d\n",
    "\n",
    "print(\"‚úÖ SkipConnectionNet model defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eitkhvjyr4l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run skip connections experiment\n",
    "skip_result = experiment(\n",
    "    model_name=\"skip_connections, focal_loss, augmentation\",\n",
    "    model_class=SkipConnectionNet,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    description=\"Skip connections around fusion for better gradient flow\",\n",
    "    use_focal=True,\n",
    "    focal_alpha=0.25,\n",
    "    focal_gamma=1.5\n",
    ")\n",
    "\n",
    "log_experiment(skip_result)\n",
    "show_leaderboard()\n",
    "\n",
    "# Compare with baseline\n",
    "comparison = compare_with_baseline(skip_result)\n",
    "print(f\"\\nüìä SKIP CONNECTIONS ANALYSIS:\")\n",
    "print(f\"R¬≤_z improvement: {comparison['r2z_improvement']:+.3f} ({comparison['r2z_improvement_pct']:+.1f}%)\")\n",
    "print(f\"3D error change: {comparison['error_improvement_mm']:+.2f}mm ({comparison['error_improvement_pct']:+.1f}%)\")\n",
    "print(f\"Is better than baseline? {'‚úÖ YES' if comparison['is_better'] else '‚ùå NO'}\")\n",
    "\n",
    "if comparison['is_better']:\n",
    "    print(\"üü¢ PROMISING! Consider full training run.\")\n",
    "else:\n",
    "    print(\"üî¥ SKIP - Moving to next approach.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60pydoubtcm",
   "metadata": {},
   "source": [
    "## üß™ Experiment 3: Spatial Attention for Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88du2jul67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SpatialAttentionDepthNet model defined!\n"
     ]
    }
   ],
   "source": [
    "class SpatialAttentionModule(nn.Module):\n",
    "    \"\"\"Spatial attention to weight important regions for depth prediction\"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        # Channel reduction for attention map\n",
    "        self.attention_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // 4, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels // 4, 1, 1),\n",
    "            nn.Sigmoid()  # Attention weights [0, 1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        attn_map = self.attention_conv(x)  # (B, 1, H, W)\n",
    "        attended = x * attn_map  # Element-wise multiplication\n",
    "        return attended, attn_map\n",
    "\n",
    "\n",
    "class SpatialAttentionDepthNet(StereoTwoStageNet):\n",
    "    \"\"\"\n",
    "    Enhanced model with spatial attention for depth prediction\n",
    "    \n",
    "    Key improvements:\n",
    "    1. Applies learned attention to weight depth-relevant spatial regions\n",
    "    2. Dual-path processing: attended global context + attended spatial features\n",
    "    3. Returns attention maps for visualization and analysis\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone_name=\"resnet18\", pretrained=True):\n",
    "        super().__init__(backbone_name, pretrained)\n",
    "        \n",
    "        # Calculate fused channels\n",
    "        fused_channels = 128 * len(self.feature_dims) * 2\n",
    "        \n",
    "        # Spatial attention module\n",
    "        self.spatial_attention = SpatialAttentionModule(fused_channels)\n",
    "        \n",
    "        # Enhanced fusion with spatial pathway\n",
    "        self.spatial_fusion = nn.Sequential(\n",
    "            nn.Conv2d(fused_channels, 512, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Keep original global fusion for axis prediction\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(fused_channels, 256, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        \n",
    "        # Enhanced depth head with spatial features\n",
    "        # Combines: global features (256) + spatial features (256) + axis (4) = 516\n",
    "        self.offset_depth_head = nn.Sequential(\n",
    "            nn.Linear(256 + 256 + 4, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),  # Regularization for larger head\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 2)  # (offset_t, depth_z_raw)\n",
    "        )\n",
    "        \n",
    "    def _fused_vec(self, left_img, right_img):\n",
    "        \"\"\"Extract fused features with spatial attention\n",
    "        \n",
    "        Returns:\n",
    "            fused_vec: Concatenated global + spatial features (B, 512)\n",
    "            global_4d: Global features for axis head (B, 256, 1, 1)\n",
    "            attn_map: Attention map for visualization (B, 1, H, W)\n",
    "        \"\"\"\n",
    "        feats_l = self.backbone(left_img)\n",
    "        feats_r = self.backbone(right_img)\n",
    "        H, W = feats_l[-1].shape[2:]\n",
    "        \n",
    "        # Multi-scale fusion (same as baseline)\n",
    "        fused_scales = []\n",
    "        for fl, fr, proj in zip(feats_l, feats_r, self.proj):\n",
    "            fl = proj(fl)\n",
    "            fr = proj(fr)\n",
    "            fl = F.adaptive_avg_pool2d(fl, (H, W))\n",
    "            fr = F.adaptive_avg_pool2d(fr, (H, W))\n",
    "            fused_scales.append(torch.cat([fl, fr], dim=1))\n",
    "\n",
    "        x = torch.cat(fused_scales, dim=1)  # (B, 1280, H, W)\n",
    "        \n",
    "        # Apply spatial attention - FIX #2: Keep attention map for visualization\n",
    "        attended_x, attn_map = self.spatial_attention(x)\n",
    "        \n",
    "        # Spatial pathway: preserve spatial features for depth\n",
    "        spatial_features = self.spatial_fusion(attended_x)  # (B, 256, H, W)\n",
    "        spatial_vec = F.adaptive_avg_pool2d(spatial_features, 1).view(spatial_features.size(0), -1)  # (B, 256)\n",
    "        \n",
    "        # Global pathway: for axis prediction - FIX #1: Use attended features\n",
    "        global_4d = self.fusion(attended_x)  # FIXED: was self.fusion(x)\n",
    "        global_vec = global_4d.view(global_4d.size(0), -1)\n",
    "        \n",
    "        fused_vec = torch.cat([global_vec, spatial_vec], dim=1)\n",
    "        return fused_vec, global_4d, attn_map\n",
    "    \n",
    "    def forward(self, left_img, right_img):\n",
    "        fused_vec, fused_4d, attn_map = self._fused_vec(left_img, right_img)\n",
    "        \n",
    "        # Stage 1: Axis prediction (uses attended global features)\n",
    "        axis_params = self.axis_head(fused_4d)\n",
    "        origin = axis_params[:, :2]\n",
    "        direction = F.normalize(axis_params[:, 2:], dim=1)\n",
    "        \n",
    "        # Stage 2: Depth prediction (uses global + spatial + axis)\n",
    "        conditioned = torch.cat([fused_vec, origin.detach(), direction.detach()], dim=1)\n",
    "        \n",
    "        od = self.offset_depth_head(conditioned)\n",
    "        offset_t = od[:, 0:1]\n",
    "        depth_z = self.softplus(od[:, 1:2])\n",
    "        \n",
    "        xy_inter = origin + offset_t * direction\n",
    "        intersection = torch.cat([xy_inter, depth_z], dim=1)\n",
    "        \n",
    "        return {\n",
    "            \"origin\": origin,\n",
    "            \"direction\": direction,\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": intersection,\n",
    "            \"attn_map\": attn_map  # Return attention map for analysis\n",
    "        }\n",
    "\n",
    "    def forward_e2e(self, left_img, right_img):\n",
    "        \"\"\"End-to-end forward pass (no gradient detachment)\"\"\"\n",
    "        fused_vec, fused_4d, attn_map = self._fused_vec(left_img, right_img)\n",
    "        \n",
    "        axis_params = self.axis_head(fused_4d)\n",
    "        origin = axis_params[:, :2]\n",
    "        direction = F.normalize(axis_params[:, 2:], dim=1)\n",
    "        \n",
    "        conditioned = torch.cat([fused_vec, origin, direction], dim=1)\n",
    "        \n",
    "        od = self.offset_depth_head(conditioned)\n",
    "        offset_t = od[:, 0:1]\n",
    "        depth_z = self.softplus(od[:, 1:2])\n",
    "        \n",
    "        xy_inter = origin + offset_t * direction\n",
    "        intersection = torch.cat([xy_inter, depth_z], dim=1)\n",
    "        \n",
    "        return {\n",
    "            \"origin\": origin,\n",
    "            \"direction\": direction,\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": intersection,\n",
    "            \"attn_map\": attn_map  # Return attention map for analysis\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ SpatialAttentionDepthNet model defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gkfytfq0r5i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run spatial attention experiment\n",
    "spatial_result = experiment(\n",
    "    model_name=\"spatial_attention, focal_loss, augmentation\",\n",
    "    model_class=SpatialAttentionDepthNet,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    description=\"Spatial attention to preserve depth-relevant spatial features\",\n",
    "    strategy=\"spatial\"\n",
    ")\n",
    "\n",
    "log_experiment(spatial_result)\n",
    "show_leaderboard()\n",
    "\n",
    "# Compare with baseline\n",
    "comparison = compare_with_baseline(spatial_result)\n",
    "print(f\"\\nüìä SPATIAL ATTENTION ANALYSIS:\")\n",
    "print(f\"R¬≤_z improvement: {comparison['r2z_improvement']:+.3f} ({comparison['r2z_improvement_pct']:+.1f}%)\")\n",
    "print(f\"3D error change: {comparison['error_improvement_mm']:+.2f}mm ({comparison['error_improvement_pct']:+.1f}%)\")\n",
    "print(f\"Is better than baseline? {'‚úÖ YES' if comparison['is_better'] else '‚ùå NO'}\")\n",
    "\n",
    "if comparison['r2z_improvement'] > 0.05:\n",
    "    print(\"\\nüü¢ EXCELLENT! Spatial attention significantly improved depth prediction.\")\n",
    "    print(\"   ‚Üí This validates that preserving spatial features is critical for depth!\")\n",
    "    print(\"   ‚Üí Next: Try combining with multi-head depth predictor or remove gradient detachment\")\n",
    "elif comparison['r2z_improvement'] > 0.02:\n",
    "    print(\"\\nüü° GOOD! Spatial attention shows promise.\")\n",
    "    print(\"   ‚Üí Consider full training run to see true potential\")\n",
    "else:\n",
    "    print(\"\\nüî¥ NO IMPROVEMENT. Try alternative approaches.\")\n",
    "    print(\"   ‚Üí Next: Multi-head depth predictor or stronger backbone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v48saqdgtoa",
   "metadata": {},
   "source": [
    "## üß™ Experiment 4: Progressive Gradient Unblocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xjj6s75e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressiveGradientNet(StereoTwoStageNet):\n",
    "    \"\"\"\n",
    "    Progressive Gradient Unblocking\n",
    "    \n",
    "    Gradually removes .detach() during Stage C to allow end-to-end learning\n",
    "    while maintaining training stability.\n",
    "    \n",
    "    Key concept: Instead of abruptly switching from detached (Stage B) to\n",
    "    end-to-end (Stage C), smoothly transition using alpha schedule:\n",
    "    - Epoch 1: alpha=0.8 ‚Üí 80% detached, 20% gradients\n",
    "    - Epoch 5: alpha=0.0 ‚Üí 0% detached, 100% gradients (full e2e)\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone_name=\"resnet18\", pretrained=True):\n",
    "        super().__init__(backbone_name, pretrained)\n",
    "        # Register buffer for detach_alpha (not a trainable parameter)\n",
    "        self.register_buffer('detach_alpha', torch.tensor(1.0))\n",
    "        \n",
    "    def set_detach_alpha(self, alpha):\n",
    "        \"\"\"Set the detachment strength (1.0 = full detach, 0.0 = no detach)\"\"\"\n",
    "        self.detach_alpha = torch.tensor(alpha)\n",
    "        \n",
    "    def forward(self, left_img, right_img):\n",
    "        fused_vec, fused_4d = self._fused_vec(left_img, right_img)\n",
    "        axis_params = self.axis_head(fused_4d)\n",
    "        origin = axis_params[:, :2]\n",
    "        direction = F.normalize(axis_params[:, 2:], dim=1)\n",
    "        \n",
    "        # Progressive detachment: alpha controls gradient flow\n",
    "        # alpha=1.0: fully detached (Stages A/B)\n",
    "        # alpha=0.0: no detachment (end of Stage C)\n",
    "        alpha = self.detach_alpha.item()\n",
    "        origin_input = alpha * origin.detach() + (1 - alpha) * origin\n",
    "        direction_input = alpha * direction.detach() + (1 - alpha) * direction\n",
    "        \n",
    "        conditioned = torch.cat([fused_vec, origin_input, direction_input], dim=1)\n",
    "        od = self.offset_depth_head(conditioned)\n",
    "        offset_t = od[:, 0:1]\n",
    "        depth_z = self.softplus(od[:, 1:2])\n",
    "        \n",
    "        xy_inter = origin + offset_t * direction\n",
    "        intersection = torch.cat([xy_inter, depth_z], dim=1)\n",
    "        \n",
    "        return {\n",
    "            \"origin\": origin,\n",
    "            \"direction\": direction,\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": intersection\n",
    "        }\n",
    "\n",
    "    def forward_e2e(self, left_img, right_img):\n",
    "        \"\"\"Same as forward() - progressive logic handles both cases\"\"\"\n",
    "        return self.forward(left_img, right_img)\n",
    "\n",
    "print(\"‚úÖ ProgressiveGradientNet model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qp0qze9g39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run progressive gradient unblocking experiment\n",
    "progressive_result = experiment(\n",
    "    model_name=\"progressive_gradient, focal_loss, augmentation\",\n",
    "    model_class=ProgressiveGradientNet,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    description=\"Progressive gradient unblocking: alpha 1.0‚Üí0.0 during Stage C\",\n",
    "    strategy=\"progressive\",\n",
    "    use_focal=True,\n",
    "    focal_alpha=0.25,\n",
    "    focal_gamma=1.5\n",
    ")\n",
    "\n",
    "log_experiment(progressive_result)\n",
    "show_leaderboard()\n",
    "\n",
    "# Compare with baseline\n",
    "comparison = compare_with_baseline(progressive_result)\n",
    "print(f\"\\nüìä PROGRESSIVE GRADIENT ANALYSIS:\")\n",
    "print(f\"R¬≤_z improvement: {comparison['r2z_improvement']:+.3f} ({comparison['r2z_improvement_pct']:+.1f}%)\")\n",
    "print(f\"3D error change: {comparison['error_improvement_mm']:+.2f}mm ({comparison['error_improvement_pct']:+.1f}%)\")\n",
    "print(f\"Is better than baseline? {'‚úÖ YES' if comparison['is_better'] else '‚ùå NO'}\")\n",
    "\n",
    "if comparison['r2z_improvement'] > 0.05:\n",
    "    print(\"\\nüü¢ EXCELLENT! Progressive gradient unblocking significantly improved depth.\")\n",
    "    print(\"   ‚Üí End-to-end learning is working!\")\n",
    "    print(\"   ‚Üí Next: Try combining with stronger backbone or depth curriculum\")\n",
    "elif comparison['r2z_improvement'] > 0.02:\n",
    "    print(\"\\nüü° GOOD! Progressive unblocking shows promise.\")\n",
    "    print(\"   ‚Üí Consider full training run to see true potential\")\n",
    "else:\n",
    "    print(\"\\nüî¥ MINIMAL IMPROVEMENT.\")\n",
    "    print(\"   ‚Üí Gradient flow may not be the main bottleneck\")\n",
    "    print(\"   ‚Üí Next: Try depth loss curriculum or auxiliary depth supervision\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auxiliary_depth_header",
   "metadata": {},
   "source": [
    "## üß™ Experiment 5: Auxiliary Depth Supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "auxiliary_depth_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AuxiliaryDepthNet model defined!\n"
     ]
    }
   ],
   "source": [
    "class AuxiliaryDepthNet(StereoTwoStageNet):\n",
    "    \"\"\"\n",
    "    Auxiliary Depth Supervision\n",
    "\n",
    "    Adds a depth map prediction head for direct depth supervision\n",
    "    using the existing depth_labels/*.npy files.\n",
    "\n",
    "    Expected impact: +10-15% R¬≤_z improvement\n",
    "    Risk: Low\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone_name=\"resnet18\", pretrained=True, seed=42):\n",
    "        super().__init__(backbone_name, pretrained, seed)\n",
    "\n",
    "        # Store fused features before pooling for depth map prediction\n",
    "        # The fused features have shape (B, 256, H, W) before pooling\n",
    "\n",
    "        # Depth map head: upsamples from pooled features to produce depth map\n",
    "        # We'll use the 4D fused features before pooling\n",
    "        self.depth_map_head = nn.Sequential(\n",
    "            # Start from 256 channels at downsampled resolution\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 2x upsample\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 4x upsample\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),    # 8x upsample\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 1, kernel_size=3, padding=1),                        # Final depth map\n",
    "            nn.Softplus(beta=1.0)  # Ensure positive depth values\n",
    "        )\n",
    "\n",
    "    def _fused_vec_with_4d(self, left_img, right_img):\n",
    "        \"\"\"Modified version that returns both vector and 4D features before pooling\"\"\"\n",
    "        feats_l = self.backbone(left_img)\n",
    "        feats_r = self.backbone(right_img)\n",
    "        H, W = feats_l[-1].shape[2:]\n",
    "\n",
    "        fused_scales = []\n",
    "        for fl, fr, proj in zip(feats_l, feats_r, self.proj):\n",
    "            fl = proj(fl)\n",
    "            fr = proj(fr)\n",
    "            fl = F.adaptive_avg_pool2d(fl, (H, W))\n",
    "            fr = F.adaptive_avg_pool2d(fr, (H, W))\n",
    "            fused_scales.append(torch.cat([fl, fr], dim=1))\n",
    "\n",
    "        fused_4d = torch.cat(fused_scales, dim=1)  # (B, fused_channels, H, W)\n",
    "\n",
    "        # Apply 1x1 conv before pooling\n",
    "        fused_4d_reduced = self.fusion[0](fused_4d)  # (B, 256, H, W)\n",
    "        fused_4d_reduced = self.fusion[1](fused_4d_reduced)  # ReLU\n",
    "\n",
    "        # Pool to get vector\n",
    "        pooled = self.fusion[2](fused_4d_reduced)  # AdaptiveAvgPool2d\n",
    "        v = pooled.view(pooled.size(0), -1)\n",
    "\n",
    "        return v, pooled, fused_4d_reduced\n",
    "\n",
    "    def forward(self, left_img, right_img):\n",
    "        # Get both vector and 4D features\n",
    "        fused_vec, fused_pooled, fused_4d = self._fused_vec_with_4d(left_img, right_img)\n",
    "\n",
    "        # Predict axis\n",
    "        axis_params = self.axis_head(fused_pooled)\n",
    "        origin = axis_params[:, :2]\n",
    "        direction = F.normalize(axis_params[:, 2:], dim=1)\n",
    "\n",
    "        # Predict intersection (offset + depth)\n",
    "        conditioned = torch.cat([fused_vec, origin.detach(), direction.detach()], dim=1)\n",
    "        od = self.offset_depth_head(conditioned)\n",
    "        offset_t = od[:, 0:1]\n",
    "        depth_z = self.softplus(od[:, 1:2])\n",
    "\n",
    "        xy_inter = origin + offset_t * direction\n",
    "        intersection = torch.cat([xy_inter, depth_z], dim=1)\n",
    "\n",
    "        # Predict depth map from 4D features\n",
    "        depth_map = self.depth_map_head(fused_4d)  # (B, 1, H_out, W_out)\n",
    "\n",
    "        return {\n",
    "            \"origin\": origin,\n",
    "            \"direction\": direction,\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": intersection,\n",
    "            \"depth_map\": depth_map  # Add depth map to outputs\n",
    "        }\n",
    "\n",
    "    def forward_e2e(self, left_img, right_img):\n",
    "        # Same as forward but without detach\n",
    "        fused_vec, fused_pooled, fused_4d = self._fused_vec_with_4d(left_img, right_img)\n",
    "\n",
    "        axis_params = self.axis_head(fused_pooled)\n",
    "        origin = axis_params[:, :2]\n",
    "        direction = F.normalize(axis_params[:, 2:], dim=1)\n",
    "\n",
    "        conditioned = torch.cat([fused_vec, origin, direction], dim=1)  # no detach\n",
    "        od = self.offset_depth_head(conditioned)\n",
    "        offset_t = od[:, 0:1]\n",
    "        depth_z = self.softplus(od[:, 1:2])\n",
    "\n",
    "        xy_inter = origin + offset_t * direction\n",
    "        intersection = torch.cat([xy_inter, depth_z], dim=1)\n",
    "\n",
    "        depth_map = self.depth_map_head(fused_4d)\n",
    "\n",
    "        return {\n",
    "            \"origin\": origin,\n",
    "            \"direction\": direction,\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": intersection,\n",
    "            \"depth_map\": depth_map\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ AuxiliaryDepthNet model defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aux_helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_depth_map_for_batch(depth_map, target_size=(56, 56)):\n",
    "    \"\"\"Process and resize depth map from batch to target size\n",
    "\n",
    "    Args:\n",
    "        depth_map (np.ndarray): The depth map from dataset batch\n",
    "        target_size (tuple): Target size for resizing, default (56, 56)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Processed depth map of shape (1, H, W)\n",
    "    \"\"\"\n",
    "    # Convert to tensor if needed\n",
    "    if not isinstance(depth_map, torch.Tensor):\n",
    "        depth_map = torch.from_numpy(depth_map).float()\n",
    "    \n",
    "    # Add batch and channel dimensions if needed\n",
    "    if depth_map.dim() == 2:  # (H, W)\n",
    "        depth_map = depth_map.unsqueeze(0).unsqueeze(0)  # (1, 1, H, W)\n",
    "    elif depth_map.dim() == 3:  # (1, H, W)\n",
    "        depth_map = depth_map.unsqueeze(0)  # (1, 1, H, W)\n",
    "    \n",
    "    # Resize to match model output size (56x56 for 224x224 input with 8x upsampling)\n",
    "    depth_resized = F.interpolate(depth_map, size=target_size, mode='bilinear', align_corners=False)\n",
    "    \n",
    "    return depth_resized.squeeze(0)  # (1, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fde6922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auxiliary_depth_loss(outputs, batch, **kwargs):\n",
    "    \"\"\"\n",
    "    Compute loss with auxiliary depth map supervision\n",
    "    \"\"\"\n",
    "    # Extract auxiliary-specific parameters\n",
    "    w_depth_map = kwargs.get('w_depth_map', 0.5)\n",
    "    use_log_depth = kwargs.get('use_log_depth', True)\n",
    "    eps = kwargs.get('eps', 1e-6)\n",
    "\n",
    "    # Get main intersection loss\n",
    "    main_loss = stereo_two_stage_loss(outputs, batch, **kwargs)\n",
    "    \n",
    "    # Skip depth map loss if weight is 0\n",
    "    if w_depth_map == 0:\n",
    "        return main_loss\n",
    "        \n",
    "    # Compute depth map loss if we have the predictions and targets\n",
    "    depth_map_loss = torch.tensor(0.0, device=main_loss.device)\n",
    "    if 'depth_map' in outputs and 'gt_depth_map' in batch:\n",
    "        pred_depth_map = outputs['depth_map']\n",
    "        gt_depth_map = batch['gt_depth_map']\n",
    "        \n",
    "        # Only compute loss on valid depths (> 0.1)\n",
    "        valid_mask = gt_depth_map > 0.1\n",
    "        if valid_mask.sum() > 10:\n",
    "            if use_log_depth:\n",
    "                pred_log = torch.log(pred_depth_map[valid_mask] + eps)\n",
    "                gt_log = torch.log(gt_depth_map[valid_mask] + eps)\n",
    "                depth_map_loss = F.l1_loss(pred_log, gt_log)\n",
    "            else:\n",
    "                depth_map_loss = F.l1_loss(pred_depth_map[valid_mask], gt_depth_map[valid_mask])\n",
    "    \n",
    "    # Combine losses\n",
    "    total_loss = main_loss + w_depth_map * depth_map_loss\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a3b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run auxiliary depth experiment\n",
    "result = experiment(\n",
    "    model_name=\"auxiliary_depth, augmentation\",\n",
    "    model_class=AuxiliaryDepthNet,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    description=\"Auxiliary depth map supervision using depth_labels/*.npy\",\n",
    "    strategy=\"auxiliary\",\n",
    "    w_depth_map={  # Strategy-specific weight scheduling\n",
    "        'A': 0.0,    # No depth supervision in axis stage\n",
    "        'B': 0.5,    # Moderate depth supervision in intersection stage\n",
    "        'C': 1.0     # Full depth supervision in end-to-end stage\n",
    "    }\n",
    ")\n",
    "\n",
    "log_experiment(result)\n",
    "show_leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curriculum_header",
   "metadata": {},
   "source": [
    "## üß™ Experiment 7: Depth Loss Curriculum\n",
    "\n",
    "**‚ö†Ô∏è Important Note on Loss Interpretation:**\n",
    "- When using curriculum learning, the **loss value changes meaning across epochs** because w_z increases\n",
    "- **DO NOT compare loss values** between epochs - they are NOT comparable!\n",
    "- **Instead, focus on R¬≤_z and 3D error** which remain directly comparable\n",
    "- Loss may increase even as performance improves due to changing weight schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curriculum_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth Loss Curriculum\n",
    "# Gradually increase depth weight during training for better convergence\n",
    "\n",
    "def get_curriculum_weights(stage, epoch, total_epochs):\n",
    "    \"\"\"\n",
    "    Progressive depth weight scheduling\n",
    "    \n",
    "    FIXES APPLIED:\n",
    "    - Bug #1 FIXED: Now reaches target values by using (total_epochs - 1) in denominator\n",
    "    \n",
    "    Args:\n",
    "        stage: Training stage ('A', 'B', or 'C')\n",
    "        epoch: Current epoch (0-indexed)\n",
    "        total_epochs: Total number of epochs\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'w_z' key containing the curriculum weight\n",
    "    \"\"\"\n",
    "    if stage == 'A':\n",
    "        # Stage A: Start with baseline, gradually increase (5.0 ‚Üí 7.0)\n",
    "        progress = epoch / (total_epochs - 1) if total_epochs > 1 else 1.0\n",
    "        w_z = 5.0 + progress * 2.0  # 5.0 ‚Üí 7.0\n",
    "        return {'w_z': w_z}\n",
    "    elif stage == 'B':\n",
    "        # Stage B: Gradually increase depth importance (7.0 ‚Üí 10.0)\n",
    "        progress = epoch / (total_epochs - 1) if total_epochs > 1 else 1.0\n",
    "        w_z = 7.0 + progress * 3.0  # 7.0 ‚Üí 10.0\n",
    "        return {'w_z': w_z}\n",
    "    else:\n",
    "        # Stage C: High depth emphasis, further increase (10.0 ‚Üí 15.0)\n",
    "        progress = epoch / (total_epochs - 1) if total_epochs > 1 else 1.0\n",
    "        w_z = 10.0 + progress * 2.0  # 10.0 ‚Üí 15.0\n",
    "        return {'w_z': w_z}\n",
    "\n",
    "print(\"‚úÖ Depth loss curriculum functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_curriculum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Depth Loss Curriculum experiment\n",
    "# The ONLY untested method from original Top 5!\n",
    "\n",
    "result = experiment(\n",
    "    model_name=\"depth_curriculum + skip_connections + focal_loss\",\n",
    "    model_class=SkipConnectionNet,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    description=\"Depth loss curriculum: w_z gradually increases (1‚Üí10‚Üí15)\",\n",
    "    strategy=\"curriculum\",\n",
    "    use_focal=True,\n",
    "    focal_alpha=0.25,\n",
    "    focal_gamma=1.5\n",
    ")\n",
    "\n",
    "log_experiment(result)\n",
    "show_leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0920beb",
   "metadata": {},
   "source": [
    "### üìä Curriculum Learning Metrics Explanation\n",
    "\n",
    "**Understanding the Output:**\n",
    "\n",
    "For curriculum experiments, you'll see output like:\n",
    "```\n",
    "Epoch 1/15 | Loss: 0.3173 (w_z=1.0) | R¬≤_z: 0.048 | 3D: 12.88mm\n",
    "Epoch 4/15 | Loss: 0.3523 (w_z=2.9) | R¬≤_z: 0.249 | 3D: 11.24mm\n",
    "```\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "1. **Loss Increases ‚â† Performance Degrades**\n",
    "   - Loss = weighted sum of errors\n",
    "   - As w_z grows from 1.0 ‚Üí 10.0, same depth error contributes more to total loss\n",
    "   - Example: depth_error = 0.1 ‚Üí Loss contribution = w_z * 0.1\n",
    "     - Epoch 1: 1.0 * 0.1 = 0.10\n",
    "     - Epoch 4: 2.9 * 0.1 = 0.29 (nearly 3x larger!)\n",
    "\n",
    "2. **Track These Instead:**\n",
    "   - ‚úÖ **R¬≤_z**: Directly comparable across epochs (correlation-based, not loss-based)\n",
    "   - ‚úÖ **3D Error (mm)**: Absolute metric, always comparable\n",
    "   - ‚ùå **Loss**: Changes meaning as w_z changes, NOT comparable\n",
    "\n",
    "3. **Why This Design Works:**\n",
    "   - Early epochs: Low w_z ‚Üí model focuses on learning basic depth patterns\n",
    "   - Later epochs: High w_z ‚Üí depth precision becomes more important\n",
    "   - Progressive emphasis guides learning from coarse to fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af401a6",
   "metadata": {},
   "source": [
    "## üß™ Experiment 8: Multi-Head Depth Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c09e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadDepthNet(StereoTwoStageNet):\n",
    "    def __init__(self, backbone_name=\"resnet18\", pretrained=True):\n",
    "        super().__init__(backbone_name, pretrained)\n",
    "        in_dim = 256 + 4  # [fused_vec, origin, direction]\n",
    "        \n",
    "        # Each head predicts (offset_t, depth_z)\n",
    "        # Coarse head: Direct prediction with no hidden layers\n",
    "        self.head_coarse = nn.Linear(in_dim, 2)\n",
    "        \n",
    "        self.head_medium = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        \n",
    "        self.head_fine = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        \n",
    "        # Remove original head since we're using multi-heads\n",
    "        delattr(self, 'offset_depth_head')\n",
    "\n",
    "    def forward(self, left_img, right_img):\n",
    "        fused_vec, fused_4d = self._fused_vec(left_img, right_img)\n",
    "        axis_params = self.axis_head(fused_4d)\n",
    "        origin = axis_params[:, :2]\n",
    "        direction = F.normalize(axis_params[:, 2:], dim=1)\n",
    "        \n",
    "        # Prepare input for heads\n",
    "        conditioned = torch.cat([fused_vec, origin.detach(), direction.detach()], dim=1)\n",
    "        \n",
    "        # Get predictions from each head\n",
    "        pred_coarse = self.head_coarse(conditioned)  # (B, 2)\n",
    "        pred_medium = self.head_medium(conditioned)  # (B, 2)\n",
    "        pred_fine = self.head_fine(conditioned)    # (B, 2)\n",
    "        \n",
    "        # Weighted average of predictions (fine head gets more weight)\n",
    "        weights = torch.tensor([0.2, 0.3, 0.5], device=conditioned.device)\n",
    "        offset_t = (weights[0] * pred_coarse[:, 0:1] + \n",
    "                   weights[1] * pred_medium[:, 0:1] + \n",
    "                   weights[2] * pred_fine[:, 0:1])\n",
    "        \n",
    "        depth_z = self.softplus(weights[0] * pred_coarse[:, 1:2] + \n",
    "                              weights[1] * pred_medium[:, 1:2] + \n",
    "                              weights[2] * pred_fine[:, 1:2])\n",
    "        \n",
    "        # Compute intersection\n",
    "        xy_inter = origin + offset_t * direction\n",
    "        intersection = torch.cat([xy_inter, depth_z], dim=1)\n",
    "        \n",
    "        return {\n",
    "            \"origin\": origin,\n",
    "            \"direction\": direction,\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": intersection,\n",
    "            # Add individual predictions for analysis\n",
    "            \"head_preds\": {\n",
    "                \"coarse\": pred_coarse,\n",
    "                \"medium\": pred_medium,\n",
    "                \"fine\": pred_fine\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def forward_e2e(self, left_img, right_img):\n",
    "        # Same logic but without detach()\n",
    "        fused_vec, fused_4d = self._fused_vec(left_img, right_img)\n",
    "        axis_params = self.axis_head(fused_4d)\n",
    "        origin = axis_params[:, :2]\n",
    "        direction = F.normalize(axis_params[:, 2:], dim=1)\n",
    "        \n",
    "        conditioned = torch.cat([fused_vec, origin, direction], dim=1)\n",
    "        \n",
    "        pred_coarse = self.head_coarse(conditioned)\n",
    "        pred_medium = self.head_medium(conditioned)\n",
    "        pred_fine = self.head_fine(conditioned)\n",
    "        \n",
    "        weights = torch.tensor([0.2, 0.3, 0.5], device=conditioned.device)\n",
    "        offset_t = (weights[0] * pred_coarse[:, 0:1] + \n",
    "                   weights[1] * pred_medium[:, 0:1] + \n",
    "                   weights[2] * pred_fine[:, 0:1])\n",
    "        \n",
    "        depth_z = self.softplus(weights[0] * pred_coarse[:, 1:2] + \n",
    "                              weights[1] * pred_medium[:, 1:2] + \n",
    "                              weights[2] * pred_fine[:, 1:2])\n",
    "        \n",
    "        xy_inter = origin + offset_t * direction\n",
    "        intersection = torch.cat([xy_inter, depth_z], dim=1)\n",
    "        \n",
    "        return {\n",
    "            \"origin\": origin,\n",
    "            \"direction\": direction,\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": intersection,\n",
    "            \"head_preds\": {\n",
    "                \"coarse\": pred_coarse,\n",
    "                \"medium\": pred_medium,\n",
    "                \"fine\": pred_fine\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e8530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_loss(outputs, targets, head_weights=None, **kwargs):\n",
    "    \"\"\"Loss function for multi-head models with per-head supervision\"\"\"\n",
    "    if head_weights is None:\n",
    "        head_weights = {'coarse': 0.2, 'medium': 0.3, 'fine': 0.5}\n",
    "    \n",
    "    # Individual head losses\n",
    "    head_losses = {}\n",
    "    for head_name, preds in outputs['head_preds'].items():\n",
    "        # Compute intersection from raw predictions\n",
    "        # Note: depth_z needs softplus since preds are raw outputs\n",
    "        offset_t = preds[:, 0:1]\n",
    "        depth_z = F.softplus(preds[:, 1:2])\n",
    "        \n",
    "        head_out = {\n",
    "            \"origin\": outputs[\"origin\"],\n",
    "            \"direction\": outputs[\"direction\"],\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": torch.cat([\n",
    "                outputs[\"origin\"] + offset_t * F.normalize(outputs[\"direction\"], dim=1),\n",
    "                depth_z\n",
    "            ], dim=1)\n",
    "        }\n",
    "        head_losses[head_name] = stereo_two_stage_loss(head_out, targets, **kwargs)\n",
    "    \n",
    "    # Use ONLY individual head losses (weighted combination)\n",
    "    # The ensemble prediction is already a weighted combination of the heads,\n",
    "    # so we don't need to add a separate ensemble loss\n",
    "    total_loss = sum(w * head_losses[name] for name, w in head_weights.items())\n",
    "    \n",
    "    return total_loss, head_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257105a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multi-head depth predictor experiment\n",
    "multihead_result = experiment(\n",
    "    model_name=\"multihead_depth, augmentation\",\n",
    "    model_class=MultiHeadDepthNet,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    description=\"Multi-head depth predictor: ensemble of coarse, medium, fine heads\"\n",
    ")\n",
    "\n",
    "log_experiment(multihead_result)\n",
    "show_leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb9092",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98677890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnectionSpatialAttentionNet(StereoTwoStageNet):\n",
    "    \"\"\"\n",
    "    Combined architecture: Skip connections + Spatial attention for depth prediction.\n",
    "    \n",
    "    Triple-path fusion:\n",
    "    1. Main convolutional path (spatial attention applied)\n",
    "    2. Skip connection path (direct linear transformation)\n",
    "    3. Spatial attention maps for depth-relevant region weighting\n",
    "    \n",
    "    This combines the gradient flow benefits of skip connections with\n",
    "    the region-focused learning of spatial attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone_name=\"resnet18\", pretrained=True):\n",
    "        super().__init__(backbone_name, pretrained)\n",
    "        \n",
    "        # Calculate fused channels\n",
    "        fused_channels = 128 * len(self.feature_dims) * 2\n",
    "        \n",
    "        # Spatial attention module\n",
    "        self.spatial_attention = SpatialAttentionModule(fused_channels)\n",
    "        \n",
    "        # Enhanced fusion with increased capacity (for attention-weighted features)\n",
    "        self.spatial_fusion = nn.Sequential(\n",
    "            nn.Conv2d(fused_channels, 512, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        \n",
    "        # Skip connection path - processes raw concatenated features\n",
    "        self.skip_fusion = nn.Linear(fused_channels, 256)\n",
    "        \n",
    "    def _fused_vec(self, left_img, right_img):\n",
    "        \"\"\"Extract fused features with skip connection and spatial attention\"\"\"\n",
    "        feats_l = self.backbone(left_img)\n",
    "        feats_r = self.backbone(right_img)\n",
    "        H, W = feats_l[-1].shape[2:]\n",
    "        \n",
    "        # Project and fuse features at multiple scales\n",
    "        fused_scales = []\n",
    "        for fl, fr, proj in zip(feats_l, feats_r, self.proj):\n",
    "            fl_proj = proj(fl)\n",
    "            fr_proj = proj(fr)\n",
    "            fl_proj = F.adaptive_avg_pool2d(fl_proj, (H, W))\n",
    "            fr_proj = F.adaptive_avg_pool2d(fr_proj, (H, W))\n",
    "            fused_scales.append(torch.cat([fl_proj, fr_proj], dim=1))\n",
    "\n",
    "        # Concatenate all scales: (B, 1280, H, W)\n",
    "        x = torch.cat(fused_scales, dim=1)\n",
    "        \n",
    "        # Apply spatial attention to weight depth-relevant regions\n",
    "        attended_x, attention_map = self.spatial_attention(x)\n",
    "        \n",
    "        # Main fusion path: Convolutional transformation on attended features\n",
    "        fused_4d = self.spatial_fusion(attended_x)\n",
    "        fused_vec = fused_4d.view(fused_4d.size(0), -1)  # (B, 256)\n",
    "        \n",
    "        # Skip connection path: Direct linear transformation of raw features\n",
    "        skip_pooled = F.adaptive_avg_pool2d(x, 1).view(x.size(0), -1)  # (B, 1280)\n",
    "        skip_vec = self.skip_fusion(skip_pooled)  # (B, 256)\n",
    "        \n",
    "        # Triple fusion: Combine main attended path + skip path\n",
    "        combined_vec = fused_vec + skip_vec\n",
    "        \n",
    "        return combined_vec, fused_4d\n",
    "\n",
    "print(\"‚úÖ SkipConnectionSpatialAttentionNet model defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ff554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run skip_connections + spatial_attention experiment\n",
    "skip_spatial_result = experiment(\n",
    "    model_name=\"skip_connections + spatial_attention + focal_loss\",\n",
    "    model_class=SkipConnectionSpatialAttentionNet,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    description=\"Combined skip connections + spatial attention for enhanced depth prediction\"\n",
    ")\n",
    "\n",
    "log_experiment(skip_spatial_result)\n",
    "show_leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b752db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run depth_curriculum + skip_connections experiment\n",
    "depth_curriculum_skip_result = experiment(\n",
    "    model_name=\"depth_curriculum + skip_connections\",\n",
    "    model_class=SkipConnectionMultiHeadNet,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    description=\"Depth loss curriculum: w_z gradually increases + skip connections for better gradient flow\",\n",
    "    strategy=\"curriculum\"\n",
    ")\n",
    "\n",
    "log_experiment(depth_curriculum_skip_result)\n",
    "show_leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2bd83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnectionMultiHeadNet(StereoTwoStageNet):\n",
    "    \"\"\"\n",
    "    Combined architecture: Skip connections + Multi-head depth prediction.\n",
    "\n",
    "    Features:\n",
    "    1. Skip connections for better gradient flow in fusion\n",
    "    2. Three depth heads (coarse, medium, fine) for ensemble prediction\n",
    "    3. Each head gets the skip-enhanced features\n",
    "\n",
    "    This combines the gradient flow benefits of skip connections with\n",
    "    the ensemble robustness of multi-head prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone_name=\"resnet18\", pretrained=True):\n",
    "        super().__init__(backbone_name, pretrained)\n",
    "\n",
    "        # Calculate fused channels\n",
    "        fused_channels = 128 * len(self.feature_dims) * 2\n",
    "\n",
    "        # Enhanced fusion with skip connections (same as SkipConnectionNet)\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(fused_channels, 512, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        # Skip connection path\n",
    "        self.skip_fusion = nn.Linear(fused_channels, 256)\n",
    "\n",
    "        # Multi-head depth predictors (coarse, medium, fine)\n",
    "        # Each head takes the skip-enhanced features and predicts depth\n",
    "        self.depth_head_coarse = nn.Sequential(\n",
    "            nn.Linear(256 + 4, 128),  # +4 for axis parameters\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 2)  # (t, z_raw)\n",
    "        )\n",
    "\n",
    "        self.depth_head_medium = nn.Sequential(\n",
    "            nn.Linear(256 + 4, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "        self.depth_head_fine = nn.Sequential(\n",
    "            nn.Linear(256 + 4, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def _fused_vec(self, left_img, right_img):\n",
    "        \"\"\"Extract fused features with skip connection enhancement\"\"\"\n",
    "        feats_l = self.backbone(left_img)\n",
    "        feats_r = self.backbone(right_img)\n",
    "        H, W = feats_l[-1].shape[2:]\n",
    "\n",
    "        # Project and fuse features at multiple scales\n",
    "        fused_scales = []\n",
    "        for fl, fr, proj in zip(feats_l, feats_r, self.proj):\n",
    "            fl_proj = proj(fl)\n",
    "            fr_proj = proj(fr)\n",
    "            fl_proj = F.adaptive_avg_pool2d(fl_proj, (H, W))\n",
    "            fr_proj = F.adaptive_avg_pool2d(fr_proj, (H, W))\n",
    "            fused_scales.append(torch.cat([fl_proj, fr_proj], dim=1))\n",
    "\n",
    "        # Concatenate all scales: (B, 1280, H, W)\n",
    "        x = torch.cat(fused_scales, dim=1)\n",
    "\n",
    "        # Main fusion path: Convolutional transformation\n",
    "        fused_4d = self.fusion(x)\n",
    "        fused_vec = fused_4d.view(fused_4d.size(0), -1)  # (B, 256)\n",
    "\n",
    "        # Skip connection path: Direct linear transformation\n",
    "        skip_pooled = F.adaptive_avg_pool2d(x, 1).view(x.size(0), -1)  # (B, 1280)\n",
    "        skip_vec = self.skip_fusion(skip_pooled)  # (B, 256)\n",
    "\n",
    "        # Combine both pathways for enhanced features\n",
    "        combined_vec = fused_vec + skip_vec\n",
    "\n",
    "        return combined_vec, fused_4d\n",
    "\n",
    "    def forward(self, left_img, right_img):\n",
    "        \"\"\"Forward pass with multi-head depth prediction\"\"\"\n",
    "        fused_vec, fused_4d = self._fused_vec(left_img, right_img)\n",
    "\n",
    "        # Axis prediction (shared across heads)\n",
    "        axis_params = self.axis_head(fused_4d)\n",
    "        origin = axis_params[:, :2]\n",
    "        direction = F.normalize(axis_params[:, 2:], dim=1)\n",
    "\n",
    "        # Condition each depth head on the axis and skip-enhanced features\n",
    "        conditioned = torch.cat([fused_vec, origin.detach(), direction.detach()], dim=1)\n",
    "\n",
    "        # Multi-head depth predictions (raw outputs, before softplus)\n",
    "        depth_coarse_raw = self.depth_head_coarse(conditioned)\n",
    "        depth_medium_raw = self.depth_head_medium(conditioned)\n",
    "        depth_fine_raw = self.depth_head_fine(conditioned)\n",
    "\n",
    "        # Compute ensemble depth prediction (weighted average of heads)\n",
    "        # Each head outputs (B, 2): [offset_t_raw, depth_z_raw]\n",
    "        depth_ensemble_raw = (0.2 * depth_coarse_raw + 0.3 * depth_medium_raw + 0.5 * depth_fine_raw)\n",
    "\n",
    "        # Extract offset and depth from ensemble\n",
    "        offset_t = depth_ensemble_raw[:, 0:1]\n",
    "        depth_z_raw = depth_ensemble_raw[:, 1:2]\n",
    "        depth_z = F.softplus(depth_z_raw)\n",
    "\n",
    "        # Compute intersection point\n",
    "        xy_inter = origin + offset_t * direction\n",
    "        intersection = torch.cat([xy_inter, depth_z], dim=1)\n",
    "\n",
    "        # Return structure expected by multihead_loss and validate\n",
    "        return {\n",
    "            \"origin\": origin,\n",
    "            \"direction\": direction,\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": intersection,\n",
    "            \"head_preds\": {\n",
    "                \"coarse\": depth_coarse_raw,\n",
    "                \"medium\": depth_medium_raw,\n",
    "                \"fine\": depth_fine_raw\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def forward_e2e(self, left_img, right_img):\n",
    "        \"\"\"End-to-end forward pass with multi-head depth prediction (no gradient detachment)\"\"\"\n",
    "        fused_vec, fused_4d = self._fused_vec(left_img, right_img)\n",
    "\n",
    "        # Axis prediction (shared across heads)\n",
    "        axis_params = self.axis_head(fused_4d)\n",
    "        origin = axis_params[:, :2]\n",
    "        direction = F.normalize(axis_params[:, 2:], dim=1)\n",
    "\n",
    "        # Condition each depth head on the axis and skip-enhanced features (no detach for e2e)\n",
    "        conditioned = torch.cat([fused_vec, origin, direction], dim=1)\n",
    "\n",
    "        # Multi-head depth predictions (raw outputs, before softplus)\n",
    "        depth_coarse_raw = self.depth_head_coarse(conditioned)\n",
    "        depth_medium_raw = self.depth_head_medium(conditioned)\n",
    "        depth_fine_raw = self.depth_head_fine(conditioned)\n",
    "\n",
    "        # Compute ensemble depth prediction (weighted average of heads)\n",
    "        # Each head outputs (B, 2): [offset_t_raw, depth_z_raw]\n",
    "        depth_ensemble_raw = (0.2 * depth_coarse_raw + 0.3 * depth_medium_raw + 0.5 * depth_fine_raw)\n",
    "\n",
    "        # Extract offset and depth from ensemble\n",
    "        offset_t = depth_ensemble_raw[:, 0:1]\n",
    "        depth_z_raw = depth_ensemble_raw[:, 1:2]\n",
    "        depth_z = F.softplus(depth_z_raw)\n",
    "\n",
    "        # Compute intersection point\n",
    "        xy_inter = origin + offset_t * direction\n",
    "        intersection = torch.cat([xy_inter, depth_z], dim=1)\n",
    "\n",
    "        # Return structure expected by multihead_loss and validate\n",
    "        return {\n",
    "            \"origin\": origin,\n",
    "            \"direction\": direction,\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": intersection,\n",
    "            \"head_preds\": {\n",
    "                \"coarse\": depth_coarse_raw,\n",
    "                \"medium\": depth_medium_raw,\n",
    "                \"fine\": depth_fine_raw\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ SkipConnectionMultiHeadNet model defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ab5185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run skip_connections + multihead_depth experiment\n",
    "skip_multihead_result = experiment(\n",
    "    model_name=\"skip_connections + multihead_depth\",\n",
    "    model_class=SkipConnectionMultiHeadNet,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    description=\"Skip connections + multi-head depth predictor: ensemble of coarse/medium/fine heads with gradient flow enhancement\",\n",
    "    strategy=\"MultiHead\",\n",
    "    head_weights={'coarse': 0.2, 'medium': 0.3, 'fine': 0.5},\n",
    "    use_focal=True,\n",
    "    focal_alpha=0.25,\n",
    "    focal_gamma=1.5\n",
    ")\n",
    "\n",
    "log_experiment(skip_multihead_result)\n",
    "show_leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd85a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run skip_connections + multihead_depth experiment with curriculum learning\n",
    "def experiment_multihead_curriculum(model_name, model_class, train_loader, val_loader, description=\"\",\n",
    "                                 head_weights={\"coarse\": 0.2, \"medium\": 0.3, \"fine\": 0.5},\n",
    "                                 use_focal=False, focal_alpha=0.25, focal_gamma=1.5):\n",
    "    \"\"\"Combined MultiHead + Curriculum experiment\"\"\"\n",
    "    # Reset seed for reproducibility\n",
    "    set_seed(42)\n",
    "    \n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else\n",
    "                         \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"{'='*60}\\n üöÄ EXPERIMENT: {model_name}\\n{'='*60}\\nDescription: {description}\\n        Device: {device}\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = model_class().to(device)\n",
    "    \n",
    "    # MultiHead + Curriculum setup\n",
    "    print(f\"üìä Multi-Head + Curriculum Configuration:\")\n",
    "    print(f\"   Head Weights: coarse={head_weights['coarse']:.1f}, medium={head_weights['medium']:.1f}, fine={head_weights['fine']:.1f}\")\n",
    "    print(f\"   Curriculum: Progressive depth weight increase (5.0‚Üí7.0‚Üí10.0‚Üí15.0)\")\n",
    "    \n",
    "    # Stage A: Axis pretraining\n",
    "    print(f\"{'='*60}\\nSTAGE A: Axis Pretraining ({QUICK_CONFIG['epochs']['A']} epochs)\\n{'='*60}\")\n",
    "    logs_A = train_stage_curriculum_multihead(model, train_loader, val_loader, 'A', \n",
    "                                           QUICK_CONFIG['epochs']['A'], device,\n",
    "                                           head_weights=head_weights,\n",
    "                                           use_focal=use_focal, focal_alpha=focal_alpha, focal_gamma=focal_gamma)\n",
    "    \n",
    "    # Stage B: Depth training with curriculum\n",
    "    print(f\"{'='*60}\\nSTAGE B: MultiHead + Curriculum ({QUICK_CONFIG['epochs']['B']} epochs)\\n{'='*60}\")\n",
    "    logs_B = train_stage_curriculum_multihead(model, train_loader, val_loader, 'B', \n",
    "                                           QUICK_CONFIG['epochs']['B'], device,\n",
    "                                           head_weights=head_weights,\n",
    "                                           use_focal=use_focal, focal_alpha=focal_alpha, focal_gamma=focal_gamma)\n",
    "    \n",
    "    # Stage C: End-to-end fine-tuning\n",
    "    print(f\"{'='*60}\\nSTAGE C: End-to-End Fine-tuning ({QUICK_CONFIG['epochs']['C']} epochs)\\n{'='*60}\")\n",
    "    logs_C = train_stage_curriculum_multihead(model, train_loader, val_loader, 'C', \n",
    "                                           QUICK_CONFIG['epochs']['C'], device,\n",
    "                                           head_weights=head_weights,\n",
    "                                           use_focal=use_focal, focal_alpha=focal_alpha, focal_gamma=focal_gamma)\n",
    "    \n",
    "    # Final evaluation\n",
    "    final_metrics = validate(model, val_loader, device)\n",
    "    runtime = time.time() - start_time\n",
    "    \n",
    "    result = {\n",
    "        \"model_name\": model_name,\n",
    "        \"r2_z\": final_metrics[\"r2xyz\"][2],\n",
    "        \"3d_error_mm\": final_metrics.get(\"e3d\", 0),\n",
    "        \"2d_error_px\": final_metrics.get(\"e2d\", 0),\n",
    "        \"runtime_min\": runtime / 60,\n",
    "        \"decision\": \"COMBINED\",\n",
    "        \"description\": description\n",
    "    }\n",
    "\n",
    "    print(f\"‚úÖ EXPERIMENT COMPLETE: {model_name}\")\n",
    "    print(f\"   R¬≤_z: {result['r2_z']:.3f}, 3D Error: {result['3d_error_mm']:.1f}mm\")\n",
    "    print(f\"   Runtime: {result['runtime_min']:.1f} minutes\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Custom training stage for MultiHead + Curriculum\n",
    "def train_stage_curriculum_multihead(model, train_loader, val_loader, stage, max_epochs, device,\n",
    "                                   head_weights, use_amp=False, use_focal=False, focal_alpha=0.25, focal_gamma=1.5):\n",
    "    \"\"\"Train one stage with MultiHead loss and curriculum depth weighting\"\"\"\n",
    "    e2e = configure_model_for_stage(model, stage)\n",
    "    optim = make_optim(model, stage)\n",
    "    scaler = GradScaler(enabled=use_amp)\n",
    "    \n",
    "    # Loss weights setup\n",
    "    loss_weights = QUICK_CONFIG['stage_loss_weights'][stage].copy()\n",
    "    loss_weights.update(QUICK_CONFIG['global_loss_weights'])\n",
    "    loss_weights['head_weights'] = head_weights\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'r2_z': []}\n",
    "    \n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # Apply curriculum weight for this epoch\n",
    "        loss_weights['w_z'] = get_curriculum_weights(stage, epoch-1, max_epochs)['w_z']\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        train_metrics = defaultdict(float)\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            if e2e:\n",
    "                out = model.forward_e2e(batch['left'], batch['right'])\n",
    "            else:\n",
    "                out = model(batch['left'], batch['right'])\n",
    "            \n",
    "            # MultiHead loss with curriculum\n",
    "            loss, head_losses = multihead_loss(out, batch, **loss_weights)\n",
    "            \n",
    "            # Track losses\n",
    "            train_metrics['loss'] += loss.item()\n",
    "            for name, hloss in head_losses.items():\n",
    "                train_metrics[f\"{name}_loss\"] += hloss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optim)\n",
    "            scaler.update()\n",
    "            optim.zero_grad()\n",
    "        \n",
    "        # Validation\n",
    "        val_metrics = validate(model, val_loader, device)\n",
    "        \n",
    "        # Logging\n",
    "        avg_loss = train_metrics['loss'] / len(train_loader)\n",
    "        r2_z = val_metrics['r2xyz'][2]\n",
    "        w_z = loss_weights['w_z']\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{max_epochs} | Loss: {avg_loss:.4f} | Val R¬≤_z: {r2_z:.3f} | w_z: {w_z:.1f}\")\n",
    "        \n",
    "        history['train_loss'].append(avg_loss)\n",
    "        history['val_loss'].append(val_metrics['val_loss'])\n",
    "        history['r2_z'].append(r2_z)\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Run the combined experiment\n",
    "skip_multihead_curriculum_result = experiment_multihead_curriculum(\n",
    "    model_name=\"skip_connections + multihead_depth + curriculum\",\n",
    "    model_class=SkipConnectionMultiHeadNet,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    description=\"Skip connections + multi-head depth + curriculum learning: progressive depth weight increase\",\n",
    "    head_weights={'coarse': 0.2, 'medium': 0.3, 'fine': 0.5},\n",
    "    use_focal=True,\n",
    "    focal_alpha=0.25,\n",
    "    focal_gamma=1.5\n",
    ")\n",
    "\n",
    "log_experiment(skip_multihead_curriculum_result)\n",
    "show_leaderboard()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effeb0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run skip_connections + multihead_depth experiment\n",
    "skip_multihead_result = experiment(\n",
    "    model_name=\"skip_connections + multihead_depth\",\n",
    "    model_class=SkipConnectionMultiHeadNet,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    description=\"Skip connections + multi-head depth predictor: ensemble of coarse/medium/fine heads with gradient flow enhancement\",\n",
    "    strategy=\"MultiHead\",\n",
    "    head_weights={'coarse': 0.2, 'medium': 0.3, 'fine': 0.5}\n",
    ")\n",
    "\n",
    "log_experiment(skip_multihead_result)\n",
    "show_leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e63431",
   "metadata": {},
   "source": [
    "## Experiment: Skip Connections + Auxiliary Depth\n",
    "\n",
    "**Hypothesis**: Combining skip connections (better gradient flow) with auxiliary depth supervision (richer spatial features) should improve performance.\n",
    "\n",
    "**Architecture**:\n",
    "- Skip connections: Dual-path fusion (main conv path + skip linear path)\n",
    "- Auxiliary depth: Separate depth head with transposed convolutions\n",
    "- Expected R¬≤_z: 0.55+ (combining strengths of both approaches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ea2df88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SkipConnectionAuxiliaryDepthNet model defined (FIXED)!\n"
     ]
    }
   ],
   "source": [
    "class SkipConnectionAuxiliaryDepthNet(StereoTwoStageNet):\n",
    "    \"\"\"Skip connections + Auxiliary depth supervision\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone_name=\"resnet18\", pretrained=True):\n",
    "        super().__init__(backbone_name, pretrained)\n",
    "        \n",
    "        # Skip connection fusion - preserve spatial dimensions\n",
    "        fused_channels = 128 * len(self.feature_dims) * 2\n",
    "        self.fusion_conv = nn.Sequential(\n",
    "            nn.Conv2d(fused_channels, 512, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.skip_fusion = nn.Linear(fused_channels, 256)\n",
    "        \n",
    "        # Auxiliary depth head - starts from 7x7 feature map\n",
    "        self.depth_map_head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 7x7 -> 14x14\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 14x14 -> 28x28\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),    # 28x28 -> 56x56\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 1, kernel_size=3, padding=1),                        # 56x56 -> 56x56\n",
    "            nn.Softplus(beta=1.0)  # Ensure positive depth values\n",
    "        )\n",
    "    \n",
    "    def _fused_vec_with_4d(self, left_img, right_img):\n",
    "        feats_l = self.backbone(left_img)\n",
    "        feats_r = self.backbone(right_img)\n",
    "        H, W = feats_l[-1].shape[2:]\n",
    "        \n",
    "        fused_scales = []\n",
    "        for fl, fr, proj in zip(feats_l, feats_r, self.proj):\n",
    "            fl_proj = proj(fl)\n",
    "            fr_proj = proj(fr)\n",
    "            fl_proj = F.adaptive_avg_pool2d(fl_proj, (H, W))\n",
    "            fr_proj = F.adaptive_avg_pool2d(fr_proj, (H, W))\n",
    "            fused_scales.append(torch.cat([fl_proj, fr_proj], dim=1))\n",
    "        \n",
    "        x = torch.cat(fused_scales, dim=1)  # (B, 1280, H, W) - H,W typically 7x7\n",
    "        fused_4d = self.fusion_conv(x)      # (B, 256, H, W)\n",
    "        \n",
    "        # Skip connection\n",
    "        skip_pooled = F.adaptive_avg_pool2d(x, 1).view(x.size(0), -1)\n",
    "        skip_vec = self.skip_fusion(skip_pooled)\n",
    "        \n",
    "        # Pooled vector for intersection head\n",
    "        fused_vec = F.adaptive_avg_pool2d(fused_4d, 1).view(fused_4d.size(0), -1)\n",
    "        combined_vec = fused_vec + skip_vec  # Residual addition\n",
    "        \n",
    "        return combined_vec, fused_4d\n",
    "    \n",
    "    def forward(self, left_img, right_img):\n",
    "        fused_vec, fused_4d = self._fused_vec_with_4d(left_img, right_img)\n",
    "        \n",
    "        pooled = F.adaptive_avg_pool2d(fused_4d, 1)\n",
    "        axis_params = self.axis_head(pooled)\n",
    "        origin = axis_params[:, :2]\n",
    "        direction = F.normalize(axis_params[:, 2:], dim=1)\n",
    "        \n",
    "        conditioned = torch.cat([fused_vec, origin.detach(), direction.detach()], dim=1)\n",
    "        od = self.offset_depth_head(conditioned)\n",
    "        offset_t = od[:, 0:1]\n",
    "        depth_z = self.softplus(od[:, 1:2])\n",
    "        \n",
    "        xy_inter = origin + offset_t * direction\n",
    "        intersection = torch.cat([xy_inter, depth_z], dim=1)\n",
    "        depth_map = self.depth_map_head(fused_4d)\n",
    "        \n",
    "        return {\n",
    "            \"origin\": origin,\n",
    "            \"direction\": direction,\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": intersection,\n",
    "            \"depth_map\": depth_map\n",
    "        }\n",
    "    \n",
    "    def forward_e2e(self, left_img, right_img):\n",
    "        fused_vec, fused_4d = self._fused_vec_with_4d(left_img, right_img)\n",
    "        \n",
    "        pooled = F.adaptive_avg_pool2d(fused_4d, 1)\n",
    "        axis_params = self.axis_head(pooled)\n",
    "        origin = axis_params[:, :2]\n",
    "        direction = F.normalize(axis_params[:, 2:], dim=1)\n",
    "        \n",
    "        conditioned = torch.cat([fused_vec, origin, direction], dim=1)\n",
    "        od = self.offset_depth_head(conditioned)\n",
    "        offset_t = od[:, 0:1]\n",
    "        depth_z = self.softplus(od[:, 1:2])\n",
    "        \n",
    "        xy_inter = origin + offset_t * direction\n",
    "        intersection = torch.cat([xy_inter, depth_z], dim=1)\n",
    "        depth_map = self.depth_map_head(fused_4d)\n",
    "        \n",
    "        return {\n",
    "            \"origin\": origin,\n",
    "            \"direction\": direction,\n",
    "            \"offset_t\": offset_t,\n",
    "            \"depth_z\": depth_z,\n",
    "            \"intersection\": intersection,\n",
    "            \"depth_map\": depth_map\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ SkipConnectionAuxiliaryDepthNet model defined (FIXED)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a1d3309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ EXPERIMENT: skip_connections + auxiliary_depth\n",
      "============================================================\n",
      "Description: Skip connections for gradient flow + auxiliary depth supervision\n",
      "Device: mps | Expected time: ~5-15 minutes\n",
      "\n",
      "\n",
      "============================================================\n",
      "STAGE A: Axis Pretraining (20 epochs)\n",
      "============================================================\n",
      "\n",
      "Epoch 1/20 | Loss: 3.1604 | Val: 1.6880 | Ang: 27.82¬∞\n",
      "\n",
      "Epoch 2/20 | Loss: 1.3264 | Val: 1.4528 | Ang: 28.10¬∞\n",
      "\n",
      "Epoch 3/20 | Loss: 1.1800 | Val: 1.3516 | Ang: 26.89¬∞\n",
      "\n",
      "Epoch 4/20 | Loss: 0.9812 | Val: 0.6296 | Ang: 16.46¬∞\n",
      "\n",
      "Epoch 5/20 | Loss: 0.2929 | Val: 0.2112 | Ang: 8.66¬∞\n",
      "\n",
      "Epoch 6/20 | Loss: 0.1895 | Val: 0.1189 | Ang: 3.55¬∞\n",
      "\n",
      "Epoch 7/20 | Loss: 0.1384 | Val: 0.1305 | Ang: 6.07¬∞\n",
      "\n",
      "Epoch 8/20 | Loss: 0.1214 | Val: 0.1022 | Ang: 3.69¬∞\n",
      "\n",
      "Epoch 9/20 | Loss: 0.1042 | Val: 0.0883 | Ang: 2.59¬∞\n",
      "\n",
      "Epoch 10/20 | Loss: 0.0959 | Val: 0.0818 | Ang: 2.86¬∞\n",
      "\n",
      "Epoch 11/20 | Loss: 0.0866 | Val: 0.0820 | Ang: 3.05¬∞\n",
      "\n",
      "Epoch 12/20 | Loss: 0.0742 | Val: 0.0613 | Ang: 2.30¬∞\n",
      "\n",
      "Epoch 13/20 | Loss: 0.0679 | Val: 0.0625 | Ang: 3.87¬∞\n",
      "\n",
      "Epoch 14/20 | Loss: 0.0621 | Val: 0.0526 | Ang: 3.22¬∞\n",
      "\n",
      "Epoch 15/20 | Loss: 0.0518 | Val: 0.0471 | Ang: 2.85¬∞\n",
      "\n",
      "Epoch 16/20 | Loss: 0.0462 | Val: 0.0397 | Ang: 2.51¬∞\n",
      "\n",
      "Epoch 17/20 | Loss: 0.0405 | Val: 0.0333 | Ang: 2.35¬∞\n",
      "\n",
      "Epoch 18/20 | Loss: 0.0327 | Val: 0.0293 | Ang: 2.37¬∞\n",
      "\n",
      "Epoch 19/20 | Loss: 0.0312 | Val: 0.0264 | Ang: 2.41¬∞\n",
      "\n",
      "Epoch 20/20 | Loss: 0.0287 | Val: 0.0264 | Ang: 2.62¬∞\n",
      "\n",
      "\n",
      "============================================================\n",
      "STAGE B: Intersection Training (30 epochs)\n",
      "============================================================\n",
      "\n",
      "Epoch 1/30 | Loss: 2.0662 | Val: 0.9181 | R¬≤_z: 0.056 | 3D: 12.45mm\n",
      "\n",
      "Epoch 2/30 | Loss: 0.9443 | Val: 0.8325 | R¬≤_z: 0.175 | 3D: 12.14mm\n",
      "\n",
      "Epoch 3/30 | Loss: 0.8747 | Val: 0.7663 | R¬≤_z: 0.230 | 3D: 11.48mm\n",
      "\n",
      "Epoch 4/30 | Loss: 0.8245 | Val: 0.7483 | R¬≤_z: 0.286 | 3D: 10.67mm\n",
      "\n",
      "Epoch 5/30 | Loss: 0.7633 | Val: 0.8254 | R¬≤_z: 0.125 | 3D: 11.37mm\n",
      "\n",
      "Epoch 6/30 | Loss: 0.7509 | Val: 0.7056 | R¬≤_z: 0.291 | 3D: 10.16mm\n",
      "\n",
      "Epoch 7/30 | Loss: 0.6930 | Val: 0.7234 | R¬≤_z: 0.294 | 3D: 9.61mm\n",
      "\n",
      "Epoch 8/30 | Loss: 0.6927 | Val: 0.9752 | R¬≤_z: -0.246 | 3D: 14.75mm\n",
      "\n",
      "Epoch 9/30 | Loss: 0.6609 | Val: 0.6781 | R¬≤_z: 0.348 | 3D: 9.37mm\n",
      "\n",
      "Epoch 10/30 | Loss: 0.6668 | Val: 0.6555 | R¬≤_z: 0.392 | 3D: 9.05mm\n",
      "\n",
      "Epoch 11/30 | Loss: 0.6793 | Val: 0.7066 | R¬≤_z: 0.318 | 3D: 10.50mm\n",
      "\n",
      "Epoch 12/30 | Loss: 0.6387 | Val: 0.6202 | R¬≤_z: 0.494 | 3D: 8.30mm\n",
      "\n",
      "Epoch 13/30 | Loss: 0.5955 | Val: 0.6375 | R¬≤_z: 0.491 | 3D: 8.16mm\n",
      "\n",
      "Epoch 14/30 | Loss: 0.6132 | Val: 0.6263 | R¬≤_z: 0.495 | 3D: 8.14mm\n",
      "\n",
      "Epoch 15/30 | Loss: 0.5741 | Val: 0.6323 | R¬≤_z: 0.458 | 3D: 8.35mm\n",
      "\n",
      "Epoch 16/30 | Loss: 0.5999 | Val: 0.6425 | R¬≤_z: 0.461 | 3D: 8.31mm\n",
      "\n",
      "Epoch 17/30 | Loss: 0.5896 | Val: 0.6222 | R¬≤_z: 0.449 | 3D: 8.93mm\n",
      "\n",
      "Epoch 18/30 | Loss: 0.5830 | Val: 0.5985 | R¬≤_z: 0.496 | 3D: 8.03mm\n",
      "\n",
      "Epoch 19/30 | Loss: 0.5448 | Val: 0.5791 | R¬≤_z: 0.549 | 3D: 7.91mm\n",
      "\n",
      "Epoch 20/30 | Loss: 0.5578 | Val: 0.5994 | R¬≤_z: 0.539 | 3D: 7.76mm\n",
      "\n",
      "Epoch 21/30 | Loss: 0.5179 | Val: 0.5652 | R¬≤_z: 0.564 | 3D: 7.64mm\n",
      "\n",
      "Epoch 22/30 | Loss: 0.5649 | Val: 0.5918 | R¬≤_z: 0.444 | 3D: 8.45mm\n",
      "\n",
      "Epoch 23/30 | Loss: 0.5690 | Val: 0.5771 | R¬≤_z: 0.531 | 3D: 8.02mm\n",
      "\n",
      "Epoch 24/30 | Loss: 0.5577 | Val: 0.6121 | R¬≤_z: 0.547 | 3D: 7.83mm\n",
      "\n",
      "Epoch 25/30 | Loss: 0.5561 | Val: 0.6172 | R¬≤_z: 0.473 | 3D: 8.40mm\n",
      "\n",
      "Epoch 26/30 | Loss: 0.5400 | Val: 0.5872 | R¬≤_z: 0.533 | 3D: 8.08mm\n",
      "\n",
      "Epoch 27/30 | Loss: 0.5198 | Val: 0.5663 | R¬≤_z: 0.554 | 3D: 7.38mm\n",
      "\n",
      "Epoch 28/30 | Loss: 0.5289 | Val: 0.6411 | R¬≤_z: 0.415 | 3D: 8.94mm\n",
      "\n",
      "Epoch 29/30 | Loss: 0.5173 | Val: 0.5614 | R¬≤_z: 0.554 | 3D: 7.41mm\n",
      "\n",
      "Epoch 30/30 | Loss: 0.4978 | Val: 0.5831 | R¬≤_z: 0.524 | 3D: 7.76mm\n",
      "\n",
      "\n",
      "üü¢ Stage B R¬≤_z: 0.554. Running FULL Stage C (40 epochs).\n",
      "\n",
      "============================================================\n",
      "STAGE C: End-to-End Fine-tuning (40 epochs)\n",
      "============================================================\n",
      "\n",
      "Epoch 1/40 | Loss: 3.2008 | Val: 3.4348 | R¬≤_z: 0.536 | 3D: 7.67mm\n",
      "\n",
      "Epoch 2/40 | Loss: 2.7469 | Val: 3.2717 | R¬≤_z: 0.528 | 3D: 7.84mm\n",
      "\n",
      "Epoch 3/40 | Loss: 2.6101 | Val: 3.0364 | R¬≤_z: 0.550 | 3D: 7.56mm\n",
      "\n",
      "Epoch 4/40 | Loss: 2.5987 | Val: 2.9728 | R¬≤_z: 0.547 | 3D: 7.71mm\n",
      "\n",
      "Epoch 5/40 | Loss: 2.5350 | Val: 2.9785 | R¬≤_z: 0.542 | 3D: 7.71mm\n",
      "\n",
      "Epoch 6/40 | Loss: 2.5167 | Val: 2.9649 | R¬≤_z: 0.539 | 3D: 7.73mm\n",
      "\n",
      "Epoch 7/40 | Loss: 2.4405 | Val: 2.8619 | R¬≤_z: 0.559 | 3D: 7.48mm\n",
      "\n",
      "Epoch 8/40 | Loss: 2.4320 | Val: 2.8352 | R¬≤_z: 0.557 | 3D: 7.50mm\n",
      "\n",
      "Epoch 9/40 | Loss: 2.3932 | Val: 2.8771 | R¬≤_z: 0.547 | 3D: 7.63mm\n",
      "\n",
      "Epoch 10/40 | Loss: 2.3307 | Val: 2.8646 | R¬≤_z: 0.547 | 3D: 7.64mm\n",
      "\n",
      "Epoch 11/40 | Loss: 2.3519 | Val: 2.8452 | R¬≤_z: 0.533 | 3D: 7.77mm\n",
      "\n",
      "Epoch 12/40 | Loss: 2.3303 | Val: 2.8440 | R¬≤_z: 0.535 | 3D: 7.78mm\n",
      "\n",
      "Epoch 13/40 | Loss: 2.2805 | Val: 2.7853 | R¬≤_z: 0.554 | 3D: 7.60mm\n",
      "\n",
      "Epoch 14/40 | Loss: 2.3006 | Val: 2.8018 | R¬≤_z: 0.544 | 3D: 7.64mm\n",
      "\n",
      "Epoch 15/40 | Loss: 2.3384 | Val: 2.7591 | R¬≤_z: 0.562 | 3D: 7.60mm\n",
      "\n",
      "Epoch 16/40 | Loss: 2.2844 | Val: 2.7375 | R¬≤_z: 0.562 | 3D: 7.52mm\n",
      "\n",
      "Epoch 17/40 | Loss: 2.2357 | Val: 2.8094 | R¬≤_z: 0.505 | 3D: 7.91mm\n",
      "\n",
      "Epoch 18/40 | Loss: 2.3137 | Val: 2.7372 | R¬≤_z: 0.550 | 3D: 7.58mm\n",
      "\n",
      "Epoch 19/40 | Loss: 2.1966 | Val: 2.7138 | R¬≤_z: 0.547 | 3D: 7.53mm\n",
      "\n",
      "Epoch 20/40 | Loss: 2.2247 | Val: 2.7748 | R¬≤_z: 0.552 | 3D: 7.61mm\n",
      "\n",
      "Epoch 21/40 | Loss: 2.2229 | Val: 2.7757 | R¬≤_z: 0.530 | 3D: 7.71mm\n",
      "\n",
      "Epoch 22/40 | Loss: 2.2285 | Val: 2.6941 | R¬≤_z: 0.554 | 3D: 7.55mm\n",
      "\n",
      "Epoch 23/40 | Loss: 2.1717 | Val: 2.6968 | R¬≤_z: 0.543 | 3D: 7.74mm\n",
      "\n",
      "Epoch 24/40 | Loss: 2.0787 | Val: 2.7244 | R¬≤_z: 0.538 | 3D: 7.65mm\n",
      "\n",
      "Epoch 25/40 | Loss: 2.1821 | Val: 2.6960 | R¬≤_z: 0.540 | 3D: 7.61mm\n",
      "\n",
      "Epoch 26/40 | Loss: 2.1942 | Val: 2.6453 | R¬≤_z: 0.561 | 3D: 7.47mm\n",
      "\n",
      "Epoch 27/40 | Loss: 2.0772 | Val: 2.6370 | R¬≤_z: 0.561 | 3D: 7.44mm\n",
      "\n",
      "Epoch 28/40 | Loss: 2.2023 | Val: 2.6953 | R¬≤_z: 0.529 | 3D: 7.66mm\n",
      "\n",
      "Epoch 29/40 | Loss: 2.1741 | Val: 2.6507 | R¬≤_z: 0.556 | 3D: 7.64mm\n",
      "\n",
      "Epoch 30/40 | Loss: 2.0575 | Val: 2.6594 | R¬≤_z: 0.558 | 3D: 7.47mm\n",
      "\n",
      "Epoch 31/40 | Loss: 2.0234 | Val: 2.6297 | R¬≤_z: 0.559 | 3D: 7.44mm\n",
      "\n",
      "Epoch 32/40 | Loss: 2.0300 | Val: 2.6405 | R¬≤_z: 0.559 | 3D: 7.53mm\n",
      "\n",
      "Epoch 33/40 | Loss: 2.1234 | Val: 2.6321 | R¬≤_z: 0.553 | 3D: 7.47mm\n",
      "\n",
      "Epoch 34/40 | Loss: 2.0736 | Val: 2.6539 | R¬≤_z: 0.556 | 3D: 7.58mm\n",
      "\n",
      "Epoch 35/40 | Loss: 2.1624 | Val: 2.6475 | R¬≤_z: 0.574 | 3D: 7.38mm\n",
      "\n",
      "Epoch 36/40 | Loss: 2.0752 | Val: 2.6351 | R¬≤_z: 0.544 | 3D: 7.55mm\n",
      "\n",
      "Epoch 37/40 | Loss: 2.1430 | Val: 2.5848 | R¬≤_z: 0.562 | 3D: 7.47mm\n",
      "\n",
      "Epoch 38/40 | Loss: 2.0499 | Val: 2.6141 | R¬≤_z: 0.561 | 3D: 7.48mm\n",
      "\n",
      "Epoch 39/40 | Loss: 2.0906 | Val: 2.6001 | R¬≤_z: 0.567 | 3D: 7.49mm\n",
      "\n",
      "Epoch 40/40 | Loss: 1.9612 | Val: 2.5717 | R¬≤_z: 0.558 | 3D: 7.45mm\n",
      "\n",
      "\n",
      "============================================================\n",
      "‚úÖ EXPERIMENT COMPLETE: skip_connections + auxiliary_depth\n",
      "üü¢ WINNER | R¬≤_z: 0.558 | 3D Error: 7.45mm | Time: 52.9min\n",
      "============================================================\n",
      "\n",
      "‚úÖ Experiment logged: skip_connections + auxiliary_depth\n",
      "\n",
      "üèÜ EXPERIMENT LEADERBOARD\n",
      "================================================================================\n",
      "Rank Model                R¬≤_z     3D_err   2D_err   Time     Decision    \n",
      "--------------------------------------------------------------------------------\n",
      "1    skip_connections + focal_loss + augmentation 0.570    7.6      26.3     24.0     üü¢ WINNER    \n",
      "2    skip_connections + auxiliary_depth 0.558    7.4      18.5     52.9     üü¢ WINNER    \n",
      "3    skip_connections + focal_loss (full training) 0.557    7.1      15.2     49.9     üü¢ WINNER    \n",
      "4    skip_connections + spatial_attention 0.554    7.1      23.8     34.2     üü¢ WINNER    \n",
      "5    depth_curriculum + skip_connections 0.537    7.6      24.8     26.0     üü° DECENT    \n",
      "6    skip_connections + auxiliary_depth 0.536    7.6      24.3     31.9     üü° DECENT    \n",
      "7    skip_connections + spatial_attention (full training) 0.534    7.0      9.6      60.0     üü° DECENT    \n",
      "8    skip_connections + multihead_depth 0.528    7.6      23.5     27.0     üü° DECENT    \n",
      "9    depth_curriculum     0.517    7.8      19.5     23.6     üü° DECENT    \n",
      "10   skip_connections + multihead_depth + curriculum 0.516    8.1      26.7     25.2     COMBINED    \n",
      "11   spatial_attention + focal_loss 0.515    7.9      24.7     25.3     üü° DECENT    \n",
      "12   skip_connections + focal_loss 0.500    8.1      24.1     23.0     üü° DECENT    \n",
      "13   depth_curriculum + skip_connections + focal_loss 0.496    8.2      25.2     27.8     üü° DECENT    \n",
      "14   skip_connections     0.491    8.0      23.7     35.7     üü° DECENT    \n",
      "15   multihead_depth      0.491    7.9      21.4     24.4     üü° DECENT    \n",
      "16   auxiliary_depth      0.480    7.9      22.6     31.4     üü° DECENT    \n",
      "17   spatial_attention    0.479    8.2      23.2     37.7     üü° DECENT    \n",
      "18   depth_curriculum + augmentation 0.475    8.2      26.4     25.8     üü° DECENT    \n",
      "19   augmentation + focal_loss 0.473    8.4      25.7     26.5     üü° DECENT    \n",
      "20   augmentation         0.473    8.4      25.7     24.1     üü° DECENT    \n",
      "21   multihead_depth + focal_loss 0.472    7.9      21.3     25.6     üü° DECENT    \n",
      "22   depth_curriculum + focal_loss + augmentation 0.465    8.2      27.0     24.0     üü° DECENT    \n",
      "23   auxiliary_depth + focal_loss + augmentation 0.465    8.0      25.3     25.6     üü° DECENT    \n",
      "24   progressive_gradient + focal_loss + augmentation 0.461    8.6      26.6     24.1     üü° DECENT    \n",
      "25   auxiliary_depth + focal_loss 0.460    8.3      22.6     29.1     üü° DECENT    \n",
      "26   progressive_gradient unblocking + focal_loss 0.458    8.4      23.3     126.9    üü° DECENT    \n",
      "27   baseline             0.456    8.3      22.6     29.8     üü° DECENT    \n",
      "28   progressive_gradient 0.455    8.4      23.1     30.7     üü° DECENT    \n",
      "29   focal_loss           0.450    8.3      20.7     31.5     üü° DECENT    \n",
      "30   depth_curriculum + focal_loss 0.449    8.1      20.8     25.3     üî¥ POOR      \n",
      "31   spatial_attention + focal_loss + augmentation 0.440    8.5      21.3     25.3     üî¥ POOR      \n",
      "32   multihead_depth + focal_loss + augmentation 0.430    8.8      24.2     24.1     üî¥ POOR      \n",
      "33   multihead_depth + augmentation 0.422    8.8      24.2     28.0     üî¥ POOR      \n",
      "34   auxiliary_depth + augmentation 0.417    8.7      25.6     29.3     üî¥ POOR      \n",
      "\n",
      "üìà BEST PERFORMERS:\n",
      "   üü¢ WINNER skip_connections + focal_loss + augmentation: R¬≤_z=0.570 (-4.5% vs baseline)\n",
      "   üü¢ WINNER skip_connections + auxiliary_depth: R¬≤_z=0.558 (-6.5% vs baseline)\n",
      "   üü¢ WINNER skip_connections + focal_loss (full training): R¬≤_z=0.557 (-6.7% vs baseline)\n",
      "‚ö†Ô∏è  No baseline reference set, using documentation value: R¬≤_z=0.597\n"
     ]
    }
   ],
   "source": [
    "# Run skip_connections + auxiliary_depth experiment\n",
    "result = experiment(\n",
    "    model_name=\"skip_connections + auxiliary_depth\",\n",
    "    model_class=SkipConnectionAuxiliaryDepthNet,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    description=\"Skip connections for gradient flow + auxiliary depth supervision\",\n",
    "    strategy=\"auxiliary\",\n",
    "    w_depth_map={'A': 0.0, 'B': 0.5, 'C': 1.0}\n",
    ")\n",
    "\n",
    "log_experiment(result)\n",
    "show_leaderboard()\n",
    "comparison = compare_with_baseline(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensing-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
